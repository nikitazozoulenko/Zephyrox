{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Union, Any, Optional, Dict, Literal, Callable\n",
    "import time\n",
    "import collections\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, tensor\n",
    "import pandas as pd\n",
    "import openml\n",
    "\n",
    "#from aeon.regression.sklearn import RotationForestRegressor\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from preprocessing.stream_transforms import normalize_mean_std_traindata, normalize_streams, augment_time, add_basepoint_zero\n",
    "from utils.utils import print_name, print_shape\n",
    "from models import ResNet, NeuralEulerODE, RidgeClassifierCVModule, E2EResNet, LogisticRegressionSGD, RandFeatBoost\n",
    "\n",
    "np.set_printoptions(precision=3, threshold=5) # Print options\n",
    "device = \"cuda\" #torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: torch.Size([60000, 784])\n",
      "Train labels shape: torch.Size([60000, 10])\n",
      "Test data shape: torch.Size([10000, 784])\n",
      "Test labels shape: torch.Size([10000, 10])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Download and load the training data\n",
    "mnist_path = \"/home/nikita/hdd/MNIST\"\n",
    "trainset = datasets.MNIST(mnist_path, download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=len(trainset), shuffle=False)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.MNIST(mnist_path, download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=len(testset), shuffle=False)\n",
    "\n",
    "# Flatten the data\n",
    "X_train, y_train_cat = next(iter(trainloader))\n",
    "X_train = X_train.view(len(trainset), -1).to(device)\n",
    "X_test, y_test_cat = next(iter(testloader))\n",
    "X_test = X_test.view(len(testset), -1).to(device)\n",
    "\n",
    "# Convert train and test labels to one-hot encoding\n",
    "y_train = F.one_hot(y_train_cat, num_classes=10).float().to(device)\n",
    "y_test = F.one_hot(y_test_cat, num_classes=10).float().to(device)\n",
    "y_train_cat = y_train_cat.to(device)\n",
    "y_test_cat = y_test_cat.to(device)\n",
    "\n",
    "# Normalize by mean and std\n",
    "X_train, X_test = normalize_mean_std_traindata(X_train, X_test)\n",
    "print(f\"Train data shape: {X_train.shape}\")\n",
    "print(f\"Train labels shape: {y_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(24.9071, grad_fn=<AddBackward0>)\n",
      "tensor(24.3590, grad_fn=<AddBackward0>)\n",
      "tensor(2.1914, grad_fn=<AddBackward0>)\n",
      "tensor(2.1779, grad_fn=<AddBackward0>)\n",
      "tensor(2.1737, grad_fn=<AddBackward0>)\n",
      "tensor(2.1485, grad_fn=<AddBackward0>)\n",
      "tensor(2.1441, grad_fn=<AddBackward0>)\n",
      "tensor(2.1407, grad_fn=<AddBackward0>)\n",
      "tensor(2.1404, grad_fn=<AddBackward0>)\n",
      "tensor(2.1402, grad_fn=<AddBackward0>)\n",
      "tensor(2.1398, grad_fn=<AddBackward0>)\n",
      "tensor(2.1396, grad_fn=<AddBackward0>)\n",
      "tensor(2.1394, grad_fn=<AddBackward0>)\n",
      "tensor(2.1394, grad_fn=<AddBackward0>)\n",
      "tensor(2.1394, grad_fn=<AddBackward0>)\n",
      "tensor(2.1394, grad_fn=<AddBackward0>)\n",
      "tensor(2.1394, grad_fn=<AddBackward0>)\n",
      "tensor(2.1394, grad_fn=<AddBackward0>)\n",
      "tensor(2.1394, grad_fn=<AddBackward0>)\n",
      "tensor(2.1394, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from models import FittableModule, Dense, kaiming_normal_with_generator\n",
    "\n",
    "class LogisticRegression(FittableModule):\n",
    "    def __init__(self, \n",
    "                 generator: torch.Generator,\n",
    "                 in_dim: int = 784,\n",
    "                 out_dim: int = 10,\n",
    "                 l2_reg: float = 1.0,\n",
    "                 lr: float = 1.0,\n",
    "                 ):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "        self.l2_reg = l2_reg\n",
    "        self.lr = lr\n",
    "\n",
    "        if out_dim > 1:\n",
    "            self.loss = F.cross_entropy #this is with logits\n",
    "        else:\n",
    "            self.loss = F.binary_cross_entropy_with_logits\n",
    "\n",
    "\n",
    "    def fit(self, \n",
    "            X: Tensor, \n",
    "            y: Tensor,\n",
    "            init_W_b: Optional[Tuple[Tensor, Tensor]] = None,\n",
    "            ) -> Tuple[Tensor, Tensor]:\n",
    "        \n",
    "        # No onehot encoding\n",
    "        if y.dim() > 1:\n",
    "            y_labels = torch.argmax(y, dim=1)\n",
    "        else:\n",
    "            y_labels = y\n",
    "\n",
    "        # Put model on device\n",
    "        device = X.device\n",
    "        self.to(device)\n",
    "\n",
    "        # Initialize weights and bias\n",
    "        if init_W_b is not None:\n",
    "            W, b = init_W_b\n",
    "            self.linear.weight = W\n",
    "            self.linear.bias = b\n",
    "        else:\n",
    "            kaiming_normal_with_generator(self.linear.weight, self.generator)\n",
    "            nn.init.zeros_(self.linear.bias)\n",
    "        \n",
    "        with torch.enable_grad():\n",
    "            # Optimize\n",
    "            optimizer = torch.optim.LBFGS(self.linear.parameters(), lr=self.lr)\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                logits = self.linear(X)\n",
    "                loss = self.loss(logits, y_labels)\n",
    "                loss += self.l2_reg * torch.linalg.norm(self.linear.weight)**2\n",
    "                loss.backward()\n",
    "                print(loss)\n",
    "                return loss\n",
    "            optimizer.step(closure)\n",
    "        return self(X), y\n",
    "\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        return self.linear(X)\n",
    "\n",
    "\n",
    "N = 100\n",
    "D = 50\n",
    "C = 10\n",
    "gen = torch.Generator().manual_seed(42)\n",
    "X = torch.randn(N, D, generator=gen)\n",
    "y = torch.randint(0, C, size=(N,), generator=gen)\n",
    "model = LogisticRegression(\n",
    "        gen,\n",
    "        in_dim = D,\n",
    "        out_dim = C,\n",
    "        l2_reg = 1.0,\n",
    "    )\n",
    "_, _ = model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Random Feature Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import FittableModule, create_layer\n",
    "from ridge_ALOOCV import fit_ridge_ALOOCV\n",
    "    \n",
    "\n",
    "def fit_logistic_regression(\n",
    "        X: Tensor, \n",
    "        y: Tensor, \n",
    "        l2_reg: float = 1,\n",
    "        init_W_b: Optional[Tuple[Tensor, Tensor]] = None,\n",
    "    ):\n",
    "    \"\"\"Uses L-BFGS to fit a logistic regression model to the data.\"\"\"\n",
    "    with torch.enable_grad():\n",
    "        # Initialize weights\n",
    "        if init_W_b is None:\n",
    "            W = torch.randn(X.shape[1], y.shape[1], requires_grad=True, device=X.device)\n",
    "            b = torch.randn(y.shape[1], requires_grad=True, device=X.device)\n",
    "        else:\n",
    "            W, b = init_W_b\n",
    "        \n",
    "        # Define loss function\n",
    "        def loss_fn(W, b):\n",
    "            logits = X @ W + b\n",
    "            loss = F.cross_entropy_with_logits(logits, y)\n",
    "            return loss\n",
    "        \n",
    "        # Optimize\n",
    "        optimizer = torch.optim.LBFGS([W, b], lr=0.01)\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(W, b)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        optimizer.step(closure)\n",
    "    \n",
    "    return W, b\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GradientRandFeatBoost(FittableModule):\n",
    "    def __init__(self, \n",
    "                 generator: torch.Generator, \n",
    "                 hidden_dim: int = 128, # TODO\n",
    "                 bottleneck_dim: int = 128,\n",
    "                 out_dim: int = 1,\n",
    "                 n_layers: int = 5,\n",
    "                 activation: nn.Module = nn.Tanh(),\n",
    "                 l2_reg: float = 0.01,\n",
    "                 feature_type = \"SWIM\", # \"dense\", identity\n",
    "                 boost_lr: float = 1.0,\n",
    "                 upscale: Optional[str] = \"dense\",\n",
    "                 ):\n",
    "        super(GradientRandFeatBoost, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bottleneck_dim = bottleneck_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.activation = activation\n",
    "        self.l2_reg = l2_reg\n",
    "        self.feature_type = feature_type\n",
    "        self.boost_lr = boost_lr\n",
    "        self.upscale = upscale\n",
    "\n",
    "        # save for now. for more memory efficient implementation, we can remove a lot of this\n",
    "        self.W = []\n",
    "        self.b = []\n",
    "        self.alphas = []\n",
    "        self.layers = []\n",
    "        self.deltas = []\n",
    "\n",
    "\n",
    "    def fit(self, X: Tensor, y: Tensor):\n",
    "        # with torch.no_grad():\n",
    "\n",
    "        #optional upscale\n",
    "        if self.upscale == \"dense\":\n",
    "            self.upscale = create_layer(self.generator, self.upscale, X.shape[1], self.hidden_dim, None)\n",
    "            X, y = self.upscale.fit(X, y)\n",
    "        elif self.upscale == \"SWIM\":\n",
    "            self.upscale = create_layer(self.generator, self.upscale, X.shape[1], self.hidden_dim, self.activation)\n",
    "            X, y = self.upscale.fit(X, y)\n",
    "\n",
    "        # Create classifier W_0\n",
    "        W, b = fit_logistic_regression(X, y, self.l2_reg, init_W_b=None)\n",
    "\n",
    "        # Layerwise boosting\n",
    "        for t in range(self.n_layers):\n",
    "            # Step 1: Create random feature layer   \n",
    "            layer = create_layer(self.generator, self.feature_type, self.hidden_dim, self.bottleneck_dim, self.activation)\n",
    "            F, y = layer.fit(X, y)\n",
    "\n",
    "            # Step 2: Obtain activation gradient\n",
    "            # X shape (N, D) --- ResNet neurons\n",
    "            # F shape (N, p) --- random features\n",
    "            # y shape (N, d) --- target\n",
    "            # r shape (N, D) --- residual at currect boosting iteration\n",
    "            # W shape (D, d) --- top level classifier\n",
    "            r = y - X @ W - b   # G = (y - X @ W - b) @ W.T TODO TODO TODO CLASSFICATION\n",
    "            SW, U = torch.linalg.eigh(W @ W.T)\n",
    "            SF, V = torch.linalg.eigh(F.T @ F)\n",
    "            Delta = (U.T @ W @ r.T @ F @ V) / (N*self.l2_reg + SW[:, None]*SF[None, :])\n",
    "            Delta = (U @ Delta @ V.T).T\n",
    "            #TODO de-center F and r, and include an intercept. How to do this for my special equation?\n",
    "\n",
    "            # Step 3: Learn top level classifier\n",
    "            X = X + self.boost_lr * F @ Delta\n",
    "            W, b, alpha = fit_ridge_ALOOCV(X, y)\n",
    "\n",
    "            # store\n",
    "            self.layers.append(layer)\n",
    "            self.deltas.append(Delta)\n",
    "            self.W.append(W)\n",
    "            self.b.append(b)\n",
    "            self.alphas.append(alpha)\n",
    "\n",
    "        return X @ W + b, y\n",
    "\n",
    "\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        with torch.no_grad():\n",
    "            if self.upscale is not None:\n",
    "                X = self.upscale(X)\n",
    "            for layer, Delta in zip(self.layers, self.deltas):\n",
    "                X = X + self.boost_lr * layer(X) @ Delta\n",
    "            return X @ self.W[-1] + self.b[-1]\n",
    "        \n",
    "\n",
    "\n",
    "N = 100\n",
    "D = 50\n",
    "C = 10\n",
    "bottleneck_dim = 70\n",
    "gen = torch.Generator().manual_seed(42)\n",
    "X = torch.randn(N, D, generator=gen)\n",
    "y = torch.randint(0, C, size=(N,), generator=gen)\n",
    "model = GradientRandFeatBoost(\n",
    "        gen,\n",
    "        hidden_dim = D,\n",
    "        bottleneck_dim = bottleneck_dim,\n",
    "        out_dim = C,\n",
    "        n_layers = 1,\n",
    "        upscale = \"dense\",\n",
    "        feature_type = \"dense\",\n",
    "    )\n",
    "_, _ = model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_allmodels_1dataset(\n",
    "        generator: torch.Generator,\n",
    "        X_train: Tensor,\n",
    "        y_train: Tensor,\n",
    "        X_test: Tensor,\n",
    "        y_test: Tensor,\n",
    "        ):\n",
    "    \n",
    "    D = X_train.shape[1]\n",
    "    hidden_size = 512\n",
    "    bottleneck_dim = 1*hidden_size\n",
    "    num_epochs = 50\n",
    "    batch_size = 128\n",
    "\n",
    "    # (name, model, kwargs). kwargs separate to save memory\n",
    "    model_list = [\n",
    "        # [\"T=10 RandFeatureBoost\", RandFeatBoost,\n",
    "        #         {\"generator\": generator,\n",
    "        #          \"in_dim\": D,\n",
    "        #          \"hidden_size\": hidden_size,\n",
    "        #          \"out_dim\": 10,\n",
    "        #          \"n_blocks\": 9,\n",
    "        #          \"activation\": nn.Tanh(),\n",
    "        #          \"loss_fn\": nn.CrossEntropyLoss(),\n",
    "        #          \"adam_lr\": 1e-2,\n",
    "        #          \"boost_lr\": 1.0,\n",
    "        #          \"epochs\": num_epochs,\n",
    "        #          \"batch_size\": batch_size,\n",
    "        #          \"upscale_type\": \"SWIM\",  # \"dense\", \"identity\"\n",
    "        #          }],\n",
    "\n",
    "        # [\"Tabular Ridge\", RidgeClassifierCVModule, {}],\n",
    "\n",
    "        # [\"Logistic SGD\", LogisticRegressionSGD, \n",
    "        #         {\"generator\": generator,\n",
    "        #          \"num_epochs\": num_epochs,\n",
    "        #          \"batch_size\": batch_size,\n",
    "        #          }],\n",
    "\n",
    "        # [\"Logistic L-BFSG\", LogisticRegression, \n",
    "        #         {\"generator\": generator,\n",
    "        #          \"in_dim\": D,\n",
    "        #          \"out_dim\": 10,\n",
    "        #          }],\n",
    "\n",
    "        # [\"T=1 Dense\", ResNet,\n",
    "        #         {\"generator\": generator,\n",
    "        #          \"in_dim\": D,\n",
    "        #          \"hidden_size\": hidden_size,\n",
    "        #          \"bottleneck_dim\": None,\n",
    "        #          \"n_blocks\": 0,\n",
    "        #          \"upsample_layer\": \"dense\",\n",
    "        #          \"output_layer\": \"logistic regression\",\n",
    "        #          }],\n",
    "\n",
    "        # [\"T=1 SWIM Grad\", ResNet,\n",
    "        #         {\"generator\": generator,\n",
    "        #         \"in_dim\": D,\n",
    "        #         \"hidden_size\": hidden_size,\n",
    "        #         \"bottleneck_dim\": None,\n",
    "        #         \"n_blocks\": 0,\n",
    "        #         \"upsample_layer\": \"SWIM\",\n",
    "        #         \"output_layer\": \"logistic regression\",\n",
    "        #         }],\n",
    "        \n",
    "        # [\"T=1 SWIM Unif\", ResNet,\n",
    "        #         {\"generator\": generator,\n",
    "        #         \"in_dim\": D,\n",
    "        #         \"hidden_size\": hidden_size,\n",
    "        #         \"bottleneck_dim\": None,\n",
    "        #         \"n_blocks\": 0,\n",
    "        #         \"upsample_layer\": \"SWIM\",\n",
    "        #         \"sampling_method\": \"uniform\",\n",
    "        #         \"output_layer\": \"logistic regression\",\n",
    "        #         }],\n",
    "\n",
    "    ]\n",
    "    for lr in [1.0]:\n",
    "        for l2_reg in [1, 0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001]:\n",
    "            model_list += [\n",
    "                [f\"Logistic L-BFSG, l2={l2_reg} lr={lr}\", \n",
    "                    LogisticRegression, \n",
    "                    {\"generator\": generator,\n",
    "                    \"in_dim\": D,\n",
    "                    \"out_dim\": 10,\n",
    "                    \"l2_reg\": l2_reg,\n",
    "                    \"lr\": lr,\n",
    "                    }],\n",
    "            ]\n",
    "\n",
    "    # for n_blocks in [4]:\n",
    "    #     model_list += [\n",
    "    #     [f\"T={n_blocks+1} End2End\", E2EResNet,\n",
    "    #             {\"generator\": generator,\n",
    "    #             \"in_dim\": D,\n",
    "    #             \"hidden_size\": hidden_size,\n",
    "    #             \"bottleneck_dim\": bottleneck_dim,\n",
    "    #             \"out_dim\": 10,\n",
    "    #             \"n_blocks\": n_blocks,\n",
    "    #             \"activation\": nn.Tanh(),\n",
    "    #             \"loss\": nn.CrossEntropyLoss(),\n",
    "    #             \"lr\": 1e-2,\n",
    "    #             \"epochs\": num_epochs,\n",
    "    #             \"batch_size\": batch_size}\n",
    "    #             ],\n",
    "\n",
    "    #     [f\"T={n_blocks+1} ResSWIM Grad-dense\", ResNet,\n",
    "    #             {\"generator\": generator,\n",
    "    #             \"in_dim\": D,\n",
    "    #             \"hidden_size\": hidden_size,\n",
    "    #             \"bottleneck_dim\": bottleneck_dim,\n",
    "    #             \"n_blocks\": n_blocks,\n",
    "    #             \"upsample_layer\": \"SWIM\",\n",
    "    #             \"res_layer1\": \"SWIM\",\n",
    "    #             \"res_layer2\": \"dense\",\n",
    "    #             \"output_layer\": \"logistic regression\",\n",
    "    #             }\n",
    "    #             ],\n",
    "\n",
    "    #     [f\"T={n_blocks+1} ResSWIM Grad-id\", ResNet,\n",
    "    #             {\"generator\": generator,\n",
    "    #             \"in_dim\": D,\n",
    "    #             \"hidden_size\": hidden_size,\n",
    "    #             \"bottleneck_dim\": hidden_size,\n",
    "    #             \"n_blocks\": n_blocks,\n",
    "    #             \"upsample_layer\": \"SWIM\",\n",
    "    #             \"res_layer1\": \"SWIM\",\n",
    "    #             \"res_layer2\": \"identity\",\n",
    "    #             \"output_layer\": \"logistic regression\",\n",
    "    #             }\n",
    "    #             ],\n",
    "\n",
    "    #     [f\"T={n_blocks+1} ResDense\", ResNet,\n",
    "    #             {\"generator\": generator,\n",
    "    #             \"in_dim\": D,\n",
    "    #             \"hidden_size\": hidden_size,\n",
    "    #             \"bottleneck_dim\": hidden_size,\n",
    "    #             \"n_blocks\": n_blocks,\n",
    "    #             \"upsample_layer\": \"dense\",\n",
    "    #             \"res_layer1\": \"dense\",\n",
    "    #             \"res_layer2\": \"identity\",\n",
    "    #             \"output_layer\": \"logistic regression\",\n",
    "    #             }\n",
    "    #             ],\n",
    "    # ]\n",
    "    \n",
    "    results = []\n",
    "    model_names = []\n",
    "    for name, model, model_args in model_list:\n",
    "        print(name)\n",
    "        t0 = time.perf_counter()\n",
    "        model = model(**model_args).to(X_train.device)\n",
    "        pred_train, _ = model.fit(X_train, y_train)\n",
    "        t1 = time.perf_counter()\n",
    "        pred_test = model(X_test)\n",
    "        t2 = time.perf_counter()\n",
    "        \n",
    "        #convert to class predictions:\n",
    "        if len(pred_train.shape) == 2:\n",
    "            pred_train = torch.argmax(pred_train, dim=1)\n",
    "            pred_test = torch.argmax(pred_test, dim=1)\n",
    "        acc_train = (pred_train == y_train_cat).float().mean().item()\n",
    "        acc_test = (pred_test == y_test_cat).float().mean().item()\n",
    "\n",
    "        result = np.array( [acc_train, acc_test, t1-t0, t2-t1] )\n",
    "        results.append( result )\n",
    "        model_names.append( name )\n",
    "\n",
    "    return model_names, results\n",
    "\n",
    "\n",
    "\n",
    "def run_all_experiments(\n",
    "        name_save: str = \"PLACEHOLDER\",\n",
    "        ):\n",
    "    # Fetch and process each dataset\n",
    "    experiments = {}\n",
    "    generator = torch.Generator(device=device).manual_seed(999)\n",
    "    results = run_allmodels_1dataset(\n",
    "        generator, X_train, y_train, X_test, y_test, \n",
    "        )\n",
    "    experiments[\"MNIST\"] = results\n",
    "\n",
    "    # Save results\n",
    "    # Assuming experiments is a dict where keys are dataset names and values are tuples (model_names, results)\n",
    "    attributes = [\"acc_train\", \"acc_test\", \"t_fit\", \"t_feat\"]\n",
    "    data_list = []\n",
    "    # Process the data\n",
    "    for dataset_name, (model_names, results) in experiments.items():\n",
    "        dataset_data = {}\n",
    "        for attr_idx, attribute in enumerate(attributes):\n",
    "            for model_idx, model_name in enumerate(model_names):\n",
    "                dataset_data[(attribute, model_name)] = results[model_idx][attr_idx]\n",
    "        data_list.append(pd.DataFrame(dataset_data, index=[dataset_name]))\n",
    "\n",
    "    # Combine all datasets into a single DataFrame\n",
    "    df = pd.concat(data_list)\n",
    "    df = df.sort_index(axis=1)\n",
    "    print(df)\n",
    "    df.to_pickle(f\"MNIST_{name_save}.pkl\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic L-BFSG, l2=1 lr=1.0\n",
      "tensor(23.0018, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(22.8644, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.0741, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(1.8592, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(1.4502, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(1.4365, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(1.4337, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(1.4332, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(1.4329, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(1.4325, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(1.4316, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(1.4305, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(1.4297, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(1.4294, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(1.4293, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(1.4293, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(1.4293, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(1.4293, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(1.4293, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(1.4293, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Logistic L-BFSG, l2=0.1 lr=1.0\n",
      "tensor(5.0443, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(4.9927, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.2886, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(1.8026, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(1.1776, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.9627, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.7943, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.7514, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.7233, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.7170, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.7138, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.7126, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.7116, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.7107, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.7102, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.7097, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.7089, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.7079, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.7062, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.7082, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Logistic L-BFSG, l2=0.01 lr=1.0\n",
      "tensor(3.3226, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(3.2664, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.9705, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.7740, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.6820, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.6209, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.5610, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.5207, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.4867, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.4597, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.4383, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.4187, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.4075, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3994, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3927, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3898, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3869, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3857, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3844, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3834, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Logistic L-BFSG, l2=0.001 lr=1.0\n",
      "tensor(3.0608, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(3.0048, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.8335, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.6242, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.4876, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.4457, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.4145, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3720, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3510, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3395, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3286, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3230, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3152, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3084, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3050, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2977, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2953, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2916, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2888, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2850, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Logistic L-BFSG, l2=0.0001 lr=1.0\n",
      "tensor(2.7034, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.6536, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.7832, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.5820, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.4796, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.4081, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3596, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3393, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3222, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3089, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2981, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2902, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2825, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2770, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2726, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2695, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2659, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2639, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2607, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2566, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Logistic L-BFSG, l2=1e-05 lr=1.0\n",
      "tensor(2.7950, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.7444, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.7786, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.5621, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.4807, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.4092, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3640, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3409, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3230, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3084, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2975, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2895, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2836, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2776, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2745, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2681, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2656, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2622, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2577, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2545, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Logistic L-BFSG, l2=1e-06 lr=1.0\n",
      "tensor(2.8859, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.8349, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.9931, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.5677, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.5051, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.4211, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3778, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3435, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3242, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.3088, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2989, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2900, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2833, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2773, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2742, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2688, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2654, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2608, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2577, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(0.2555, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "                               acc_test                                   \\\n",
      "      Logistic L-BFSG, l2=0.0001 lr=1.0 Logistic L-BFSG, l2=0.001 lr=1.0   \n",
      "MNIST                            0.9238                           0.9231   \n",
      "\n",
      "                                                                      \\\n",
      "      Logistic L-BFSG, l2=0.01 lr=1.0 Logistic L-BFSG, l2=0.1 lr=1.0   \n",
      "MNIST                          0.9174                         0.8991   \n",
      "\n",
      "                                                                     \\\n",
      "      Logistic L-BFSG, l2=1 lr=1.0 Logistic L-BFSG, l2=1e-05 lr=1.0   \n",
      "MNIST                        0.848                           0.9243   \n",
      "\n",
      "                                                               acc_train  \\\n",
      "      Logistic L-BFSG, l2=1e-06 lr=1.0 Logistic L-BFSG, l2=0.0001 lr=1.0   \n",
      "MNIST                            0.922                             0.931   \n",
      "\n",
      "                                                                        ...  \\\n",
      "      Logistic L-BFSG, l2=0.001 lr=1.0 Logistic L-BFSG, l2=0.01 lr=1.0  ...   \n",
      "MNIST                         0.930167                        0.918317  ...   \n",
      "\n",
      "                            t_feat                                   \\\n",
      "      Logistic L-BFSG, l2=1 lr=1.0 Logistic L-BFSG, l2=1e-05 lr=1.0   \n",
      "MNIST                     0.000059                         0.000054   \n",
      "\n",
      "                                                                   t_fit  \\\n",
      "      Logistic L-BFSG, l2=1e-06 lr=1.0 Logistic L-BFSG, l2=0.0001 lr=1.0   \n",
      "MNIST                         0.000056                          0.192694   \n",
      "\n",
      "                                                                        \\\n",
      "      Logistic L-BFSG, l2=0.001 lr=1.0 Logistic L-BFSG, l2=0.01 lr=1.0   \n",
      "MNIST                         0.197226                        0.192606   \n",
      "\n",
      "                                                                   \\\n",
      "      Logistic L-BFSG, l2=0.1 lr=1.0 Logistic L-BFSG, l2=1 lr=1.0   \n",
      "MNIST                        0.19768                     0.232776   \n",
      "\n",
      "                                                                         \n",
      "      Logistic L-BFSG, l2=1e-05 lr=1.0 Logistic L-BFSG, l2=1e-06 lr=1.0  \n",
      "MNIST                         0.191077                         0.199203  \n",
      "\n",
      "[1 rows x 28 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"7\" halign=\"left\">acc_test</th>\n",
       "      <th colspan=\"3\" halign=\"left\">acc_train</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"3\" halign=\"left\">t_feat</th>\n",
       "      <th colspan=\"7\" halign=\"left\">t_fit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Logistic L-BFSG, l2=0.0001 lr=1.0</th>\n",
       "      <th>Logistic L-BFSG, l2=0.001 lr=1.0</th>\n",
       "      <th>Logistic L-BFSG, l2=0.01 lr=1.0</th>\n",
       "      <th>Logistic L-BFSG, l2=0.1 lr=1.0</th>\n",
       "      <th>Logistic L-BFSG, l2=1 lr=1.0</th>\n",
       "      <th>Logistic L-BFSG, l2=1e-05 lr=1.0</th>\n",
       "      <th>Logistic L-BFSG, l2=1e-06 lr=1.0</th>\n",
       "      <th>Logistic L-BFSG, l2=0.0001 lr=1.0</th>\n",
       "      <th>Logistic L-BFSG, l2=0.001 lr=1.0</th>\n",
       "      <th>Logistic L-BFSG, l2=0.01 lr=1.0</th>\n",
       "      <th>...</th>\n",
       "      <th>Logistic L-BFSG, l2=1 lr=1.0</th>\n",
       "      <th>Logistic L-BFSG, l2=1e-05 lr=1.0</th>\n",
       "      <th>Logistic L-BFSG, l2=1e-06 lr=1.0</th>\n",
       "      <th>Logistic L-BFSG, l2=0.0001 lr=1.0</th>\n",
       "      <th>Logistic L-BFSG, l2=0.001 lr=1.0</th>\n",
       "      <th>Logistic L-BFSG, l2=0.01 lr=1.0</th>\n",
       "      <th>Logistic L-BFSG, l2=0.1 lr=1.0</th>\n",
       "      <th>Logistic L-BFSG, l2=1 lr=1.0</th>\n",
       "      <th>Logistic L-BFSG, l2=1e-05 lr=1.0</th>\n",
       "      <th>Logistic L-BFSG, l2=1e-06 lr=1.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MNIST</th>\n",
       "      <td>0.9238</td>\n",
       "      <td>0.9231</td>\n",
       "      <td>0.9174</td>\n",
       "      <td>0.8991</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.9243</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.930167</td>\n",
       "      <td>0.918317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.192694</td>\n",
       "      <td>0.197226</td>\n",
       "      <td>0.192606</td>\n",
       "      <td>0.19768</td>\n",
       "      <td>0.232776</td>\n",
       "      <td>0.191077</td>\n",
       "      <td>0.199203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               acc_test                                   \\\n",
       "      Logistic L-BFSG, l2=0.0001 lr=1.0 Logistic L-BFSG, l2=0.001 lr=1.0   \n",
       "MNIST                            0.9238                           0.9231   \n",
       "\n",
       "                                                                      \\\n",
       "      Logistic L-BFSG, l2=0.01 lr=1.0 Logistic L-BFSG, l2=0.1 lr=1.0   \n",
       "MNIST                          0.9174                         0.8991   \n",
       "\n",
       "                                                                     \\\n",
       "      Logistic L-BFSG, l2=1 lr=1.0 Logistic L-BFSG, l2=1e-05 lr=1.0   \n",
       "MNIST                        0.848                           0.9243   \n",
       "\n",
       "                                                               acc_train  \\\n",
       "      Logistic L-BFSG, l2=1e-06 lr=1.0 Logistic L-BFSG, l2=0.0001 lr=1.0   \n",
       "MNIST                            0.922                             0.931   \n",
       "\n",
       "                                                                        ...  \\\n",
       "      Logistic L-BFSG, l2=0.001 lr=1.0 Logistic L-BFSG, l2=0.01 lr=1.0  ...   \n",
       "MNIST                         0.930167                        0.918317  ...   \n",
       "\n",
       "                            t_feat                                   \\\n",
       "      Logistic L-BFSG, l2=1 lr=1.0 Logistic L-BFSG, l2=1e-05 lr=1.0   \n",
       "MNIST                     0.000059                         0.000054   \n",
       "\n",
       "                                                                   t_fit  \\\n",
       "      Logistic L-BFSG, l2=1e-06 lr=1.0 Logistic L-BFSG, l2=0.0001 lr=1.0   \n",
       "MNIST                         0.000056                          0.192694   \n",
       "\n",
       "                                                                        \\\n",
       "      Logistic L-BFSG, l2=0.001 lr=1.0 Logistic L-BFSG, l2=0.01 lr=1.0   \n",
       "MNIST                         0.197226                        0.192606   \n",
       "\n",
       "                                                                   \\\n",
       "      Logistic L-BFSG, l2=0.1 lr=1.0 Logistic L-BFSG, l2=1 lr=1.0   \n",
       "MNIST                        0.19768                     0.232776   \n",
       "\n",
       "                                                                         \n",
       "      Logistic L-BFSG, l2=1e-05 lr=1.0 Logistic L-BFSG, l2=1e-06 lr=1.0  \n",
       "MNIST                         0.191077                         0.199203  \n",
       "\n",
       "[1 rows x 28 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_all_experiments(name_save=\"TESTING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Logistic L-BFSG, l2=1e-05 lr=1.0     0.9243\n",
       "Logistic L-BFSG, l2=0.0001 lr=1.0    0.9238\n",
       "Logistic L-BFSG, l2=0.001 lr=1.0     0.9231\n",
       "Logistic L-BFSG, l2=1e-06 lr=1.0     0.9220\n",
       "Logistic L-BFSG, l2=0.01 lr=1.0      0.9174\n",
       "Logistic L-BFSG, l2=0.1 lr=1.0       0.8991\n",
       "Logistic L-BFSG, l2=1 lr=1.0         0.8480\n",
       "dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(\"MNIST_TESTING.pkl\")\n",
    "df[\"acc_test\"].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T=5 End2End        0.9717\n",
    "# T=1 Dense          0.9215\n",
    "# T=1 SWIM Unif      0.9207\n",
    "# T=1 SWIM Grad      0.9204\n",
    "# Logistic SGD       0.8990\n",
    "# Tabular Ridge      0.8606\n",
    "# Logistic L-BFSG    0.8480\n",
    "# dtype: float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"acc_train\"].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to experiment with RandFeatureBoost\n",
    "\n",
    "generator = torch.Generator(device=device).manual_seed(999)\n",
    "D = X_train.shape[1]\n",
    "hidden_size = 800\n",
    "num_epochs = 50\n",
    "batch_size = 128\n",
    "n_blocks = 10\n",
    "adam_lr=1e-2\n",
    "boost_lr=0.9\n",
    "\n",
    "model = RandFeatBoost(\n",
    "    generator=generator,\n",
    "    in_dim=D,\n",
    "    hidden_size=hidden_size,\n",
    "    out_dim=10,\n",
    "    n_blocks=n_blocks,\n",
    "    activation=nn.Tanh(),\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    adam_lr=adam_lr,\n",
    "    boost_lr=boost_lr,\n",
    "    epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    upscale_type=\"SWIM\",\n",
    "    ).to(device)\n",
    "\n",
    "pred_train, _ = model.fit(X_train, y_train)\n",
    "pred_test = model(X_test)\n",
    "pred_train = torch.argmax(pred_train, dim=1)\n",
    "pred_test = torch.argmax(pred_test, dim=1)\n",
    "acc_train = (pred_train == y_train_cat).float().mean().item()\n",
    "acc_test = (pred_test == y_test_cat).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc_train)\n",
    "print(acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_all_accuracies(X_train, X_test):\n",
    "    X_train = model.upscale(X_train)\n",
    "    X_test  = model.upscale(X_test)\n",
    "\n",
    "    for t, (layer, DELTA, classifier) in enumerate(zip(model.layers, model.deltas, model.classifiers)):\n",
    "        X_train = X_train + model.boost_lr * DELTA * (layer(X_train) - X_train)\n",
    "        X_test = X_test + model.boost_lr * DELTA * (layer(X_test) - X_test)\n",
    "        \n",
    "        pred_train = classifier(X_train)\n",
    "        pred_test = classifier(X_test)\n",
    "        pred_train = torch.argmax(pred_train, dim=1)\n",
    "        pred_test = torch.argmax(pred_test, dim=1)\n",
    "        acc_train = (pred_train == y_train_cat).float().mean().item()\n",
    "        acc_test = (pred_test == y_test_cat).float().mean().item()\n",
    "\n",
    "        print(f\"Block {t}: Train acc: {acc_train}, Test acc: {acc_test}\")\n",
    "\n",
    "\n",
    "print_all_accuracies(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SWIM-ID vs DENSE-ID vs SWIM-DENSE \n",
    "# implement 'finding gradient direction' gradient boosting\n",
    "\n",
    "# Test whether this is actually better than non-boost with same hidden size !!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment with DENSE\n",
    "\n",
    "generator = torch.Generator(device=device).manual_seed(999)\n",
    "D = X_train.shape[1]\n",
    "hidden_size = 800\n",
    "num_epochs = 50\n",
    "batch_size = 128\n",
    "n_blocks = 10\n",
    "adam_lr=1e-2\n",
    "boost_lr=0.9\n",
    "\n",
    "model = RandFeatBoost(\n",
    "    generator=generator,\n",
    "    in_dim=D,\n",
    "    hidden_size=hidden_size,\n",
    "    out_dim=10,\n",
    "    n_blocks=n_blocks,\n",
    "    activation=nn.Tanh(),\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    adam_lr=adam_lr,\n",
    "    boost_lr=boost_lr,\n",
    "    epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    upscale_type=\"dense\",\n",
    "    second_in_resblock=\"identity\",\n",
    "    ).to(device)\n",
    "\n",
    "pred_train, _ = model.fit(X_train, y_train)\n",
    "pred_test = model(X_test)\n",
    "pred_train = torch.argmax(pred_train, dim=1)\n",
    "pred_test = torch.argmax(pred_test, dim=1)\n",
    "acc_train = (pred_train == y_train_cat).float().mean().item()\n",
    "acc_test = (pred_test == y_test_cat).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_all_accuracies(X_train, X_test):\n",
    "    X_train = model.upscale(X_train)\n",
    "    X_test  = model.upscale(X_test)\n",
    "\n",
    "    for t, (layer, DELTA, classifier) in enumerate(zip(model.layers, model.deltas, model.classifiers)):\n",
    "        X_train = X_train + model.boost_lr * DELTA * (layer(X_train) - X_train)\n",
    "        X_test = X_test + model.boost_lr * DELTA * (layer(X_test) - X_test)\n",
    "        \n",
    "        pred_train = classifier(X_train)\n",
    "        pred_test = classifier(X_test)\n",
    "        pred_train = torch.argmax(pred_train, dim=1)\n",
    "        pred_test = torch.argmax(pred_test, dim=1)\n",
    "        acc_train = (pred_train == y_train_cat).float().mean().item()\n",
    "        acc_test = (pred_test == y_test_cat).float().mean().item()\n",
    "\n",
    "        print(f\"Block {t}: Train acc: {acc_train}, Test acc: {acc_test}\")\n",
    "\n",
    "print_all_accuracies(X_train, X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
