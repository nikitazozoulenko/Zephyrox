{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Union, Any, Optional, Dict, Literal, Callable\n",
    "import time\n",
    "import collections\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor, tensor\n",
    "import pandas as pd\n",
    "import openml\n",
    "\n",
    "#from aeon.regression.sklearn import RotationForestRegressor\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from preprocessing.stream_transforms import normalize_mean_std_traindata, normalize_streams, augment_time, add_basepoint_zero\n",
    "from utils.utils import print_name, print_shape\n",
    "from models import ResNet, NeuralEulerODE, RidgeClassifierCVModule, E2EResNet, LogisticRegressionSGD, RandFeatBoost\n",
    "\n",
    "np.set_printoptions(precision=3, threshold=5) # Print options\n",
    "device = \"cuda\" #torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: torch.Size([60000, 784])\n",
      "Train labels shape: torch.Size([60000, 10])\n",
      "Test data shape: torch.Size([10000, 784])\n",
      "Test labels shape: torch.Size([10000, 10])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Download and load the training data\n",
    "mnist_path = \"/home/nikita/hdd/MNIST\"\n",
    "trainset = datasets.MNIST(mnist_path, download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=len(trainset), shuffle=False)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.MNIST(mnist_path, download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=len(testset), shuffle=False)\n",
    "\n",
    "# Flatten the data\n",
    "X_train, y_train_cat = next(iter(trainloader))\n",
    "X_train = X_train.view(len(trainset), -1).to(device)\n",
    "X_test, y_test_cat = next(iter(testloader))\n",
    "X_test = X_test.view(len(testset), -1).to(device)\n",
    "\n",
    "# Convert train and test labels to one-hot encoding\n",
    "y_train = nn.functional.one_hot(y_train_cat, num_classes=10).float().to(device)\n",
    "y_test = nn.functional.one_hot(y_test_cat, num_classes=10).float().to(device)\n",
    "y_train_cat = y_train_cat.to(device)\n",
    "y_test_cat = y_test_cat.to(device)\n",
    "\n",
    "# Normalize by mean and std\n",
    "X_train, X_test = normalize_mean_std_traindata(X_train, X_test)\n",
    "print(f\"Train data shape: {X_train.shape}\")\n",
    "print(f\"Train labels shape: {y_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import LogisticRegression\n",
    "\n",
    "N = 100\n",
    "D = 50\n",
    "C = 10\n",
    "gen = torch.Generator().manual_seed(42)\n",
    "X = torch.randn(N, D, generator=gen)\n",
    "y = torch.randint(0, C, size=(N,), generator=gen)\n",
    "model = LogisticRegression(\n",
    "        gen,\n",
    "        in_dim = D,\n",
    "        out_dim = C,\n",
    "        l2_reg = 1.0,\n",
    "        max_iter = 100,\n",
    "    )\n",
    "_, _ = model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Random Feature Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linesearch 0.060540515929460526\n",
      "Linesearch 0.06334822624921799\n"
     ]
    }
   ],
   "source": [
    "from models import FittableModule, create_layer\n",
    "from ridge_ALOOCV import fit_ridge_ALOOCV\n",
    "\n",
    "\n",
    "def line_search_cross_entropy(cls, X, y, G_hat):\n",
    "    \"\"\"Solves the line search risk minimizatin problem\n",
    "    R(W, X + a * g) for mutliclass cross entropy loss\"\"\"\n",
    "    # No onehot encoding\n",
    "    if y.dim() > 1:\n",
    "        y_labels = torch.argmax(y, dim=1)\n",
    "    else:\n",
    "        y_labels = y\n",
    "\n",
    "    # Optimize\n",
    "    with torch.enable_grad():\n",
    "        alpha = torch.tensor([0.0], requires_grad=True, device=X.device, dtype=X.dtype)\n",
    "        optimizer = torch.optim.LBFGS([alpha])\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            logits = cls(X + alpha * G_hat)\n",
    "            loss = nn.functional.cross_entropy(logits, y_labels)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        optimizer.step(closure)\n",
    "\n",
    "    return alpha.detach().item()\n",
    "\n",
    "\n",
    "\n",
    "class GradientRandomFeatureBoostingClassification(FittableModule):\n",
    "    def __init__(self, \n",
    "                 generator: torch.Generator, \n",
    "                 hidden_dim: int = 128, # TODO\n",
    "                 bottleneck_dim: int = 128,\n",
    "                 out_dim: int = 1,\n",
    "                 n_layers: int = 5,\n",
    "                 activation: nn.Module = nn.Tanh(),\n",
    "                 l2_reg: float = 1,\n",
    "                 feature_type = \"SWIM\", # \"dense\", identity\n",
    "                 boost_lr: float = 1.0,\n",
    "                 upscale: Optional[str] = \"dense\",\n",
    "                 ):\n",
    "        super(GradientRandomFeatureBoostingClassification, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bottleneck_dim = bottleneck_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.activation = activation\n",
    "        self.l2_reg = l2_reg\n",
    "        self.feature_type = feature_type\n",
    "        self.boost_lr = boost_lr\n",
    "        self.upscale = upscale\n",
    "\n",
    "        # save for now. for more memory efficient implementation, we can remove a lot of this\n",
    "        self.classifiers = []\n",
    "        self.alphas = []\n",
    "        self.layers = []\n",
    "        self.deltas = []\n",
    "\n",
    "\n",
    "    def fit(self, X: Tensor, y: Tensor):\n",
    "        with torch.no_grad():\n",
    "\n",
    "            #optional upscale\n",
    "            if self.upscale == \"dense\":\n",
    "                self.upscale = create_layer(self.generator, self.upscale, X.shape[1], self.hidden_dim, None)\n",
    "                X, y = self.upscale.fit(X, y)\n",
    "            elif self.upscale == \"SWIM\":\n",
    "                self.upscale = create_layer(self.generator, self.upscale, X.shape[1], self.hidden_dim, self.activation)\n",
    "                X, y = self.upscale.fit(X, y)\n",
    "\n",
    "            # Create classifier W_0\n",
    "            cls = LogisticRegression(\n",
    "                self.generator,\n",
    "                in_dim = self.hidden_dim,\n",
    "                out_dim = self.out_dim,\n",
    "                l2_reg = self.l2_reg,\n",
    "                max_iter = 100,\n",
    "            ).to(X.device)\n",
    "            cls.fit(X, y)\n",
    "            self.classifiers.append(cls)\n",
    "\n",
    "            # Layerwise boosting\n",
    "            N = X.size(0)\n",
    "            for t in range(self.n_layers):\n",
    "                # Step 1: Create random feature layer   \n",
    "                layer = create_layer(self.generator, self.feature_type, self.hidden_dim, self.bottleneck_dim, self.activation)\n",
    "                F, y = layer.fit(X, y)\n",
    "\n",
    "                # Step 2: Obtain activation gradient\n",
    "                # X shape (N, D) --- ResNet neurons\n",
    "                # F shape (N, p) --- random features\n",
    "                # y shape (N, d) --- one-hot target\n",
    "                # r shape (N, D) --- residual at currect boosting iteration\n",
    "                # W shape (D, d) --- top level classifier\n",
    "                # probs shape (N, d) --- predicted probabilities\n",
    "\n",
    "                probs = nn.functional.softmax(cls(X), dim=1)\n",
    "                G = (y - probs) @ cls.linear.weight #negative gradient TODO divide by N?\n",
    "\n",
    "                # fit Least Squares to negative gradient (finding functional direction)\n",
    "                Delta, Delta_b, _ = fit_ridge_ALOOCV(F, G)\n",
    "\n",
    "                # Line search for risk minimization of R(W_t, Phi_t + linesearch * G_hat)\n",
    "                G_hat = F @ Delta + Delta_b\n",
    "                linesearch = line_search_cross_entropy(cls, X, y, G_hat)\n",
    "                print(\"Linesearch\", linesearch)\n",
    "\n",
    "                # Step 3: Learn top level classifier\n",
    "                X = X + self.boost_lr * linesearch * G_hat\n",
    "                cls = LogisticRegression(\n",
    "                    self.generator,\n",
    "                    in_dim = self.hidden_dim,\n",
    "                    out_dim = self.out_dim,\n",
    "                    l2_reg = self.l2_reg,\n",
    "                    max_iter = 20,\n",
    "                ).to(X.device)\n",
    "                cls.fit(\n",
    "                    X, \n",
    "                    y, \n",
    "                    init_W_b = (cls.linear.weight.detach().clone(), cls.linear.bias.detach().clone()) #TODO do i want this? or start from scratch?\n",
    "                )\n",
    "\n",
    "                #update Delta scale\n",
    "                Delta = Delta * linesearch\n",
    "                Delta_b = Delta_b * linesearch\n",
    "\n",
    "                # store\n",
    "                self.layers.append(layer)\n",
    "                self.deltas.append((Delta, Delta_b))\n",
    "                self.classifiers.append(cls)\n",
    "\n",
    "        return cls(X), y\n",
    "\n",
    "\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        with torch.no_grad():\n",
    "            if self.upscale is not None:\n",
    "                X = self.upscale(X)\n",
    "            for layer, (Delta, Delta_b) in zip(self.layers, self.deltas):\n",
    "                X = X + self.boost_lr * (layer(X) @ Delta + Delta_b)\n",
    "            return self.classifiers[-1](X)\n",
    "        \n",
    "\n",
    "\n",
    "N = 100\n",
    "D = 50\n",
    "C = 10\n",
    "bottleneck_dim = 70\n",
    "gen = torch.Generator().manual_seed(42)\n",
    "X = torch.randn(N, D, generator=gen)\n",
    "y = torch.randint(0, C, size=(N,), generator=gen)\n",
    "y = nn.functional.one_hot(y, num_classes=C).float()\n",
    "model = GradientRandomFeatureBoostingClassification(\n",
    "        gen,\n",
    "        hidden_dim = D,\n",
    "        bottleneck_dim = bottleneck_dim,\n",
    "        out_dim = C,\n",
    "        n_layers = 2,\n",
    "        upscale = \"dense\",\n",
    "        feature_type = \"dense\",\n",
    "    )\n",
    "_, _ = model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_allmodels_1dataset(\n",
    "        generator: torch.Generator,\n",
    "        X_train: Tensor,\n",
    "        y_train: Tensor,\n",
    "        X_test: Tensor,\n",
    "        y_test: Tensor,\n",
    "        ):\n",
    "    \n",
    "    D = X_train.shape[1]\n",
    "    hidden_size = 1024\n",
    "    bottleneck_dim = 1*hidden_size\n",
    "    num_epochs = 50\n",
    "    batch_size = 128\n",
    "\n",
    "    # (name, model, kwargs). kwargs separate to save memory\n",
    "    model_list = [\n",
    "        # [\"T=10 RandFeatureBoost\", RandFeatBoost,\n",
    "        #         {\"generator\": generator,\n",
    "        #          \"in_dim\": D,\n",
    "        #          \"hidden_size\": hidden_size,\n",
    "        #          \"out_dim\": 10,\n",
    "        #          \"n_blocks\": 9,\n",
    "        #          \"activation\": nn.Tanh(),\n",
    "        #          \"loss_fn\": nn.CrossEntropyLoss(),\n",
    "        #          \"adam_lr\": 1e-2,\n",
    "        #          \"boost_lr\": 1.0,\n",
    "        #          \"epochs\": num_epochs,\n",
    "        #          \"batch_size\": batch_size,\n",
    "        #          \"upscale_type\": \"SWIM\",  # \"dense\", \"identity\"\n",
    "        #          }],\n",
    "\n",
    "        # [\"Tabular Ridge\", RidgeClassifierCVModule, {}],\n",
    "\n",
    "        # [\"Logistic SGD\", LogisticRegressionSGD, \n",
    "        #         {\"generator\": generator,\n",
    "        #          \"num_epochs\": num_epochs,\n",
    "        #          \"batch_size\": batch_size,\n",
    "        #          }],\n",
    "\n",
    "        # [\"Logistic L-BFSG\", LogisticRegression, \n",
    "        #         {\"generator\": generator,\n",
    "        #          \"in_dim\": D,\n",
    "        #          \"out_dim\": 10,\n",
    "        #          }],\n",
    "\n",
    "        # [\"T=1 Dense\", ResNet,\n",
    "        #         {\"generator\": generator,\n",
    "        #          \"in_dim\": D,\n",
    "        #          \"hidden_size\": hidden_size,\n",
    "        #          \"bottleneck_dim\": None,\n",
    "        #          \"n_blocks\": 0,\n",
    "        #          \"upsample_layer\": \"dense\",\n",
    "        #          \"output_layer\": \"logistic regression\",\n",
    "        #          }],\n",
    "\n",
    "        # [\"T=1 SWIM Grad\", ResNet,\n",
    "        #         {\"generator\": generator,\n",
    "        #         \"in_dim\": D,\n",
    "        #         \"hidden_size\": hidden_size,\n",
    "        #         \"bottleneck_dim\": None,\n",
    "        #         \"n_blocks\": 0,\n",
    "        #         \"upsample_layer\": \"SWIM\",\n",
    "        #         \"output_layer\": \"logistic regression\",\n",
    "        #         }],\n",
    "        \n",
    "        # [\"T=1 SWIM Unif\", ResNet,\n",
    "        #         {\"generator\": generator,\n",
    "        #         \"in_dim\": D,\n",
    "        #         \"hidden_size\": hidden_size,\n",
    "        #         \"bottleneck_dim\": None,\n",
    "        #         \"n_blocks\": 0,\n",
    "        #         \"upsample_layer\": \"SWIM\",\n",
    "        #         \"sampling_method\": \"uniform\",\n",
    "        #         \"output_layer\": \"logistic regression\",\n",
    "        #         }],\n",
    "\n",
    "    ]\n",
    "    for lr in [1.0]:\n",
    "        for l2_reg in [1, 0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001]:\n",
    "            model_list += [\n",
    "                [f\"Logistic L-BFSG, l2={l2_reg} lr={lr}\", \n",
    "                    LogisticRegression, \n",
    "                    {\"generator\": generator,\n",
    "                    \"in_dim\": D,\n",
    "                    \"out_dim\": 10,\n",
    "                    \"l2_reg\": l2_reg,\n",
    "                    \"lr\": lr,\n",
    "                    }],\n",
    "            ]\n",
    "    \n",
    "    for n_blocks in range(0, 20, 5):\n",
    "        model_list += [\n",
    "            [f\"T={n_blocks+1} Gradient GRFBoost\", \n",
    "             GradientRandomFeatureBoostingClassification,\n",
    "                {\"generator\": generator,\n",
    "                \"hidden_dim\": hidden_size,\n",
    "                \"bottleneck_dim\": bottleneck_dim,\n",
    "                \"out_dim\": 10,\n",
    "                \"n_layers\": n_blocks,\n",
    "                \"activation\": nn.Tanh(),\n",
    "                \"l2_reg\": 0.00001,\n",
    "                \"feature_type\": \"SWIM\",\n",
    "                \"boost_lr\": 0.5,\n",
    "                \"upscale\": \"SWIM\",\n",
    "                },\n",
    "                ],\n",
    "        ]\n",
    "\n",
    "    # for n_blocks in [4]:\n",
    "    #     model_list += [\n",
    "    #     [f\"T={n_blocks+1} End2End\", E2EResNet,\n",
    "    #             {\"generator\": generator,\n",
    "    #             \"in_dim\": D,\n",
    "    #             \"hidden_size\": hidden_size,\n",
    "    #             \"bottleneck_dim\": bottleneck_dim,\n",
    "    #             \"out_dim\": 10,\n",
    "    #             \"n_blocks\": n_blocks,\n",
    "    #             \"activation\": nn.Tanh(),\n",
    "    #             \"loss\": nn.CrossEntropyLoss(),\n",
    "    #             \"lr\": 1e-2,\n",
    "    #             \"epochs\": num_epochs,\n",
    "    #             \"batch_size\": batch_size}\n",
    "    #             ],\n",
    "\n",
    "    #     [f\"T={n_blocks+1} ResSWIM Grad-dense\", ResNet,\n",
    "    #             {\"generator\": generator,\n",
    "    #             \"in_dim\": D,\n",
    "    #             \"hidden_size\": hidden_size,\n",
    "    #             \"bottleneck_dim\": bottleneck_dim,\n",
    "    #             \"n_blocks\": n_blocks,\n",
    "    #             \"upsample_layer\": \"SWIM\",\n",
    "    #             \"res_layer1\": \"SWIM\",\n",
    "    #             \"res_layer2\": \"dense\",\n",
    "    #             \"output_layer\": \"logistic regression\",\n",
    "    #             }\n",
    "    #             ],\n",
    "\n",
    "    #     [f\"T={n_blocks+1} ResSWIM Grad-id\", ResNet,\n",
    "    #             {\"generator\": generator,\n",
    "    #             \"in_dim\": D,\n",
    "    #             \"hidden_size\": hidden_size,\n",
    "    #             \"bottleneck_dim\": hidden_size,\n",
    "    #             \"n_blocks\": n_blocks,\n",
    "    #             \"upsample_layer\": \"SWIM\",\n",
    "    #             \"res_layer1\": \"SWIM\",\n",
    "    #             \"res_layer2\": \"identity\",\n",
    "    #             \"output_layer\": \"logistic regression\",\n",
    "    #             }\n",
    "    #             ],\n",
    "\n",
    "    #     [f\"T={n_blocks+1} ResDense\", ResNet,\n",
    "    #             {\"generator\": generator,\n",
    "    #             \"in_dim\": D,\n",
    "    #             \"hidden_size\": hidden_size,\n",
    "    #             \"bottleneck_dim\": hidden_size,\n",
    "    #             \"n_blocks\": n_blocks,\n",
    "    #             \"upsample_layer\": \"dense\",\n",
    "    #             \"res_layer1\": \"dense\",\n",
    "    #             \"res_layer2\": \"identity\",\n",
    "    #             \"output_layer\": \"logistic regression\",\n",
    "    #             }\n",
    "    #             ],\n",
    "    # ]\n",
    "    \n",
    "    results = []\n",
    "    model_names = []\n",
    "    for name, model, model_args in model_list:\n",
    "        print(name)\n",
    "        t0 = time.perf_counter()\n",
    "        model = model(**model_args).to(X_train.device)\n",
    "        pred_train, _ = model.fit(X_train, y_train)\n",
    "        t1 = time.perf_counter()\n",
    "        pred_test = model(X_test)\n",
    "        t2 = time.perf_counter()\n",
    "        \n",
    "        #convert to class predictions:\n",
    "        if len(pred_train.shape) == 2:\n",
    "            pred_train = torch.argmax(pred_train, dim=1)\n",
    "            pred_test = torch.argmax(pred_test, dim=1)\n",
    "        acc_train = (pred_train == y_train_cat).float().mean().item()\n",
    "        acc_test = (pred_test == y_test_cat).float().mean().item()\n",
    "\n",
    "        result = np.array( [acc_train, acc_test, t1-t0, t2-t1] )\n",
    "        results.append( result )\n",
    "        model_names.append( name )\n",
    "\n",
    "    return model_names, results\n",
    "\n",
    "\n",
    "\n",
    "def run_all_experiments(\n",
    "        name_save: str = \"PLACEHOLDER\",\n",
    "        ):\n",
    "    # Fetch and process each dataset\n",
    "    experiments = {}\n",
    "    generator = torch.Generator(device=device).manual_seed(999)\n",
    "    results = run_allmodels_1dataset(\n",
    "        generator, X_train, y_train, X_test, y_test, \n",
    "        )\n",
    "    experiments[\"MNIST\"] = results\n",
    "\n",
    "    # Save results\n",
    "    # Assuming experiments is a dict where keys are dataset names and values are tuples (model_names, results)\n",
    "    attributes = [\"acc_train\", \"acc_test\", \"t_fit\", \"t_feat\"]\n",
    "    data_list = []\n",
    "    # Process the data\n",
    "    for dataset_name, (model_names, results) in experiments.items():\n",
    "        dataset_data = {}\n",
    "        for attr_idx, attribute in enumerate(attributes):\n",
    "            for model_idx, model_name in enumerate(model_names):\n",
    "                dataset_data[(attribute, model_name)] = results[model_idx][attr_idx]\n",
    "        data_list.append(pd.DataFrame(dataset_data, index=[dataset_name]))\n",
    "\n",
    "    # Combine all datasets into a single DataFrame\n",
    "    df = pd.concat(data_list)\n",
    "    df = df.sort_index(axis=1)\n",
    "    print(df)\n",
    "    df.to_pickle(f\"MNIST_{name_save}.pkl\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic L-BFSG, l2=1 lr=1.0\n",
      "Logistic L-BFSG, l2=0.1 lr=1.0\n",
      "Logistic L-BFSG, l2=0.01 lr=1.0\n",
      "Logistic L-BFSG, l2=0.001 lr=1.0\n",
      "Logistic L-BFSG, l2=0.0001 lr=1.0\n",
      "Logistic L-BFSG, l2=1e-05 lr=1.0\n",
      "Logistic L-BFSG, l2=1e-06 lr=1.0\n",
      "T=1 Gradient GRFBoost\n",
      "T=6 Gradient GRFBoost\n",
      "Linesearch 0.12934249639511108\n",
      "Linesearch 0.5913510322570801\n",
      "Linesearch 0.8685545325279236\n",
      "Linesearch 1.1419506072998047\n",
      "Linesearch 1.4398552179336548\n",
      "T=11 Gradient GRFBoost\n",
      "Linesearch 0.12393397837877274\n",
      "Linesearch 0.5805110931396484\n",
      "Linesearch 0.8636003732681274\n",
      "Linesearch 1.0384536981582642\n",
      "Linesearch 1.3703676462173462\n",
      "Linesearch 1.4415912628173828\n",
      "Linesearch 1.8288248777389526\n",
      "Linesearch 2.098085880279541\n",
      "Linesearch 1.590389370918274\n",
      "Linesearch 2.4286677837371826\n",
      "T=16 Gradient GRFBoost\n",
      "Linesearch 0.1339123398065567\n",
      "Linesearch 0.5738614201545715\n",
      "Linesearch 0.8393223881721497\n",
      "Linesearch 1.082709789276123\n",
      "Linesearch 1.1137275695800781\n",
      "Linesearch 1.108267068862915\n",
      "Linesearch 1.8491078615188599\n",
      "Linesearch 1.5853270292282104\n",
      "Linesearch 1.8628833293914795\n",
      "Linesearch 2.237271547317505\n",
      "Linesearch 2.612023115158081\n",
      "Linesearch 1.5199261903762817\n",
      "Linesearch 3.3178157806396484\n",
      "Linesearch 4.359182357788086\n",
      "Linesearch 2.8803861141204834\n",
      "                               acc_test                                   \\\n",
      "      Logistic L-BFSG, l2=0.0001 lr=1.0 Logistic L-BFSG, l2=0.001 lr=1.0   \n",
      "MNIST                            0.9238                           0.9231   \n",
      "\n",
      "                                                                      \\\n",
      "      Logistic L-BFSG, l2=0.01 lr=1.0 Logistic L-BFSG, l2=0.1 lr=1.0   \n",
      "MNIST                          0.9174                         0.8991   \n",
      "\n",
      "                                                                     \\\n",
      "      Logistic L-BFSG, l2=1 lr=1.0 Logistic L-BFSG, l2=1e-05 lr=1.0   \n",
      "MNIST                        0.848                           0.9243   \n",
      "\n",
      "                                                              \\\n",
      "      Logistic L-BFSG, l2=1e-06 lr=1.0 T=1 Gradient GRFBoost   \n",
      "MNIST                            0.922                0.9245   \n",
      "\n",
      "                                                     ...  \\\n",
      "      T=11 Gradient GRFBoost T=16 Gradient GRFBoost  ...   \n",
      "MNIST                 0.9677                 0.9699  ...   \n",
      "\n",
      "                                 t_fit                                  \\\n",
      "      Logistic L-BFSG, l2=0.001 lr=1.0 Logistic L-BFSG, l2=0.01 lr=1.0   \n",
      "MNIST                         0.176594                        0.175518   \n",
      "\n",
      "                                                                   \\\n",
      "      Logistic L-BFSG, l2=0.1 lr=1.0 Logistic L-BFSG, l2=1 lr=1.0   \n",
      "MNIST                       0.180898                     0.207598   \n",
      "\n",
      "                                                                         \\\n",
      "      Logistic L-BFSG, l2=1e-05 lr=1.0 Logistic L-BFSG, l2=1e-06 lr=1.0   \n",
      "MNIST                         0.179979                         0.182197   \n",
      "\n",
      "                                                                           \\\n",
      "      T=1 Gradient GRFBoost T=11 Gradient GRFBoost T=16 Gradient GRFBoost   \n",
      "MNIST              1.519525                8.95056              12.768086   \n",
      "\n",
      "                             \n",
      "      T=6 Gradient GRFBoost  \n",
      "MNIST              5.342348  \n",
      "\n",
      "[1 rows x 44 columns]\n"
     ]
    }
   ],
   "source": [
    "_ = run_all_experiments(name_save=\"TESTING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T=16 Gradient GRFBoost               0.9699\n",
       "T=11 Gradient GRFBoost               0.9677\n",
       "T=6 Gradient GRFBoost                0.9622\n",
       "T=1 Gradient GRFBoost                0.9245\n",
       "Logistic L-BFSG, l2=1e-05 lr=1.0     0.9243\n",
       "Logistic L-BFSG, l2=0.0001 lr=1.0    0.9238\n",
       "Logistic L-BFSG, l2=0.001 lr=1.0     0.9231\n",
       "Logistic L-BFSG, l2=1e-06 lr=1.0     0.9220\n",
       "Logistic L-BFSG, l2=0.01 lr=1.0      0.9174\n",
       "Logistic L-BFSG, l2=0.1 lr=1.0       0.8991\n",
       "Logistic L-BFSG, l2=1 lr=1.0         0.8480\n",
       "dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(\"MNIST_TESTING.pkl\")\n",
    "df[\"acc_test\"].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T=5 End2End        0.9717\n",
    "# T=1 Dense          0.9215\n",
    "# T=1 SWIM Unif      0.9207\n",
    "# T=1 SWIM Grad      0.9204\n",
    "# Logistic SGD       0.8990\n",
    "# Tabular Ridge      0.8606\n",
    "# Logistic L-BFSG    0.8480\n",
    "# dtype: float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T=16 Gradient GRFBoost               0.979667\n",
       "T=11 Gradient GRFBoost               0.976983\n",
       "T=6 Gradient GRFBoost                0.968950\n",
       "Logistic L-BFSG, l2=1e-05 lr=1.0     0.931450\n",
       "Logistic L-BFSG, l2=1e-06 lr=1.0     0.931200\n",
       "Logistic L-BFSG, l2=0.0001 lr=1.0    0.931000\n",
       "Logistic L-BFSG, l2=0.001 lr=1.0     0.930167\n",
       "T=1 Gradient GRFBoost                0.924967\n",
       "Logistic L-BFSG, l2=0.01 lr=1.0      0.918317\n",
       "Logistic L-BFSG, l2=0.1 lr=1.0       0.894617\n",
       "Logistic L-BFSG, l2=1 lr=1.0         0.838183\n",
       "dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"acc_train\"].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment layer by layer with Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linesearch 0.33945271372795105\n",
      "Linesearch 0.6494695544242859\n",
      "Linesearch 1.0083847045898438\n",
      "Linesearch 1.5123884677886963\n",
      "Linesearch 1.8697398900985718\n",
      "Linesearch 1.9759445190429688\n",
      "Linesearch 3.2475860118865967\n",
      "Linesearch 3.6875827312469482\n",
      "Linesearch 5.643091678619385\n",
      "Linesearch 0.017211027443408966\n",
      "Linesearch 0.01666221395134926\n",
      "Linesearch 0.01779244653880596\n",
      "Linesearch 0.01595654897391796\n",
      "Linesearch 7.312465667724609\n",
      "Linesearch 0.01290623564273119\n",
      "Linesearch 0.013040311634540558\n",
      "Linesearch 0.014813247136771679\n",
      "Linesearch 0.01566963642835617\n",
      "Linesearch 0.013982666656374931\n",
      "Linesearch 0.012906176969408989\n"
     ]
    }
   ],
   "source": [
    "# Time to experiment with RandFeatureBoost\n",
    "\n",
    "\n",
    "generator = torch.Generator(device=device).manual_seed(999)\n",
    "D = X_train.shape[1]\n",
    "hidden_size = 512\n",
    "n_blocks = 20\n",
    "boost_lr=1.0\n",
    "\n",
    "model = GradientRandomFeatureBoostingClassification(\n",
    "    generator=generator,\n",
    "    hidden_dim=hidden_size,\n",
    "    out_dim=10,\n",
    "    n_layers=n_blocks,\n",
    "    l2_reg=0.0001,\n",
    "    boost_lr=boost_lr,\n",
    "    feature_type=\"SWIM\",\n",
    "    upscale=\"SWIM\",\n",
    "    ).to(device)\n",
    "\n",
    "pred_train, _ = model.fit(X_train, y_train)\n",
    "pred_test = model(X_test)\n",
    "pred_train = torch.argmax(pred_train, dim=1)\n",
    "pred_test = torch.argmax(pred_test, dim=1)\n",
    "acc_train = (pred_train == y_train_cat).float().mean().item()\n",
    "acc_test = (pred_test == y_test_cat).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9619166851043701\n",
      "0.9574999809265137\n"
     ]
    }
   ],
   "source": [
    "print(acc_train)\n",
    "print(acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 118.00 MiB. GPU 0 has a total capacity of 7.91 GiB of which 53.94 MiB is free. Including non-PyTorch memory, this process has 7.43 GiB memory in use. Of the allocated memory 7.12 GiB is allocated by PyTorch, and 167.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 25\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBlock \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Train acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc_train\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc_test\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m             \u001b[38;5;66;03m# print(\"delta norm\", torch.linalg.norm(Delta).item())\u001b[39;00m\n\u001b[1;32m     21\u001b[0m             \u001b[38;5;66;03m# print(\"X_train norm\", torch.linalg.norm(X_train).item() / X_train.size(0))\u001b[39;00m\n\u001b[1;32m     22\u001b[0m             \u001b[38;5;66;03m# print(\"X_test norm\", torch.linalg.norm(X_test).item() / X_test.size(0))\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mprint_all_accuracies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[64], line 3\u001b[0m, in \u001b[0;36mprint_all_accuracies\u001b[0;34m(X_train, X_test)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_all_accuracies\u001b[39m(X_train, X_test):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 3\u001b[0m         X_train \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m         X_test  \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mupscale(X_test)\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m t, (layer, (Delta, Delta_b), \u001b[38;5;28mcls\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(model\u001b[38;5;241m.\u001b[39mlayers, model\u001b[38;5;241m.\u001b[39mdeltas, model\u001b[38;5;241m.\u001b[39mclassifiers[\u001b[38;5;241m1\u001b[39m:])):\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/zephyrox/pytorch_based/SWIM/models.py:219\u001b[0m, in \u001b[0;36mSWIMLayer.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m--> 219\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m         X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(X)\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 118.00 MiB. GPU 0 has a total capacity of 7.91 GiB of which 53.94 MiB is free. Including non-PyTorch memory, this process has 7.43 GiB memory in use. Of the allocated memory 7.12 GiB is allocated by PyTorch, and 167.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "def print_all_accuracies(X_train, X_test):\n",
    "    with torch.no_grad():\n",
    "        X_train = model.upscale(X_train)\n",
    "        X_test  = model.upscale(X_test)\n",
    "\n",
    "        for t, (layer, (Delta, Delta_b), cls) in enumerate(zip(model.layers, model.deltas, model.classifiers[1:])):\n",
    "            X_train += model.boost_lr * (layer(X_train)@Delta + Delta_b )\n",
    "            X_test +=  model.boost_lr * (layer(X_test)@Delta + Delta_b )\n",
    "\n",
    "            #delta norm\n",
    "\n",
    "            pred_train = cls(X_train)\n",
    "            pred_test = cls(X_test)\n",
    "            pred_train = torch.argmax(pred_train, dim=1)\n",
    "            pred_test = torch.argmax(pred_test, dim=1)\n",
    "            acc_train = (pred_train == y_train_cat).float().mean().item()\n",
    "            acc_test = (pred_test == y_test_cat).float().mean().item()\n",
    "\n",
    "            print(f\"Block {t}: Train acc: {acc_train}, Test acc: {acc_test}\")\n",
    "            # print(\"delta norm\", torch.linalg.norm(Delta).item())\n",
    "            # print(\"X_train norm\", torch.linalg.norm(X_train).item() / X_train.size(0))\n",
    "            # print(\"X_test norm\", torch.linalg.norm(X_test).item() / X_test.size(0))\n",
    "            \n",
    "\n",
    "print_all_accuracies(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idea: normalize the gradient before fitting the next layer. This is to find the optimal direction. Then do line search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rand feat boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:19<00:00,  1.54it/s]\n",
      "100%|██████████| 30/30 [00:20<00:00,  1.48it/s]\n",
      "100%|██████████| 30/30 [00:19<00:00,  1.56it/s]\n",
      "100%|██████████| 30/30 [00:20<00:00,  1.49it/s]\n",
      "100%|██████████| 30/30 [00:19<00:00,  1.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# Time to experiment with RandFeatureBoost\n",
    "\n",
    "generator = torch.Generator(device=device).manual_seed(999)\n",
    "D = X_train.shape[1]\n",
    "hidden_size = 128\n",
    "num_epochs = 30\n",
    "batch_size = 512\n",
    "n_blocks = 5\n",
    "adam_lr=1e-2\n",
    "boost_lr=0.9\n",
    "\n",
    "model = RandFeatBoost(\n",
    "    generator=generator,\n",
    "    in_dim=D,\n",
    "    hidden_size=hidden_size,\n",
    "    out_dim=10,\n",
    "    n_blocks=n_blocks,\n",
    "    activation=nn.Tanh(),\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    adam_lr=adam_lr,\n",
    "    boost_lr=boost_lr,\n",
    "    epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    upscale_type=\"SWIM\",\n",
    "    ).to(device)\n",
    "\n",
    "pred_train, _ = model.fit(X_train, y_train)\n",
    "pred_test = model(X_test)\n",
    "pred_train = torch.argmax(pred_train, dim=1)\n",
    "pred_test = torch.argmax(pred_test, dim=1)\n",
    "acc_train = (pred_train == y_train_cat).float().mean().item()\n",
    "acc_test = (pred_test == y_test_cat).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9162166714668274\n",
      "0.9185000061988831\n"
     ]
    }
   ],
   "source": [
    "print(acc_train)\n",
    "print(acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-3.3867]], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0875]], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[0.1648]], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[0.0923]], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.1482]], device='cuda:0', requires_grad=True)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 0: Train acc: 0.9017000198364258, Test acc: 0.9041999578475952\n",
      "Block 1: Train acc: 0.9082333445549011, Test acc: 0.9110999703407288\n",
      "Block 2: Train acc: 0.9126999974250793, Test acc: 0.9138000011444092\n",
      "Block 3: Train acc: 0.912766695022583, Test acc: 0.9143999814987183\n",
      "Block 4: Train acc: 0.9162166714668274, Test acc: 0.9185000061988831\n"
     ]
    }
   ],
   "source": [
    "def print_all_accuracies(X_train, X_test):\n",
    "    X_train = model.upscale(X_train)\n",
    "    X_test  = model.upscale(X_test)\n",
    "\n",
    "    for t, (layer, DELTA, classifier) in enumerate(zip(model.layers, model.deltas, model.classifiers)):\n",
    "        X_train = X_train + model.boost_lr * DELTA * (layer(X_train) - X_train)\n",
    "        X_test = X_test + model.boost_lr * DELTA * (layer(X_test) - X_test)\n",
    "        \n",
    "        pred_train = classifier(X_train)\n",
    "        pred_test = classifier(X_test)\n",
    "        pred_train = torch.argmax(pred_train, dim=1)\n",
    "        pred_test = torch.argmax(pred_test, dim=1)\n",
    "        acc_train = (pred_train == y_train_cat).float().mean().item()\n",
    "        acc_test = (pred_test == y_test_cat).float().mean().item()\n",
    "\n",
    "        print(f\"Block {t}: Train acc: {acc_train}, Test acc: {acc_test}\")\n",
    "\n",
    "\n",
    "print_all_accuracies(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SWIM-ID vs DENSE-ID vs SWIM-DENSE \n",
    "# implement 'finding gradient direction' gradient boosting\n",
    "\n",
    "# Test whether this is actually better than non-boost with same hidden size !!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment with DENSE\n",
    "\n",
    "generator = torch.Generator(device=device).manual_seed(999)\n",
    "D = X_train.shape[1]\n",
    "hidden_size = 800\n",
    "num_epochs = 50\n",
    "batch_size = 128\n",
    "n_blocks = 10\n",
    "adam_lr=1e-2\n",
    "boost_lr=0.9\n",
    "\n",
    "model = RandFeatBoost(\n",
    "    generator=generator,\n",
    "    in_dim=D,\n",
    "    hidden_size=hidden_size,\n",
    "    out_dim=10,\n",
    "    n_blocks=n_blocks,\n",
    "    activation=nn.Tanh(),\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    adam_lr=adam_lr,\n",
    "    boost_lr=boost_lr,\n",
    "    epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    upscale_type=\"dense\",\n",
    "    second_in_resblock=\"identity\",\n",
    "    ).to(device)\n",
    "\n",
    "pred_train, _ = model.fit(X_train, y_train)\n",
    "pred_test = model(X_test)\n",
    "pred_train = torch.argmax(pred_train, dim=1)\n",
    "pred_test = torch.argmax(pred_test, dim=1)\n",
    "acc_train = (pred_train == y_train_cat).float().mean().item()\n",
    "acc_test = (pred_test == y_test_cat).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_all_accuracies(X_train, X_test):\n",
    "    X_train = model.upscale(X_train)\n",
    "    X_test  = model.upscale(X_test)\n",
    "\n",
    "    for t, (layer, DELTA, classifier) in enumerate(zip(model.layers, model.deltas, model.classifiers)):\n",
    "        X_train = X_train + model.boost_lr * DELTA * (layer(X_train) - X_train)\n",
    "        X_test = X_test + model.boost_lr * DELTA * (layer(X_test) - X_test)\n",
    "        \n",
    "        pred_train = classifier(X_train)\n",
    "        pred_test = classifier(X_test)\n",
    "        pred_train = torch.argmax(pred_train, dim=1)\n",
    "        pred_test = torch.argmax(pred_test, dim=1)\n",
    "        acc_train = (pred_train == y_train_cat).float().mean().item()\n",
    "        acc_test = (pred_test == y_test_cat).float().mean().item()\n",
    "\n",
    "        print(f\"Block {t}: Train acc: {acc_train}, Test acc: {acc_test}\")\n",
    "\n",
    "print_all_accuracies(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEXT TIME: TODO TODO TODO TODO TODO\n",
    "\n",
    "# do gradient boosting for classification\n",
    "\n",
    "# xgboost model\n",
    "\n",
    "# optuna (with xgboost to start with?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
