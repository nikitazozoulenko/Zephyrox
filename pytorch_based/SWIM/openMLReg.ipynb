{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Union, Any, Optional, Dict, Literal, Callable\n",
    "import time\n",
    "import collections\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor, tensor\n",
    "import pandas as pd\n",
    "import openml\n",
    "\n",
    "#from aeon.regression.sklearn import RotationForestRegressor\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from preprocessing.stream_transforms import normalize_mean_std_traindata, normalize_streams, augment_time, add_basepoint_zero\n",
    "from utils.utils import print_name, print_shape\n",
    "from models import ResNet, NeuralEulerODE, RidgeCVModule, E2EResNet, StagewiseRandFeatBoostRegression\n",
    "\n",
    "np.set_printoptions(precision=3, threshold=5) # Print options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import FittableModule, create_layer\n",
    "from ridge_ALOOCV import fit_ridge_ALOOCV\n",
    "\n",
    "\n",
    "\n",
    "class GradientRandFeatBoost(FittableModule):\n",
    "    def __init__(self, \n",
    "                 generator: torch.Generator, \n",
    "                 hidden_dim: int = 128, #TODO\n",
    "                 bottleneck_dim: int = 128,\n",
    "                 out_dim: int = 1,\n",
    "                 n_layers: int = 5,\n",
    "                 activation: nn.Module = nn.Tanh(),\n",
    "                 l2_reg: float = 0.01,\n",
    "                 feature_type = \"SWIM\", # \"dense\", identity\n",
    "                 boost_lr: float = 1.0,\n",
    "                 upscale: Optional[str] = \"dense\",\n",
    "                 ):\n",
    "        super(GradientRandFeatBoost, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bottleneck_dim = bottleneck_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.activation = activation\n",
    "        self.l2_reg = l2_reg\n",
    "        self.feature_type = feature_type\n",
    "        self.boost_lr = boost_lr\n",
    "        self.upscale = upscale\n",
    "\n",
    "        # save for now. for more memory efficient implementation, we can remove a lot of this\n",
    "        self.W = []\n",
    "        self.b = []\n",
    "        self.alphas = []\n",
    "        self.layers = []\n",
    "        self.deltas = []\n",
    "\n",
    "\n",
    "    def fit(self, X: Tensor, y: Tensor):\n",
    "        with torch.no_grad():\n",
    "            #optional upscale\n",
    "            if self.upscale == \"dense\":\n",
    "                self.upscale = create_layer(self.generator, self.upscale, X.shape[1], self.hidden_dim, None)\n",
    "                X, y = self.upscale.fit(X, y)\n",
    "            elif self.upscale == \"SWIM\":\n",
    "                self.upscale = create_layer(self.generator, self.upscale, X.shape[1], self.hidden_dim, self.activation)\n",
    "                X, y = self.upscale.fit(X, y)\n",
    "\n",
    "            # Create regressor W_0\n",
    "            W, b, alpha = fit_ridge_ALOOCV(X, y)\n",
    "            self.W.append(W)\n",
    "            self.b.append(b)\n",
    "            self.alphas.append(alpha)\n",
    "\n",
    "            # Layerwise boosting\n",
    "            N = X.size(0)\n",
    "            for t in range(self.n_layers):\n",
    "                # Step 1: Create random feature layer   \n",
    "                layer = create_layer(self.generator, self.feature_type, self.hidden_dim, self.bottleneck_dim, self.activation)\n",
    "                F, y = layer.fit(X, y)\n",
    "\n",
    "                # Step 2: Obtain activation gradient and learn Delta\n",
    "                # X shape (N, D) --- ResNet neurons\n",
    "                # F shape (N, p) --- random features\n",
    "                # y shape (N, d) --- target\n",
    "                # W shape (D, d) --- top level classifier\n",
    "                # G shape (N, D) --- gradient of neurons\n",
    "                # r shape (N, D) --- residual at currect boosting iteration\n",
    "                r = y - X @ W - b\n",
    "                G = r @ W.T\n",
    "                \n",
    "                # fit to negative gradient (finding functional direction)\n",
    "                Delta, Delta_b, _ = fit_ridge_ALOOCV(F, G)\n",
    "\n",
    "                # Line search closed form risk minimization of R(W_t, Phi_{t+1})\n",
    "                Ghat = F @ Delta + Delta_b\n",
    "                XW = Ghat @ W\n",
    "                numerator = torch.sum( r * XW / N )\n",
    "                denominator = torch.sum(XW*XW/N)\n",
    "                linesearch = numerator / (denominator + 0.01)\n",
    "\n",
    "                # Step 3: Learn top level classifier\n",
    "                X = X + self.boost_lr * linesearch * Ghat\n",
    "                W, b, alpha = fit_ridge_ALOOCV(X, y)\n",
    "\n",
    "                #update Delta scale\n",
    "                Delta = Delta * linesearch\n",
    "                Delta_b = Delta_b * linesearch\n",
    "\n",
    "                # store\n",
    "                self.layers.append(layer)\n",
    "                self.deltas.append((Delta, Delta_b))\n",
    "                self.W.append(W)\n",
    "                self.b.append(b)\n",
    "                self.alphas.append(alpha)\n",
    "\n",
    "            return X @ W + b, y\n",
    "\n",
    "\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        with torch.no_grad():\n",
    "            if self.upscale is not None:\n",
    "                X = self.upscale(X)\n",
    "            for layer, (Delta, Delta_b) in zip(self.layers, self.deltas):\n",
    "                X = X + self.boost_lr * (layer(X) @ Delta + Delta_b)\n",
    "            return X @ self.W[-1] + self.b[-1]\n",
    "    \n",
    "N = 100\n",
    "D = 50\n",
    "p = 30\n",
    "d = 4\n",
    "bottleneck_dim = 70\n",
    "\n",
    "gen = torch.Generator().manual_seed(42)\n",
    "X = torch.randn(N, D, generator=gen)\n",
    "y = torch.randn(N, d, generator=gen)\n",
    "model = GradientRandFeatBoost(\n",
    "        gen,\n",
    "        hidden_dim = D,\n",
    "        bottleneck_dim = bottleneck_dim,\n",
    "        out_dim = d,\n",
    "        n_layers = 5,\n",
    "        upscale = \"dense\",\n",
    "    )\n",
    "_, _ = model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenML code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the collection with ID 353\n",
    "collection = openml.study.get_suite(353)\n",
    "dataset_ids = collection.data\n",
    "metadata_list = []\n",
    "\n",
    "# Fetch and process each dataset\n",
    "for i, dataset_id in enumerate(dataset_ids):\n",
    "    dataset = openml.datasets.get_dataset(dataset_id)\n",
    "    X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "        target=dataset.default_target_attribute\n",
    "    )\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)[..., None]\n",
    "    \n",
    "    # Determine if the dataset has categorical features\n",
    "    has_categorical = any(categorical_indicator)\n",
    "    \n",
    "    # Extract the required metadata\n",
    "    metadata = {\n",
    "        'dataset_id': dataset.id,\n",
    "        'name': dataset.name,\n",
    "        'n_obs': int(dataset.qualities['NumberOfInstances']),\n",
    "        'n_features': int(dataset.qualities['NumberOfFeatures']),\n",
    "        '%_unique_y': len(np.unique(y))/len(y),\n",
    "        'n_unique_y': len(np.unique(y)),\n",
    "        'has_categorical': has_categorical\n",
    "    }\n",
    "    \n",
    "    metadata_list.append(metadata)\n",
    "    print(f\" {i+1}/{len(dataset_ids)} Processed dataset {dataset.id}: {dataset.name}\")\n",
    "\n",
    "# Create a DataFrame from the metadata list\n",
    "df_metadata = pd.DataFrame(metadata_list).sort_values('%_unique_y', ascending=False).set_index(\"dataset_id\").sort_index()\n",
    "df_metadata.sort_values('%_unique_y', ascending=True)\n",
    "\n",
    "# Display the metadata DataFrame\n",
    "df_metadata.loc[44962, \"has_categorical\"] = True\n",
    "df_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download single dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_openml_dataset(dataset_id, \n",
    "                        normalize_X:bool = True,\n",
    "                        normalize_y:bool = True,\n",
    "                        train_test_size:float = 0.7,\n",
    "                        split_seed:int = 0,\n",
    "                        device=\"cpu\",\n",
    "                        ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    # Fetch dataset from OpenML by its ID\n",
    "    dataset = openml.datasets.get_dataset(dataset_id)\n",
    "    df, _, categorical_indicator, attribute_names = dataset.get_data()\n",
    "    df.dropna(inplace=True)\n",
    "    y = np.array(df.pop(dataset.default_target_attribute))[..., None]\n",
    "    X = np.array(df)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_test_size, random_state=split_seed)\n",
    "\n",
    "    #normalize\n",
    "    if normalize_X:\n",
    "        X_train, X_test = normalize_mean_std_traindata(X_train, X_test)\n",
    "        X_train = np.clip(X_train, -3, 3)\n",
    "        X_test = np.clip(X_test, -3, 3)\n",
    "    if normalize_y:\n",
    "        y_train, y_test = normalize_mean_std_traindata(y_train, y_test)\n",
    "\n",
    "    return (torch.tensor(X_train.astype(np.float32), requires_grad=False, device=device),\n",
    "            torch.tensor(X_test.astype(np.float32), requires_grad=False, device=device),\n",
    "            torch.tensor(y_train.astype(np.float32), requires_grad=False, device=device),\n",
    "            torch.tensor(y_test.astype(np.float32), requires_grad=False, device=device))\n",
    "\n",
    "#dataset_id = 44971  # Replace with the dataset ID you want\n",
    "dataset_id = 44971 #44970\n",
    "X_train, X_test, y_train, y_test = load_openml_dataset(dataset_id, False, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import matplotlib.pyplot as plt\n",
    "# import plotly.graph_objects as go\n",
    "# from plotly.subplots import make_subplots\n",
    "\n",
    "# def get_activation(name, activations):\n",
    "#     def hook(model, input, output):\n",
    "#         activations[name] = output.detach()\n",
    "#     return hook\n",
    "\n",
    "\n",
    "# def register_hooks(model, activations):\n",
    "#     for name, layer in model.named_modules():\n",
    "#         print(name)\n",
    "#         if \".dense\" not in name:\n",
    "#             layer.register_forward_hook(get_activation(name, activations))\n",
    "\n",
    "\n",
    "\n",
    "# def neuron_distribution_for_each_layer(X_train, y_train, X_test):\n",
    "#     D = X_train.shape[1]\n",
    "#     n_layers = 2\n",
    "#     g1 = torch.Generator().manual_seed(0)\n",
    "#     model = SampledEulerODE(g1, D, 10*D, n_layers, upsample_module='sampled', sampling_method='gradient')\n",
    "#     #model = SampledResNet(g1, D, 10*D, 10*D, n_layers, upsample_module='sampled', sampling_method='gradient')\n",
    "#     model.fit(X_train, y_train)\n",
    "\n",
    "#     activations = {}\n",
    "#     register_hooks(model, activations)\n",
    "    \n",
    "#     # Forward pass\n",
    "#     model(X_test)\n",
    "    \n",
    "#     # Plot input data distribution\n",
    "#     fig = make_subplots(rows=1, cols=1)\n",
    "#     fig.add_trace(go.Histogram(x=X_train.flatten().cpu().numpy(), nbinsx=50, name='Train', histnorm='probability density', opacity=0.5))\n",
    "#     fig.add_trace(go.Histogram(x=X_test.flatten().cpu().numpy(), nbinsx=50, name='Test', histnorm='probability density', opacity=0.5))\n",
    "#     fig.update_layout(title_text='Input Data Distribution', xaxis_title='Input Feature Value', yaxis_title='Probability Density', barmode='overlay')\n",
    "#     fig.show()\n",
    "\n",
    "#     # Plot activations\n",
    "#     for name, activation in activations.items():\n",
    "#         fig = make_subplots(rows=1, cols=1)\n",
    "#         fig.add_trace(go.Histogram(x=activation.flatten().cpu().numpy(), nbinsx=50, name='Activation', histnorm='probability density', opacity=0.5))\n",
    "#         fig.update_layout(title_text=f'Activations at Layer: {name}', xaxis_title='Activation Value', yaxis_title='Probability Density', barmode='overlay')\n",
    "#         fig.show()\n",
    "\n",
    "\n",
    "# neuron_distribution_for_each_layer(X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit on a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_allmodels_1dataset(\n",
    "        generator: torch.Generator,\n",
    "        X_train: Tensor,\n",
    "        y_train: Tensor,\n",
    "        X_test: Tensor,\n",
    "        y_test: Tensor,\n",
    "        ):\n",
    "    \n",
    "    D = X_train.shape[1]\n",
    "    hidden_size = 128\n",
    "    bottleneck_dim = hidden_size\n",
    "\n",
    "    # (name, model, kwargs). kwargs separate to save memory\n",
    "    model_list = [\n",
    "        [\"RidgeCV\", RidgeCVModule, {}],\n",
    "\n",
    "        [\"T=3 End2End\", E2EResNet,\n",
    "                {\"generator\": generator,\n",
    "                \"in_dim\": D,\n",
    "                \"hidden_size\": hidden_size,\n",
    "                \"bottleneck_dim\": bottleneck_dim,\n",
    "                \"out_dim\": 1,\n",
    "                \"n_blocks\": 2,\n",
    "                \"activation\": nn.ReLU(),\n",
    "                \"loss\": nn.MSELoss(),\n",
    "                \"lr\": 1e-3,\n",
    "                \"epochs\": 50,\n",
    "                \"batch_size\": 64,}\n",
    "                ],\n",
    "\n",
    "        [\"T=1 Dense\", ResNet,\n",
    "                {\"generator\": generator,\n",
    "                 \"in_dim\": D,\n",
    "                 \"hidden_size\": hidden_size,\n",
    "                 \"bottleneck_dim\": None,\n",
    "                 \"n_blocks\": 0,\n",
    "                 \"upsample_layer\": \"dense\",}\n",
    "                 ],\n",
    "\n",
    "        [\"T=1 SWIM Grad\", ResNet,\n",
    "                {\"generator\": generator,\n",
    "                \"in_dim\": D,\n",
    "                \"hidden_size\": hidden_size,\n",
    "                \"bottleneck_dim\": None,\n",
    "                \"n_blocks\": 0,\n",
    "                \"upsample_layer\": \"SWIM\",}\n",
    "                ],\n",
    "        \n",
    "        [\"T=1 SWIM Unif\", ResNet,\n",
    "                {\"generator\": generator,\n",
    "                \"in_dim\": D,\n",
    "                \"hidden_size\": hidden_size,\n",
    "                \"bottleneck_dim\": None,\n",
    "                \"n_blocks\": 0,\n",
    "                \"upsample_layer\": \"SWIM\",\n",
    "                \"sampling_method\": \"uniform\",}\n",
    "                ],\n",
    "    ]\n",
    "\n",
    "    for n_blocks in [2, 4]:\n",
    "        model_list += [\n",
    "        [f\"T={n_blocks+1} ResSWIM Grad-dense\", ResNet,\n",
    "                {\"generator\": generator,\n",
    "                \"in_dim\": D,\n",
    "                \"hidden_size\": hidden_size,\n",
    "                \"bottleneck_dim\": bottleneck_dim,\n",
    "                \"n_blocks\": n_blocks,\n",
    "                \"upsample_layer\": \"SWIM\",\n",
    "                \"res_layer1\": \"SWIM\",\n",
    "                \"res_layer2\": \"dense\",}\n",
    "                ],\n",
    "\n",
    "        [f\"T={n_blocks+1} ResSWIM Grad-id\", ResNet,\n",
    "                {\"generator\": generator,\n",
    "                \"in_dim\": D,\n",
    "                \"hidden_size\": hidden_size,\n",
    "                \"bottleneck_dim\": hidden_size,\n",
    "                \"n_blocks\": n_blocks,\n",
    "                \"upsample_layer\": \"SWIM\",\n",
    "                \"res_layer1\": \"SWIM\",\n",
    "                \"res_layer2\": \"identity\",}\n",
    "                ],\n",
    "                \n",
    "        [f\"T={n_blocks+1} ResDense\", ResNet,\n",
    "                {\"generator\": generator,\n",
    "                \"in_dim\": D,\n",
    "                \"hidden_size\": hidden_size,\n",
    "                \"bottleneck_dim\": hidden_size,\n",
    "                \"n_blocks\": n_blocks,\n",
    "                \"upsample_layer\": \"dense\",\n",
    "                \"res_layer1\": \"dense\",\n",
    "                \"res_layer2\": \"identity\",}\n",
    "                ],\n",
    "        ]\n",
    "        \n",
    "    for n_layers in range(0, 50, 5):\n",
    "        model_list += [\n",
    "        [f\"StagewiseRandFeatBoost_{n_layers}\", StagewiseRandFeatBoostRegression,\n",
    "                {\"generator\": generator,\n",
    "                \"hidden_dim\": hidden_size,\n",
    "                \"bottleneck_dim\": bottleneck_dim,\n",
    "                \"out_dim\": 1,\n",
    "                \"n_layers\": n_layers,\n",
    "                \"activation\": nn.Tanh(),\n",
    "                \"l2_reg\": 1,  #TODO experiment with much higher l2reg than 0.01\n",
    "                \"feature_type\": \"SWIM\",\n",
    "                \"boost_lr\": 1.0,\n",
    "                \"upscale\": \"dense\",}\n",
    "                ],\n",
    "        \n",
    "        [f\"GradientRandFeatBoost_{n_layers}\", GradientRandFeatBoost,\n",
    "                {\"generator\": generator,\n",
    "                \"hidden_dim\": hidden_size,\n",
    "                \"bottleneck_dim\": bottleneck_dim,\n",
    "                \"out_dim\": 1,\n",
    "                \"n_layers\": n_layers,\n",
    "                \"activation\": nn.Tanh(),\n",
    "                \"l2_reg\": 1,  #TODO experiment with much higher l2reg than 0.01\n",
    "                \"feature_type\": \"SWIM\",\n",
    "                \"boost_lr\": 1.0,\n",
    "                \"upscale\": \"dense\",}\n",
    "                ],\n",
    "        ]\n",
    "    \n",
    "    results = []\n",
    "    model_names = []\n",
    "    for name, model, model_args in model_list:\n",
    "        t0 = time.perf_counter()\n",
    "        model = model(**model_args).to(X_train.device)\n",
    "        pred_train, _ = model.fit(X_train, y_train)\n",
    "        t1 = time.perf_counter()\n",
    "        pred_test = model(X_test)\n",
    "        t2 = time.perf_counter()\n",
    "        rmse_train = root_mean_squared_error(y_train.cpu(), pred_train.cpu().detach()) \n",
    "        rmse_test = root_mean_squared_error(y_test.cpu(), pred_test.cpu().detach())\n",
    "\n",
    "        result = np.array( [rmse_train, rmse_test, t1-t0, t2-t1] )\n",
    "        results.append( result )\n",
    "        model_names.append( name )\n",
    "\n",
    "    return model_names, results\n",
    "\n",
    "\n",
    "\n",
    "def run_all_experiments(\n",
    "        dataset_ids: List,\n",
    "        name_save: str = \"PLACEHOLDER\",\n",
    "        device=\"cuda\",\n",
    "        ):\n",
    "    # Fetch and process each dataset\n",
    "    experiments = {}\n",
    "    for i, dataset_id in enumerate(dataset_ids):\n",
    "        X_train, X_test, y_train, y_test = load_openml_dataset(dataset_id, split_seed=0, device=device)\n",
    "        generator = torch.Generator(device=device).manual_seed(999)\n",
    "        results = run_allmodels_1dataset(\n",
    "            generator, X_train, y_train, X_test, y_test, \n",
    "            )\n",
    "        experiments[dataset_id] = results\n",
    "        print(f\" {i+1}/{len(dataset_ids)} Processed dataset {dataset_id}\")\n",
    "\n",
    "    # Save results\n",
    "    # Assuming experiments is a dict where keys are dataset names and values are tuples (model_names, results)\n",
    "    attributes = [\"RMSE_train\", \"RMSE_test\", \"t_fit\", \"t_feat\"]\n",
    "    data_list = []\n",
    "    # Process the data\n",
    "    for dataset_name, (model_names, results) in experiments.items():\n",
    "        dataset_data = {}\n",
    "        for attr_idx, attribute in enumerate(attributes):\n",
    "            for model_idx, model_name in enumerate(model_names):\n",
    "                dataset_data[(attribute, model_name)] = results[model_idx][attr_idx]\n",
    "        data_list.append(pd.DataFrame(dataset_data, index=[dataset_name]))\n",
    "\n",
    "    # Combine all datasets into a single DataFrame\n",
    "    df = pd.concat(data_list)\n",
    "    df = df.sort_index(axis=1)\n",
    "    print(df)\n",
    "    df.to_pickle(f\"OpenML_reg_{name_save}.pkl\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ids_not_categorical = list(df_metadata.query(\"has_categorical == False\").index.values)\n",
    "dataset_ids_not_categorical = sorted([int(x) for x in dataset_ids_not_categorical])\n",
    "run_all_experiments(dataset_ids_not_categorical[0:3], name_save=\"FIRSTBOOST128TestingGradient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg = pd.read_pickle(\"OpenML_reg_FIRSTBOOST128TestingGradient.pkl\")\n",
    "df_reg[\"RMSE_test\"].mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg[\"RMSE_test\"].rank(axis=1).mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg[\"RMSE_train\"].mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg[\"RMSE_train\"].rank(axis=1).mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg = pd.read_pickle(\"OpenML_reg_FIRSTBOOST128.pkl\")\n",
    "df_reg[\"RMSE_test\"].mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg[\"RMSE_test\"].rank(axis=1).mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg[\"RMSE_train\"].mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg[\"RMSE_train\"].rank(axis=1).mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg = pd.read_pickle(\"OpenML_reg_FIRSTBOOST512.pkl\")    # BAD TEST PERFORMANCE DUE TO REGULARIZATION ??\n",
    "df_reg[\"RMSE_test\"].mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg[\"RMSE_test\"].rank(axis=1).mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg[\"RMSE_train\"].mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg[\"RMSE_train\"].rank(axis=1).mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg = pd.read_pickle(\"OpenML_reg_FIRSTBOOST512lambda1.pkl\")\n",
    "df_reg[\"RMSE_test\"].mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg[\"RMSE_test\"].rank(axis=1).mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg[\"RMSE_train\"].mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg[\"RMSE_train\"].rank(axis=1).mean().sort_values()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
