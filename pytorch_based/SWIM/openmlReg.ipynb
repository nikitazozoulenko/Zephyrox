{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/aeon/base/__init__.py:24: FutureWarning: The aeon package will soon be releasing v1.0.0 with the removal of legacy modules and interfaces such as BaseTransformer and BaseForecaster. This will contain breaking changes. See aeon-toolkit.org for more information. Set aeon.AEON_DEPRECATION_WARNING or the AEON_DEPRECATION_WARNING environmental variable to 'False' to disable this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple, List, Union, Any, Optional, Dict, Literal, Callable\n",
    "import time\n",
    "import collections\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "\n",
    "from tqdm import tqdm\n",
    "import openml\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn.functional import relu\n",
    "from torch.nn.functional import tanh\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from aeon.regression.sklearn import RotationForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from aeon.datasets.tser_datasets import tser_soton\n",
    "from aeon.datasets import load_regression, load_classification\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from preprocessing.stream_transforms import normalize_mean_std_traindata, normalize_streams, augment_time, add_basepoint_zero\n",
    "from utils.utils import print_name, print_shape\n",
    "\n",
    "np.set_printoptions(precision=3, threshold=5) # Print options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenML code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fetch the collection with ID 353\n",
    "# collection = openml.study.get_suite(353)\n",
    "# dataset_ids = collection.data\n",
    "# metadata_list = []\n",
    "\n",
    "# # Fetch and process each dataset\n",
    "# for i, dataset_id in enumerate(dataset_ids):\n",
    "#     dataset = openml.datasets.get_dataset(dataset_id)\n",
    "#     X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "#         target=dataset.default_target_attribute\n",
    "#     )\n",
    "#     X = np.array(X)\n",
    "#     y = np.array(y)[..., None]\n",
    "    \n",
    "#     # Extract the required metadata\n",
    "#     metadata = {\n",
    "#         'dataset_id': dataset.id,\n",
    "#         'name': dataset.name,\n",
    "#         'n_obs': int(dataset.qualities['NumberOfInstances']),\n",
    "#         'n_features': int(dataset.qualities['NumberOfFeatures']),\n",
    "#         '%_unique_y': len(np.unique(y))/len(y),\n",
    "#         'n_unique_y': len(np.unique(y)),\n",
    "#     }\n",
    "    \n",
    "#     metadata_list.append(metadata)\n",
    "#     print(f\" {i+1}/{len(dataset_ids)} Processed dataset {dataset.id}: {dataset.name}\")\n",
    "\n",
    "# # Create a DataFrame from the metadata list\n",
    "# df_metadata = pd.DataFrame(metadata_list).sort_values('%_unique_y', ascending=False)\n",
    "# df_metadata.sort_values('%_unique_y', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download single dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_openml_dataset(dataset_id, \n",
    "                        normalize_X:bool = True,\n",
    "                        normalize_y:bool = True,\n",
    "                        train_test_size:float = 0.7,\n",
    "                        split_seed:int = 0) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    # Fetch dataset from OpenML by its ID\n",
    "    dataset = openml.datasets.get_dataset(dataset_id)\n",
    "    X, y, categorical_indicator, attribute_names = dataset.get_data(target=dataset.default_target_attribute)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)[..., None]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_test_size, random_state=split_seed)\n",
    "\n",
    "    #normalize\n",
    "    if normalize_X:\n",
    "        X_train, X_test = normalize_mean_std_traindata(X_train, X_test)\n",
    "    if normalize_y:\n",
    "        y_train, y_test = normalize_mean_std_traindata(y_train, y_test)\n",
    "\n",
    "    return (Tensor(X_train.astype(np.float32)), \n",
    "            Tensor(X_test.astype(np.float32)), \n",
    "            Tensor(y_train.astype(np.float32)), \n",
    "            Tensor(y_test.astype(np.float32)))\n",
    "\n",
    "dataset_id = 44971  # Replace with the dataset ID you want\n",
    "X_train, X_test, y_train, y_test = load_openml_dataset(dataset_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn.Module for sampled networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "##### Base classes                                          #####\n",
    "##### - FittableModule: A nn.Module with .fit(X, y) support #####\n",
    "##### - ResNetBase: which interatively calls .fit(X, y)     #####\n",
    "#################################################################\n",
    "\n",
    "class FittableModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FittableModule, self).__init__()\n",
    "    \n",
    "\n",
    "    def fit(self, \n",
    "            X: Optional[Tensor] = None, \n",
    "            y: Optional[Tensor] = None,\n",
    "        ) -> Tuple[Optional[Tensor], Optional[Tensor]]:\n",
    "        \"\"\"Given neurons of the previous layer, and target labels, fit the \n",
    "        module. Returns the forwarded activations and labels [f(X), y].\n",
    "\n",
    "        Args:\n",
    "            X (Optional[Tensor]): Forward-propagated activations of training data, shape (N, d).\n",
    "            y (Optional[Tensor]): Training labels, shape (N, p).\n",
    "        \n",
    "        Returns:\n",
    "            Forwarded activations and labels [f(X), y].\n",
    "        \"\"\"\n",
    "        return self(X), y\n",
    "\n",
    "\n",
    "\n",
    "class ResNetBase(nn.Module):\n",
    "    def __init__(self,\n",
    "                upsample:FittableModule,\n",
    "                blocks:List[FittableModule],\n",
    "                output_layer:FittableModule,\n",
    "                ):\n",
    "        \"\"\"Residual Network base class, with fit method for non-SGD training/initialization.\n",
    "\n",
    "        Args:\n",
    "            upsample (FittableModule): _description_\n",
    "            blocks (List[FittableModule]): _description_\n",
    "            output_layer (FittableModule): _description_\n",
    "        \"\"\"\n",
    "        super(ResNetBase, self).__init__()\n",
    "        self.upsample = upsample\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "        self.output_layer = output_layer\n",
    "\n",
    "    \n",
    "    def fit(self, X:Tensor, y:Tensor):\n",
    "        # X shape (N, d)\n",
    "        # y shape (N, p)\n",
    "        X, y = self.upsample.fit(X, y)\n",
    "        print(X)\n",
    "        for block in self.blocks:\n",
    "            X, y = block.fit(X, y)\n",
    "            print(X)\n",
    "        X, y = self.output_layer.fit(X, y)\n",
    "        print(X)\n",
    "        return X, y\n",
    "\n",
    "    \n",
    "    def forward(self, x:Tensor) -> Tensor:\n",
    "        # x shape (N, d)\n",
    "        x = self.upsample(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x) + x\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# I need:\n",
    "# - ResNetBase\n",
    "# - Sampled Layer\n",
    "# - 1 layer Sampled Network\n",
    "# - RidgeCV and TODO other classifiers/regressors. Should I implement this in pytorch or use sklearn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1470, 3]) out torch.float32\n",
      "tensor([[ 0.2244,  0.2429, -0.3403],\n",
      "        [-0.1479, -0.3924,  0.4136],\n",
      "        [ 0.8625,  0.9489, -0.4612],\n",
      "        ...,\n",
      "        [ 2.3646, -0.8617, -0.9985],\n",
      "        [ 1.1491, -1.4283,  0.4724],\n",
      "        [-0.7699,  1.0977,  0.5348]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "Dense(\n",
      "  (dense): Linear(in_features=11, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "######## Dense Layer ########\n",
    "#############################\n",
    "\n",
    "\n",
    "class Dense(FittableModule):\n",
    "    def __init__(self,\n",
    "                 generator: torch.Generator,\n",
    "                 in_dim: int,\n",
    "                 out_dim: int,\n",
    "                 ):\n",
    "        \"\"\"Dense MLP layer with LeCun weight initialization,\n",
    "        Gaussan bias initialization.\"\"\"\n",
    "        super(Dense, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.dense = nn.Linear(in_dim, out_dim)\n",
    "    \n",
    "    def fit(self, X:Tensor, y:Tensor):\n",
    "        with torch.no_grad():\n",
    "            nn.init.normal_(self.dense.weight, mean=0, std=self.in_dim**-0.5, generator=self.generator)\n",
    "            nn.init.normal_(self.dense.bias, mean=0, std=0.1, generator=self.generator) #arbitrary\n",
    "            return self(X), y\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.dense(X)\n",
    "    \n",
    "\n",
    "class Identity(FittableModule):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "    \n",
    "    def fit(self, X:Tensor, y:Tensor):\n",
    "        return X, y\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return X\n",
    "\n",
    "\n",
    "D = X_train.shape[1]\n",
    "g1 = torch.Generator().manual_seed(0)\n",
    "net = Dense(g1, D, 3)\n",
    "net.fit(X_train, y_train)\n",
    "out = net(X_test)\n",
    "print_name(out)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1470, 3]) out torch.float32\n",
      "tensor([[-0.1170, -0.2263, -0.4405],\n",
      "        [-0.3022, -0.4827, -0.1504],\n",
      "        [ 0.0027,  0.0866, -0.4289],\n",
      "        ...,\n",
      "        [ 0.0508,  0.1066,  0.0260],\n",
      "        [-0.2116, -0.5603,  0.3439],\n",
      "        [ 0.0938, -1.5502, -0.9001]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "PairSampledLinear(\n",
      "  (dense): Linear(in_features=11, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "#### Pair Sampled Networks ####\n",
    "###############################\n",
    "\n",
    "\n",
    "class PairSampledLinear(FittableModule):\n",
    "    def __init__(self,\n",
    "                 generator: torch.Generator,\n",
    "                 in_dim: int, \n",
    "                 out_dim: int,\n",
    "                 sampling_method: Literal['uniform', 'gradient'] = 'gradient'\n",
    "                 ):\n",
    "        \"\"\"Dense MLP layer with pair sampled weights.\n",
    "\n",
    "        Args:\n",
    "            generator (torch.Generator): PRNG object.\n",
    "            in_dim (int): Input dimension.\n",
    "            out_dim (int): Output dimension.\n",
    "            sampling_method (str): Pair sampling method. Uniform or gradient-weighted.\n",
    "        \"\"\"\n",
    "        super(PairSampledLinear, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.dense = nn.Linear(in_dim, out_dim)\n",
    "        self.sampling_method = sampling_method\n",
    "\n",
    "\n",
    "    def fit(self, \n",
    "            X: Tensor, \n",
    "            y: Tensor,\n",
    "        ) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"Given forward-propagated training data X at the previous \n",
    "        hidden layer, and supervised target labels y, fit the weights\n",
    "        iteratively by letting rows of the weight matrix be given by\n",
    "        pairs of samples from X. See paper for more details.\n",
    "\n",
    "        Args:\n",
    "            X (Tensor): Forward-propagated activations of training data, shape (N, d).\n",
    "            y (Tensor): Training labels, shape (N, p).\n",
    "        \n",
    "        Returns:\n",
    "            Forwarded activations and labels [f(X), y].\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            N, d = X.shape\n",
    "            dtype = X.dtype\n",
    "            device = X.device\n",
    "            EPS = torch.tensor(0.1, dtype=dtype, device=device)\n",
    "\n",
    "            #obtain pair indices\n",
    "            n = 5*N\n",
    "            idx1 = torch.arange(0, n, dtype=torch.int32, device=device) % N\n",
    "            delta = torch.randint(1, N, size=(n,), dtype=torch.int32, device=device, generator=self.generator)\n",
    "            idx2 = (idx1 + delta) % N\n",
    "            dx = X[idx2] - X[idx1]\n",
    "            dists = torch.linalg.norm(dx, axis=1, keepdims=True)\n",
    "            dists = torch.maximum(dists, EPS)\n",
    "            \n",
    "            if self.sampling_method==\"gradient\":\n",
    "                #calculate 'gradients'\n",
    "                dy = y[idx2] - y[idx1]\n",
    "                y_norm = torch.linalg.norm(dy, axis=1, keepdims=True) #NOTE 2023 paper uses ord=inf instead of ord=2\n",
    "                grad = (y_norm / dists).reshape(-1) \n",
    "                p = grad/grad.sum()\n",
    "            elif self.sampling_method==\"uniform\":\n",
    "                p = torch.ones(n, dtype=dtype, device=device) / n\n",
    "            else:\n",
    "                raise ValueError(f\"sampling_method must be 'uniform' or 'gradient'. Given: {self.sampling_method}\")\n",
    "\n",
    "            #sample pairs\n",
    "            selected_idx = torch.multinomial(\n",
    "                p,\n",
    "                self.out_dim,\n",
    "                replacement=True,\n",
    "                generator=self.generator\n",
    "                )\n",
    "            idx1 = idx1[selected_idx]\n",
    "            dx = dx[selected_idx]\n",
    "            dists = dists[selected_idx]\n",
    "\n",
    "            #define weights and biases\n",
    "            weights = dx / (dists**2)\n",
    "            biases = -torch.einsum('ij,ij->i', weights, X[idx1]) - 0.5\n",
    "            self.dense.weight.data = weights\n",
    "            self.dense.bias.data = biases\n",
    "            return self(X), y\n",
    "    \n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.dense(X)\n",
    "    \n",
    "    \n",
    "D = X_train.shape[1]\n",
    "g1 = torch.Generator().manual_seed(0)\n",
    "net = PairSampledLinear(g1, D, 3)\n",
    "net.fit(X_train, y_train)\n",
    "out = net(X_test)\n",
    "print_name(out)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1470, 11]) out torch.float32\n",
      "tensor([[ 2.9955e-01, -2.2911e-01, -1.0834e-01,  ...,  4.5266e-01,\n",
      "         -3.3509e-01,  4.6695e-01],\n",
      "        [ 7.5369e-02, -5.1524e-01, -7.1613e-03,  ...,  3.7685e-01,\n",
      "         -6.7468e-02,  2.6877e-01],\n",
      "        [ 3.0491e-01, -1.8205e-02, -1.2761e-01,  ...,  2.8464e-01,\n",
      "         -3.5406e-01,  3.7600e-01],\n",
      "        ...,\n",
      "        [-6.5020e-02, -1.0969e-01, -1.5286e-01,  ...,  3.6806e-04,\n",
      "         -3.4603e-02, -4.7464e-02],\n",
      "        [-3.4870e-01, -6.3833e-01, -8.5864e-02,  ...,  1.1641e-01,\n",
      "          2.6838e-01, -1.8195e-01],\n",
      "        [ 4.6800e-01, -3.4668e-01, -4.9589e-01,  ...,  1.1188e+00,\n",
      "         -7.2387e-01,  9.4487e-01]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "SampledBlock(\n",
      "  (sampled_linear): PairSampledLinear(\n",
      "    (dense): Linear(in_features=11, out_features=3, bias=True)\n",
      "  )\n",
      "  (activation): Tanh()\n",
      "  (upscale): Dense(\n",
      "    (dense): Linear(in_features=3, out_features=11, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "#### Sampled Bottleneck ResNet ####\n",
    "###################################\n",
    "\n",
    "\n",
    "class SampledBlock(FittableModule):\n",
    "    def __init__(self,\n",
    "                 generator: torch.Generator,\n",
    "                 hidden_dim: int, \n",
    "                 activation_dim: int,\n",
    "                 activation: nn.Module = nn.Tanh(),\n",
    "                 sampling_method: Literal['uniform', 'gradient'] = 'gradient'\n",
    "                 ):\n",
    "        \"\"\"A sampled layer followed by activation and linear layer.\n",
    "        Equivalent to a 1-hidden-layer Sampled Neural Network.\n",
    "\n",
    "        Args:\n",
    "            generator (torch.Generator): PRNG object.\n",
    "            in_dim (int): Input dimension.\n",
    "            out_dim (int): Output dimension.\n",
    "            activation (nn.Module): Activation function.\n",
    "            sampling_method (str): Pair sampling method. Uniform or gradient-weighted.\n",
    "        \"\"\"\n",
    "        super(SampledBlock, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.sampled_linear = PairSampledLinear(generator, hidden_dim, activation_dim, sampling_method)\n",
    "        self.activation = activation\n",
    "        self.upscale = Dense(generator, activation_dim, hidden_dim)\n",
    "    \n",
    "\n",
    "    def fit(self, X: Tensor, y: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        with torch.no_grad():\n",
    "            X, y = self.sampled_linear.fit(X, y)\n",
    "            X = self.activation(X)\n",
    "            X, y = self.upscale.fit(X, y)\n",
    "            return X, y\n",
    "\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.sampled_linear(X)\n",
    "        X = self.activation(X)\n",
    "        X = self.upscale(X)\n",
    "        return X\n",
    "    \n",
    "    \n",
    "D = X_train.shape[1]\n",
    "g1 = torch.Generator().manual_seed(0)\n",
    "net = SampledBlock(g1, D, 3)\n",
    "net.fit(X_train, y_train)\n",
    "out = net(X_test)\n",
    "print_name(out)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1470, 1]) out torch.float32\n",
      "tensor([[-0.3634],\n",
      "        [-0.4966],\n",
      "        [ 0.2163],\n",
      "        ...,\n",
      "        [ 0.5423],\n",
      "        [ 0.1570],\n",
      "        [ 0.1244]]) \n",
      "\n",
      "torch.Size([1470, 1]) y_test torch.float32\n",
      "tensor([[-1.0209],\n",
      "        [ 0.1168],\n",
      "        [ 1.2545],\n",
      "        ...,\n",
      "        [ 0.1168],\n",
      "        [ 0.1168],\n",
      "        [ 0.1168]]) \n",
      "\n",
      "RidgeCVModule()\n",
      "alpha 2.1544346900318834\n",
      "rmse 0.88653886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/sklearn/metrics/_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "### RidgeCV Layer ###\n",
    "#####################\n",
    "\n",
    "class RidgeCVModule(FittableModule):\n",
    "    def __init__(self, alphas=np.logspace(-1, 3, 10)):\n",
    "        \"\"\"RidgeCV layer using sklearn's RidgeCV. TODO dont use sklearn\"\"\"\n",
    "        super(RidgeCVModule, self).__init__()\n",
    "        self.ridge = RidgeCV(alphas=alphas)\n",
    "\n",
    "    def fit(self, X: Tensor, y: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"Fit the RidgeCV model. TODO dont use sklearn\"\"\"\n",
    "        X_np = X.detach().cpu().numpy().astype(np.float64)\n",
    "        y_np = y.detach().cpu().squeeze().numpy().astype(np.float64)\n",
    "        self.ridge.fit(X_np, y_np)\n",
    "        return self(X), y\n",
    "\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        \"\"\"Forward pass through the RidgeCV model. TODO dont use sklearn\"\"\"\n",
    "        X_np = X.detach().cpu().numpy().astype(np.float64)\n",
    "        y_pred_np = self.ridge.predict(X_np)\n",
    "        return torch.tensor(y_pred_np, dtype=X.dtype, device=X.device).unsqueeze(1) #TODO unsqueeze???\n",
    "\n",
    "\n",
    "D = X_train.shape[1]\n",
    "g1 = torch.Generator()\n",
    "net = RidgeCVModule()\n",
    "net.fit(X_train, y_train)\n",
    "out = net(X_test)\n",
    "print_name(out)\n",
    "print_name(y_test)\n",
    "print(net)\n",
    "print(\"alpha\", net.ridge.alpha_)\n",
    "print(\"rmse\", mean_squared_error(y_test.detach().cpu().numpy(), out.detach().cpu().numpy(), squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8281, -0.5969, -0.3491,  ..., -0.4852, -0.1246,  0.0519],\n",
      "        [-0.0388,  0.5599,  0.1400,  ...,  0.0787, -0.1044, -0.1064],\n",
      "        [ 0.9605, -0.2854, -0.0918,  ..., -0.6064, -0.0846,  0.0346],\n",
      "        ...,\n",
      "        [ 1.0064, -1.2030,  0.0122,  ..., -1.2521, -0.1361,  0.4100],\n",
      "        [ 1.1758,  0.3042,  0.3017,  ..., -0.2136,  0.3592, -0.0513],\n",
      "        [ 0.2833,  0.4097,  0.0871,  ..., -0.0392, -0.1120, -0.0608]])\n",
      "tensor([[-0.7473],\n",
      "        [ 0.6416],\n",
      "        [-0.0361],\n",
      "        ...,\n",
      "        [-0.8562],\n",
      "        [ 0.1393],\n",
      "        [-0.9555]])\n",
      "torch.Size([1470, 1]) out torch.float32\n",
      "tensor([[-0.3910],\n",
      "        [-0.4883],\n",
      "        [ 0.1868],\n",
      "        ...,\n",
      "        [ 0.5399],\n",
      "        [ 0.2303],\n",
      "        [ 0.0784]]) \n",
      "\n",
      "tensor([[-1.0209],\n",
      "        [ 0.1168],\n",
      "        [ 1.2545],\n",
      "        ...,\n",
      "        [ 0.1168],\n",
      "        [ 0.1168],\n",
      "        [ 0.1168]])\n",
      "SampledResNet(\n",
      "  (upsample): PairSampledLinear(\n",
      "    (dense): Linear(in_features=11, out_features=33, bias=True)\n",
      "  )\n",
      "  (blocks): ModuleList()\n",
      "  (output_layer): RidgeCVModule()\n",
      ")\n",
      "rmse 0.77995515\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "class SampledResNet(ResNetBase):\n",
    "    def __init__(self,\n",
    "                 generator: torch.Generator,\n",
    "                 in_dim: int,\n",
    "                 hidden_dim: int,\n",
    "                 activation_dim: int, #rename to bottleneck dim?\n",
    "                 n_blocks: int,\n",
    "                 activation: nn.Module = nn.Tanh(),\n",
    "                 upsample_method: Literal['dense', 'sampled', 'identity'] = 'dense',\n",
    "                 sampling_method: Literal['uniform', 'gradient'] = 'gradient'\n",
    "                 ):\n",
    "        \"\"\"A ResNet with sampled layers as bottleneck layers.\n",
    "        \"\"\"\n",
    "        if upsample_method==\"dense\":\n",
    "            upsample = Dense(generator, in_dim, hidden_dim)\n",
    "        elif upsample_method==\"sampled\":\n",
    "            upsample = PairSampledLinear(generator, in_dim, hidden_dim, sampling_method)\n",
    "        elif upsample_method==\"identity\":\n",
    "            upsample = Identity()\n",
    "        else:\n",
    "            raise ValueError(f\"upsample_method must be 'dense', 'sampled' or 'identity'. Given: {upsample_method}\")\n",
    "\n",
    "        blocks = [SampledBlock(generator, \n",
    "                               hidden_dim, \n",
    "                               activation_dim,\n",
    "                               activation,\n",
    "                               sampling_method\n",
    "                               ) for _ in range(n_blocks)]\n",
    "        ridge = RidgeCVModule()\n",
    "        super(SampledResNet, self).__init__(upsample, blocks, ridge)\n",
    "\n",
    "\n",
    "D = X_train.shape[1]\n",
    "g1 = torch.Generator().manual_seed(int(time.time()*10))\n",
    "net = SampledResNet(g1, D, 3*D, 2*D, 0, upsample_method='sampled', sampling_method='uniform')\n",
    "net.fit(X_train, y_train)\n",
    "out = net(X_test)\n",
    "print_name(out)\n",
    "print(y_test)\n",
    "print(net)\n",
    "print(\"rmse\", mean_squared_error(y_test.detach().cpu().numpy(), out.detach().cpu().numpy()))\n",
    "print(net.output_layer.ridge.alpha_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWIM tabular model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Union, Any, Optional, Dict, Set, Literal, Callable\n",
    "from abc import ABC, abstractmethod\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.lax as lax\n",
    "from jaxtyping import Array, Float, Int, PRNGKeyArray\n",
    "\n",
    "\n",
    "def init_single_SWIM_layer(\n",
    "        generator: torch.Generator,\n",
    "        X: Tensor,\n",
    "        y: Tensor,\n",
    "        hidden_size: int,\n",
    "        sampling_method: Literal[\"uniform\", \"gradient\"] = \"gradient\",\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Fits the weights for a single layer of the SWIM model.\n",
    "\n",
    "    Args:\n",
    "        generator (torch.Generator): PRNG object.\n",
    "        X (Tensor): Previous layer's output, shape (N, d).\n",
    "        y (Tensor): Target training data, shape (N, p).\n",
    "        hidden_size (int): Next hidden layer size.\n",
    "        sampling_method (str): Uniform or gradient-weighted pair sampling.\n",
    "    Returns:\n",
    "        Weights (d, hidden_size) and biases (1, hidden_size) for the next layer.\n",
    "    \"\"\"\n",
    "    N, d = X.shape\n",
    "    EPS = 0.1\n",
    "\n",
    "    #obtain pair indices\n",
    "    n = 5*N\n",
    "    idx1 = jnp.arange(0, n) % N\n",
    "    delta = torch.randint(low=1, high=N, shape=(n,), generator=generator)\n",
    "    idx2 = (idx1 + delta) % N\n",
    "    dx = X[idx2] - X[idx1]\n",
    "    dists = torch.linalg.norm(dx, axis=1, keepdims=True)\n",
    "    dists = torch.maximum(EPS, dists)\n",
    "    \n",
    "    if sampling_method==\"gradient\":\n",
    "        #calculate 'gradients'\n",
    "        dy = y[idx2] - y[idx1]\n",
    "        y_norm = torch.linalg.norm(dy, axis=1, keepdims=True)\n",
    "        grad = (y_norm / dists).reshape(-1) #NOTE 2023 paper uses ord=inf instead of ord=2\n",
    "        p = grad/grad.sum()\n",
    "    elif sampling_method==\"uniform\":\n",
    "        p = None\n",
    "    else:\n",
    "        raise ValueError(f\"sampling_method must be 'uniform' or 'gradient'. Given: {sampling_method}\")\n",
    "\n",
    "    #sample pairs\n",
    "    selected_idx = torch.multinomial(\n",
    "        p,\n",
    "        hidden_size,\n",
    "        replacement=True,\n",
    "        generator=generator\n",
    "        )\n",
    "    idx1 = idx1[selected_idx]\n",
    "    dx = dx[selected_idx]\n",
    "    dists = dists[selected_idx]\n",
    "\n",
    "    #define weights and biases\n",
    "    weights = (dx / dists**2).T\n",
    "    biases = -torch.sum(weights * X[idx1].T, axis=0, keepdims=True) - 0.5  # NOTE experiment with this. also +-0.5 ?\n",
    "    return weights, biases\n",
    "\n",
    "\n",
    "\n",
    "def forward_1_layer(\n",
    "        X: Tensor, # shape (N, d)\n",
    "        weights: Tensor, # shape (d, hidden_size)\n",
    "        biases: Tensor, # shape (1, hidden_size)\n",
    "        add_residual: bool,\n",
    "        activation: Callable,\n",
    "        scaling_factor: float = 1.0,\n",
    "    ) -> Float[Array, \"N  n_features\"]:\n",
    "    \"\"\"\n",
    "    Forward pass for a single layer of the SWIM model.\n",
    "    \"\"\"\n",
    "    d, D = weights.shape\n",
    "    X1 = activation(X @ weights + biases)\n",
    "    if add_residual and d==D:\n",
    "        print(\"residual\")\n",
    "        return scaling_factor*X1 + X\n",
    "    else:\n",
    "        print(\"not residual\")\n",
    "        return X1\n",
    "\n",
    "\n",
    "\n",
    "def SWIM_all_layers(\n",
    "        generator: torch.Generator,\n",
    "        X0: Tensor, # shape (N, d)\n",
    "        y: Tensor, #shape (N, p)\n",
    "        hidden_size: int,\n",
    "        activation: Callable,\n",
    "        n_layers: int,\n",
    "        add_residual: bool,\n",
    "        residual_scaling_factor: float = 1.0,\n",
    "        sampling_method: Literal[\"uniform\", \"gradient\"] = \"gradient\",\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Fits the weights for the SWIM model, iteratively layer by layer\n",
    "\n",
    "    Args:\n",
    "        generator (torch.Generator): PRNG object.\n",
    "        X0 (Float[Array, \"N  d\"]): First layer input.\n",
    "        y (Float[Array, \"N  p\"]): Target training data.\n",
    "        hidden_size (int): Hidden layer size.\n",
    "        activation (Callable): Activation function for the network.\n",
    "        n_layers (int): Number of layers in the network.\n",
    "        add_residual (bool): Whether to use residual connections.\n",
    "        residual_scaling_factor (float): Scaling factor for the residual connections.\n",
    "        sampling_method (str): Uniform or gradient-weighted pair sampling for weight initialization.\n",
    "\n",
    "    Returns:\n",
    "        Weights (d, n_features) and biases (1, n_features) for the next layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def scan_body(carry, seed): # (carry, x) -> (carry, y)\n",
    "        X, y = carry\n",
    "        w, b = init_single_SWIM_layer(seed, X, y, n_features, sampling_method)\n",
    "        X = forward_1_layer(X, w, b, add_residual, activation, residual_scaling_factor)\n",
    "        return (X, y), (w, b)\n",
    "\n",
    "    init_carry = (X0, y)\n",
    "    carry, WaB = lax.scan(\n",
    "        scan_body,\n",
    "        init_carry,\n",
    "        xs=jax.random.split(seed, n_layers),\n",
    "    )\n",
    "    return WaB\n",
    "\n",
    "\n",
    "\n",
    "class SWIM_MLP():\n",
    "    def __init__(\n",
    "            self,\n",
    "            seed: PRNGKeyArray,\n",
    "            n_features: int = 512,\n",
    "            n_layers: int = 3,\n",
    "            add_residual: bool = False,\n",
    "            sampling_method: Literal[\"uniform\", \"gradient-weighted\"] = \"gradient-weighted\",\n",
    "            activation = lambda x : jnp.maximum(0,x+0.5), # jnp.tanh,\n",
    "            residual_scaling_factor: Optional[float] = None,\n",
    "        ):\n",
    "        \"\"\"Implementation of the original paper's SWIM model\n",
    "        https://gitlab.com/felix.dietrich/swimnetworks-paper/,\n",
    "        but with support for residual connections.\n",
    "\n",
    "        Args:\n",
    "            seed (PRNGKeyArray): Random seed for matrices, biases, initial value.\n",
    "            n_features (int): Hidden layer dimension.\n",
    "            n_layers (int): Number of layers in the network.\n",
    "            add_residual (bool): Whether to use residual connections.\n",
    "            sampling_method (str): Uniform or gradient-weighted pair sampling for weight initialization.\n",
    "            activation (Callable): Activation function for the network.\n",
    "            residual_scaling_factor (Optional[float]): Scaling factor for the residual skip connections.\n",
    "        \"\"\"\n",
    "        self.n_features = n_features\n",
    "        self.n_layers = n_layers\n",
    "        self.seed = seed\n",
    "        self.add_residual = add_residual\n",
    "        self.sampling_method = sampling_method\n",
    "        self.activation = activation\n",
    "        self.w1 = None\n",
    "        self.b1 = None\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "        self.residual_scaling_factor = 1/self.n_layers if residual_scaling_factor is None else residual_scaling_factor\n",
    "\n",
    "\n",
    "    def fit(\n",
    "            self, \n",
    "            X: Float[Array, \"N  D\"], \n",
    "            y: Float[Array, \"N  p\"]\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initializes MLP weights and biases, using SWIM algorithm.\n",
    "\n",
    "        Args:\n",
    "            X (Float[Array, \"N  D\"]): Input training data.\n",
    "            y (Float[Array, \"N  p\"]): Target training data.\n",
    "        \"\"\"\n",
    "        # Get shape, dtype\n",
    "        seed1, seedrest = jax.random.split(self.seed, 2)\n",
    "\n",
    "        #first do first layer, which cannot always be done in a scan loop\n",
    "        self.w1, self.b1 = init_single_SWIM_layer(\n",
    "            seed1, X, y, self.n_features, self.sampling_method\n",
    "            )\n",
    "        X = forward_1_layer(X, self.w1, self.b1, self.add_residual)\n",
    "        \n",
    "        #rest of the layers\n",
    "        if self.n_layers > 1:\n",
    "            self.weights, self.biases = SWIM_all_layers(\n",
    "                seedrest, X, y, self.n_features, self.activation, self.n_layers-1, \n",
    "                self.add_residual, self.residual_scaling_factor, self.sampling_method\n",
    "                )\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self, X: Float[Array, \"N  D\"], only_last=True) -> Float[Array, \"N  n_features\"]:\n",
    "        #First hidden layer\n",
    "        X = forward_1_layer(X, self.w1, self.b1, self.add_residual, self.activation, self.residual_scaling_factor)\n",
    "        if self.n_layers == 1:\n",
    "            return X\n",
    "        #subsequent layers in a scan loop\n",
    "        else:\n",
    "            def scan_body(carry, inputs):\n",
    "                X = carry\n",
    "                w, b = inputs\n",
    "                return forward_1_layer(X, w, b, self.add_residual, self.activation, self.residual_scaling_factor), X #TODO temporarily return all laters\n",
    "\n",
    "            last_X, stacked_X = lax.scan(scan_body, X, xs=(self.weights, self.biases))\n",
    "            if only_last:\n",
    "                return last_X\n",
    "            else:\n",
    "                return jnp.concat([stacked_X, last_X[None]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-18 16:51:30.330419: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unable to initialize backend 'cuda': FAILED_PRECONDITION: No visible GPU devices. (you may need to uninstall the failing plugin package, or set JAX_PLATFORMS=cpu to skip this backend.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/jax/_src/xla_bridge.py:935\u001b[0m, in \u001b[0;36m_init_backend\u001b[0;34m(platform)\u001b[0m\n\u001b[1;32m    934\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitializing backend \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m, platform)\n\u001b[0;32m--> 935\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[43mregistration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfactory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;66;03m# TODO(skye): consider raising more descriptive errors directly from backend\u001b[39;00m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;66;03m# factories instead of returning None.\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/jax/_src/xla_bridge.py:646\u001b[0m, in \u001b[0;36mregister_plugin.<locals>.factory\u001b[0;34m()\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m distributed\u001b[38;5;241m.\u001b[39mglobal_state\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 646\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mxla_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_c_api_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplugin_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdated_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    648\u001b[0m distribute_options \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode_id\u001b[39m\u001b[38;5;124m'\u001b[39m: distributed\u001b[38;5;241m.\u001b[39mglobal_state\u001b[38;5;241m.\u001b[39mprocess_id,\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_nodes\u001b[39m\u001b[38;5;124m'\u001b[39m: distributed\u001b[38;5;241m.\u001b[39mglobal_state\u001b[38;5;241m.\u001b[39mnum_processes,\n\u001b[1;32m    651\u001b[0m }\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/jaxlib/xla_client.py:198\u001b[0m, in \u001b[0;36mmake_c_api_client\u001b[0;34m(plugin_name, options, distributed_client)\u001b[0m\n\u001b[1;32m    197\u001b[0m   options \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_xla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_c_api_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplugin_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistributed_client\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: FAILED_PRECONDITION: No visible GPU devices.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 57\u001b[0m\n\u001b[1;32m     53\u001b[0m         plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28mprint\u001b[39m(feat_test\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 57\u001b[0m \u001b[43mneuron_distribution_for_each_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 17\u001b[0m, in \u001b[0;36mneuron_distribution_for_each_layer\u001b[0;34m(X_train, y_train, X_test, hidden_size, n_layers)\u001b[0m\n\u001b[1;32m     13\u001b[0m train_layers\u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     14\u001b[0m test_layers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m SWIM_MLP(\n\u001b[0;32m---> 17\u001b[0m     \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPRNGKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m, \n\u001b[1;32m     18\u001b[0m     hidden_size, \n\u001b[1;32m     19\u001b[0m     n_layers,\n\u001b[1;32m     20\u001b[0m     add_residual\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     21\u001b[0m     sampling_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgradient-weighted\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m#activation = jnp.tanh,\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m#residual_scaling_factor=1.0,\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \n\u001b[1;32m     25\u001b[0m     )\n\u001b[1;32m     27\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     29\u001b[0m feat_test  \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(X_train, only_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mreshape(n_layers, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/jax/_src/random.py:241\u001b[0m, in \u001b[0;36mPRNGKey\u001b[0;34m(seed, impl)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mPRNGKey\u001b[39m(seed: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    225\u001b[0m             impl: PRNGSpecDesc \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m KeyArray:\n\u001b[1;32m    226\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Create a pseudo-random number generator (PRNG) key given an integer seed.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m  The resulting key carries the default PRNG implementation, as\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m    and ``fold_in``.\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _return_prng_keys(\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43m_key\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPRNGKey\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimpl\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/jax/_src/random.py:203\u001b[0m, in \u001b[0;36m_key\u001b[0;34m(ctor_name, seed, impl_spec)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(seed):\n\u001b[1;32m    200\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    201\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mctor_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m accepts a scalar seed, but was given an array of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    202\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mshape(seed)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != (). Use jax.vmap for batching\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprng\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimpl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/jax/_src/prng.py:634\u001b[0m, in \u001b[0;36mrandom_seed\u001b[0;34m(seeds, impl)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrandom_seed\u001b[39m(seeds: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m typing\u001b[38;5;241m.\u001b[39mArrayLike, impl: PRNGImpl) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PRNGKeyArray:\n\u001b[1;32m    630\u001b[0m   \u001b[38;5;66;03m# Avoid overflow error in X32 mode by first converting ints to int64.\u001b[39;00m\n\u001b[1;32m    631\u001b[0m   \u001b[38;5;66;03m# This breaks JIT invariance for large ints, but supports the common\u001b[39;00m\n\u001b[1;32m    632\u001b[0m   \u001b[38;5;66;03m# use-case of instantiating with Python hashes in X32 mode.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(seeds, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 634\u001b[0m     seeds_arr \u001b[38;5;241m=\u001b[39m \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseeds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    636\u001b[0m     seeds_arr \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39masarray(seeds)\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:2242\u001b[0m, in \u001b[0;36masarray\u001b[0;34m(a, dtype, order, copy)\u001b[0m\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2241\u001b[0m   dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mcanonicalize_dtype(dtype, allow_extended_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m-> 2242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:2197\u001b[0m, in \u001b[0;36marray\u001b[0;34m(object, dtype, copy, order, ndmin)\u001b[0m\n\u001b[1;32m   2194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2195\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected input type for array: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2197\u001b[0m out_array: Array \u001b[38;5;241m=\u001b[39m \u001b[43mlax_internal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_element_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweak_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweak_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ndmin \u001b[38;5;241m>\u001b[39m ndim(out_array):\n\u001b[1;32m   2200\u001b[0m   out_array \u001b[38;5;241m=\u001b[39m lax\u001b[38;5;241m.\u001b[39mexpand_dims(out_array, \u001b[38;5;28mrange\u001b[39m(ndmin \u001b[38;5;241m-\u001b[39m ndim(out_array)))\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/jax/_src/lax/lax.py:558\u001b[0m, in \u001b[0;36m_convert_element_type\u001b[0;34m(operand, new_dtype, weak_type)\u001b[0m\n\u001b[1;32m    556\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m type_cast(Array, operand)\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 558\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_element_type_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mweak_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mweak_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/jax/_src/core.py:422\u001b[0m, in \u001b[0;36mPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m    420\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39menable_checks\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    421\u001b[0m           \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(arg, Tracer) \u001b[38;5;129;01mor\u001b[39;00m valid_jaxtype(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args)), args\n\u001b[0;32m--> 422\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfind_top_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/jax/_src/core.py:425\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[0;32m--> 425\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_raise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    426\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/jax/_src/core.py:913\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_primitive\u001b[39m(\u001b[38;5;28mself\u001b[39m, primitive, tracers, params):\n\u001b[0;32m--> 913\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprimitive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtracers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/jax/_src/dispatch.py:87\u001b[0m, in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m     85\u001b[0m prev \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mjax_jit\u001b[38;5;241m.\u001b[39mswap_thread_local_state_disable_jit(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m   outs \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m   lib\u001b[38;5;241m.\u001b[39mjax_jit\u001b[38;5;241m.\u001b[39mswap_thread_local_state_disable_jit(prev)\n",
      "    \u001b[0;31m[... skipping hidden 16 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.10/site-packages/jax/_src/xla_bridge.py:869\u001b[0m, in \u001b[0;36mbackends\u001b[0;34m()\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    868\u001b[0m         err_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (you may need to uninstall the failing plugin package, or set JAX_PLATFORMS=cpu to skip this backend.)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 869\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(err_msg)\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m _default_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config\u001b[38;5;241m.\u001b[39mjax_platforms\u001b[38;5;241m.\u001b[39mvalue:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unable to initialize backend 'cuda': FAILED_PRECONDITION: No visible GPU devices. (you may need to uninstall the failing plugin package, or set JAX_PLATFORMS=cpu to skip this backend.)"
     ]
    }
   ],
   "source": [
    "def neuron_distribution_for_each_layer(\n",
    "        X_train: Array,\n",
    "        y_train: Array,\n",
    "        X_test: Array,\n",
    "        hidden_size: int,\n",
    "        n_layers: int,\n",
    "        ) -> Tuple[Array, Array]:\n",
    "    \"\"\"Looks at the distribution of neurons for each layer of a neural network model\n",
    "    (used to compare SWIM, residual sampling, and random feature networks).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the arrays to store the neuron distribution\n",
    "    train_layers= []\n",
    "    test_layers = []\n",
    "\n",
    "    model = SWIM_MLP(\n",
    "        jax.random.PRNGKey(0), \n",
    "        hidden_size, \n",
    "        n_layers,\n",
    "        add_residual=True,\n",
    "        sampling_method=\"gradient-weighted\",\n",
    "        #activation = jnp.tanh,\n",
    "        #residual_scaling_factor=1.0,\n",
    "        \n",
    "        )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    feat_test  = model.transform(X_train, only_last=False).reshape(n_layers, -1)\n",
    "    feat_train = model.transform(X_test, only_last=False).reshape(n_layers, -1)\n",
    "    \n",
    "    print(feat_test[1]-feat_test[1])\n",
    "\n",
    "    #features are shape (n_layers, n_samples, n_features)\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(X_train.flatten(), bins=50, alpha=0.5, label='Train', density=True)\n",
    "    plt.hist(X_test.flatten(), bins=50, alpha=0.5, label='Test', density=True)\n",
    "    plt.title('Input Data Distribution')\n",
    "    plt.xlabel('Input Feature Value')\n",
    "    plt.ylabel('Probability Density')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    for layer in range(n_layers):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(feat_train[layer], bins=50, alpha=0.5, label='Train', density=True)\n",
    "        plt.hist(feat_test[layer], bins=50, alpha=0.5, label='Test', density=True)\n",
    "        plt.title(f'Layer {layer + 1} Neuron Distribution')\n",
    "        plt.xlabel('Neuron Activation')\n",
    "        plt.ylabel('Probability Density')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    print(feat_test.shape)\n",
    "\n",
    "neuron_distribution_for_each_layer(X_train, y_train, X_test, 128, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to look at the distribution of weights (eigenvalues? absolute values of rows? distribution of (assuming iid) matrix entries?)\n",
    "\n",
    "distribution of neurons at each layer\n",
    "\n",
    "This is for both SWIM, Residual SWIM, random features, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
