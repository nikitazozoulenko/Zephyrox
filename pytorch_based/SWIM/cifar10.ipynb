{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Union, Any, Optional, Dict, Literal, Callable\n",
    "import time\n",
    "import collections\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor, tensor\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from preprocessing.stream_transforms import normalize_mean_std_traindata, normalize_streams, augment_time, add_basepoint_zero\n",
    "from utils.utils import print_name, print_shape\n",
    "from models import ResNet, NeuralEulerODE, RidgeClassifierCVModule, E2EResNet, LogisticRegressionModule, RandFeatBoost\n",
    "\n",
    "np.set_printoptions(precision=3, threshold=5) # Print options\n",
    "device = \"cuda\" #torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data_loader, model, device):\n",
    "    \"\"\"Function to extract features from a dataset using pre-train model\"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, target in tqdm(data_loader):\n",
    "            images = images.to(device)\n",
    "            output = model(images)\n",
    "            output = output.view(output.size(0), -1)\n",
    "            features.append(output.cpu().numpy())\n",
    "            labels.append(target.cpu().numpy())\n",
    "    \n",
    "    features = np.concatenate(features, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def load_cifar10(\n",
    "        data_path = \"/home/nikita/hdd/cifar10/\",\n",
    "        train_name = \"resnet18_train_features.csv\",\n",
    "        test_name = \"resnet18_test_features.csv\",\n",
    "        ):\n",
    "    \"\"\"Loads a pretrained ResNet18 model and extracts features from the CIFAR-10 dataset\"\"\"\n",
    "    # see if data has already been processed\n",
    "    if train_name in os.listdir(data_path) and \\\n",
    "            test_name in os.listdir(data_path):\n",
    "        print(\"Loading preprocessed data\")\n",
    "        train_df = pd.read_csv(data_path + train_name)\n",
    "        test_df = pd.read_csv(data_path + test_name)\n",
    "        return train_df.to_numpy(), test_df.to_numpy()\n",
    "\n",
    "    # Define the DataLoaders and transformations for the CIFAR-10 dataset\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(224),  # Resize images to 224x224 as required by ResNet18\n",
    "        transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # Normalize with ImageNet mean and std\n",
    "    ])\n",
    "    train_dataset = torchvision.datasets.CIFAR10(root=data_path, train=True, download=True, transform=transform)\n",
    "    test_dataset = torchvision.datasets.CIFAR10(root=data_path, train=False, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "    # Load the pre-trained ResNet18 model, remove classification head\n",
    "    model = resnet18(weights=True)\n",
    "    model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Extract features for the training and test datasets\n",
    "    train_features, train_labels = extract_features(train_loader, model, device)\n",
    "    test_features, test_labels = extract_features(test_loader, model, device)\n",
    "\n",
    "    # Create a DataFrame to store the features and labels, save to CSV\n",
    "    train_df = pd.DataFrame(train_features)\n",
    "    train_df['target'] = train_labels\n",
    "    test_df = pd.DataFrame(test_features)\n",
    "    test_df['target'] = test_labels\n",
    "    train_df.to_csv(data_path + train_name, index=False)\n",
    "    test_df.to_csv(data_path + test_name, index=False)\n",
    "    return train_df.to_numpy(), test_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data\n",
      "Train data shape: torch.Size([50000, 512]), dtype: torch.float32\n",
      "Train labels shape: torch.Size([50000, 10]), dtype: torch.float32\n",
      "Test data shape: torch.Size([10000, 512]), dtype: torch.float32\n",
      "Test labels shape: torch.Size([10000, 10]), dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "train, test = load_cifar10()\n",
    "\n",
    "X_train = torch.from_numpy(train[:, :-1].astype(np.float32)).to(device)\n",
    "y_train_cat = torch.from_numpy(train[:, -1].astype(np.int64)).to(device)\n",
    "X_test = torch.from_numpy(test[:, :-1].astype(np.float32)).to(device)\n",
    "y_test_cat = torch.from_numpy(test[:, -1].astype(np.int64)).to(device)\n",
    "\n",
    "# Convert train and test labels to one-hot encoding\n",
    "y_train = F.one_hot(y_train_cat, num_classes=10).float()\n",
    "y_test = F.one_hot(y_test_cat, num_classes=10).float()\n",
    "print(f\"Train data shape: {X_train.shape}, dtype: {X_train.dtype}\")\n",
    "print(f\"Train labels shape: {y_train.shape}, dtype: {y_train.dtype}\")\n",
    "print(f\"Test data shape: {X_test.shape}, dtype: {X_test.dtype}\")\n",
    "print(f\"Test labels shape: {y_test.shape}, dtype: {y_test.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_allmodels_1dataset(\n",
    "        generator: torch.Generator,\n",
    "        X_train: Tensor,\n",
    "        y_train: Tensor,\n",
    "        X_test: Tensor,\n",
    "        y_test: Tensor,\n",
    "        ):\n",
    "    \n",
    "    D = X_train.shape[1]\n",
    "    hidden_size = 512\n",
    "    bottleneck_dim = 1*hidden_size\n",
    "    num_epochs = 40\n",
    "    batch_size = 512\n",
    "    adam_lr = 0.01\n",
    "    \n",
    "    # (name, model, kwargs). kwargs separate to save memory\n",
    "    model_list = [\n",
    "        [\"T=10 RandFeatureBoost\", RandFeatBoost,\n",
    "                {\"generator\": generator,\n",
    "                 \"in_dim\": D,\n",
    "                 \"hidden_size\": hidden_size,\n",
    "                 \"out_dim\": 10,\n",
    "                 \"n_blocks\": 9,\n",
    "                 \"activation\": nn.Tanh(),\n",
    "                 \"loss_fn\": nn.CrossEntropyLoss(),\n",
    "                 \"adam_lr\": adam_lr,\n",
    "                 \"boost_lr\": 1.0,\n",
    "                 \"epochs\": num_epochs,\n",
    "                 \"batch_size\": batch_size,\n",
    "                 \"upscale_type\": \"SWIM\",  # \"dense\", \"identity\"\n",
    "                 }],\n",
    "\n",
    "        [\"Tabular RidgeClassifier\", RidgeClassifierCVModule, {}],\n",
    "\n",
    "        [\"Logistic Regression\", LogisticRegressionModule, \n",
    "                {\"generator\": generator,\n",
    "                 \"num_epochs\": num_epochs,\n",
    "                 \"batch_size\": batch_size,\n",
    "                 \"lr\": adam_lr,\n",
    "                 }],\n",
    "\n",
    "        [\"T=1 Dense\", ResNet,\n",
    "                {\"generator\": generator,\n",
    "                 \"in_dim\": D,\n",
    "                 \"hidden_size\": hidden_size,\n",
    "                 \"bottleneck_dim\": None,\n",
    "                 \"n_blocks\": 0,\n",
    "                 \"upsample_layer\": \"dense\",\n",
    "                 \"output_layer\": \"logistic regression\",\n",
    "                 }],\n",
    "\n",
    "        [\"T=1 SWIM Grad\", ResNet,\n",
    "                {\"generator\": generator,\n",
    "                \"in_dim\": D,\n",
    "                \"hidden_size\": hidden_size,\n",
    "                \"bottleneck_dim\": None,\n",
    "                \"n_blocks\": 0,\n",
    "                \"upsample_layer\": \"SWIM\",\n",
    "                \"output_layer\": \"logistic regression\",\n",
    "                }],\n",
    "    ]\n",
    "\n",
    "    for n_blocks in [3]:\n",
    "        model_list += [\n",
    "        [f\"T={n_blocks+1} End2End\", E2EResNet,\n",
    "                {\"generator\": generator,\n",
    "                \"in_dim\": D,\n",
    "                \"hidden_size\": hidden_size,\n",
    "                \"bottleneck_dim\": bottleneck_dim,\n",
    "                \"out_dim\": 10,\n",
    "                \"n_blocks\": n_blocks,\n",
    "                \"activation\": nn.Tanh(),\n",
    "                \"loss\": nn.CrossEntropyLoss(),\n",
    "                \"lr\": adam_lr,\n",
    "                \"epochs\": num_epochs,\n",
    "                \"batch_size\": batch_size}\n",
    "                ],\n",
    "\n",
    "        [f\"T={n_blocks+1} ResSWIM Grad-dense\", ResNet,\n",
    "                {\"generator\": generator,\n",
    "                \"in_dim\": D,\n",
    "                \"hidden_size\": hidden_size,\n",
    "                \"bottleneck_dim\": bottleneck_dim,\n",
    "                \"n_blocks\": n_blocks,\n",
    "                \"upsample_layer\": \"SWIM\",\n",
    "                \"res_layer1\": \"SWIM\",\n",
    "                \"res_layer2\": \"dense\",\n",
    "                \"output_layer\": \"logistic regression\",\n",
    "                }\n",
    "                ],\n",
    "\n",
    "        [f\"T={n_blocks+1} ResSWIM Grad-id\", ResNet,\n",
    "                {\"generator\": generator,\n",
    "                \"in_dim\": D,\n",
    "                \"hidden_size\": hidden_size,\n",
    "                \"bottleneck_dim\": hidden_size,\n",
    "                \"n_blocks\": n_blocks,\n",
    "                \"upsample_layer\": \"SWIM\",\n",
    "                \"res_layer1\": \"SWIM\",\n",
    "                \"res_layer2\": \"identity\",\n",
    "                \"output_layer\": \"logistic regression\",\n",
    "                }\n",
    "                ],\n",
    "\n",
    "        [f\"T={n_blocks+1} ResDense\", ResNet,\n",
    "                {\"generator\": generator,\n",
    "                \"in_dim\": D,\n",
    "                \"hidden_size\": hidden_size,\n",
    "                \"bottleneck_dim\": hidden_size,\n",
    "                \"n_blocks\": n_blocks,\n",
    "                \"upsample_layer\": \"dense\",\n",
    "                \"res_layer1\": \"dense\",\n",
    "                \"res_layer2\": \"identity\",\n",
    "                \"output_layer\": \"logistic regression\",\n",
    "                }\n",
    "                ],\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    model_names = []\n",
    "    for name, model, model_args in model_list:\n",
    "        print(name)\n",
    "        t0 = time.perf_counter()\n",
    "        model = model(**model_args).to(X_train.device)\n",
    "        pred_train, _ = model.fit(X_train, y_train)\n",
    "        t1 = time.perf_counter()\n",
    "        pred_test = model(X_test)\n",
    "        t2 = time.perf_counter()\n",
    "        \n",
    "        #convert to class predictions:\n",
    "        if len(pred_train.shape) == 2:\n",
    "            pred_train = torch.argmax(pred_train, dim=1)\n",
    "            pred_test = torch.argmax(pred_test, dim=1)\n",
    "        acc_train = (pred_train == y_train_cat).float().mean().item()\n",
    "        acc_test = (pred_test == y_test_cat).float().mean().item()\n",
    "\n",
    "        result = np.array( [acc_train, acc_test, t1-t0, t2-t1] )\n",
    "        results.append( result )\n",
    "        model_names.append( name )\n",
    "\n",
    "    return model_names, results\n",
    "\n",
    "\n",
    "\n",
    "def run_all_experiments(\n",
    "        name_save: str = \"PLACEHOLDER\",\n",
    "        ):\n",
    "    # Fetch and process each dataset\n",
    "    experiments = {}\n",
    "    generator = torch.Generator(device=device).manual_seed(999)\n",
    "    results = run_allmodels_1dataset(\n",
    "        generator, X_train, y_train, X_test, y_test, \n",
    "        )\n",
    "    experiments[\"MNIST\"] = results\n",
    "\n",
    "    # Save results\n",
    "    # Assuming experiments is a dict where keys are dataset names and values are tuples (model_names, results)\n",
    "    attributes = [\"acc_train\", \"acc_test\", \"t_fit\", \"t_feat\"]\n",
    "    data_list = []\n",
    "    # Process the data\n",
    "    for dataset_name, (model_names, results) in experiments.items():\n",
    "        dataset_data = {}\n",
    "        for attr_idx, attribute in enumerate(attributes):\n",
    "            for model_idx, model_name in enumerate(model_names):\n",
    "                dataset_data[(attribute, model_name)] = results[model_idx][attr_idx]\n",
    "        data_list.append(pd.DataFrame(dataset_data, index=[dataset_name]))\n",
    "\n",
    "    # Combine all datasets into a single DataFrame\n",
    "    df = pd.concat(data_list)\n",
    "    df = df.sort_index(axis=1)\n",
    "    print(df)\n",
    "    df.to_pickle(f\"cifar10_{name_save}.pkl\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T=10 RandFeatureBoost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:24<00:00,  1.62it/s]\n",
      "100%|██████████| 40/40 [00:23<00:00,  1.68it/s]\n",
      "100%|██████████| 40/40 [00:23<00:00,  1.70it/s]\n",
      "100%|██████████| 40/40 [00:22<00:00,  1.80it/s]\n",
      "100%|██████████| 40/40 [00:22<00:00,  1.80it/s]\n",
      "100%|██████████| 40/40 [00:22<00:00,  1.77it/s]\n",
      "100%|██████████| 40/40 [00:22<00:00,  1.75it/s]\n",
      "100%|██████████| 40/40 [00:22<00:00,  1.75it/s]\n",
      "100%|██████████| 40/40 [00:22<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular RidgeClassifier\n",
      "Logistic Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:20<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T=1 Dense\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:15<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T=1 SWIM Grad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:15<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T=4 End2End\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:30<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T=4 ResSWIM Grad-dense\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:15<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T=4 ResSWIM Grad-id\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:15<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T=4 ResDense\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:15<00:00,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 acc_test                                                \\\n",
      "      Logistic Regression T=1 Dense T=1 SWIM Grad T=10 RandFeatureBoost   \n",
      "MNIST               0.854    0.8506         0.855                0.8578   \n",
      "\n",
      "                                                                           \\\n",
      "      T=4 End2End T=4 ResDense T=4 ResSWIM Grad-dense T=4 ResSWIM Grad-id   \n",
      "MNIST        0.87       0.8486                  0.835              0.8297   \n",
      "\n",
      "                                        acc_train  ...  \\\n",
      "      Tabular RidgeClassifier Logistic Regression  ...   \n",
      "MNIST                  0.8608             0.88682  ...   \n",
      "\n",
      "                       t_feat               t_fit                           \\\n",
      "      Tabular RidgeClassifier Logistic Regression  T=1 Dense T=1 SWIM Grad   \n",
      "MNIST                 0.03654           20.387619  15.102281     15.790536   \n",
      "\n",
      "                                                                             \\\n",
      "      T=10 RandFeatureBoost T=4 End2End T=4 ResDense T=4 ResSWIM Grad-dense   \n",
      "MNIST            208.186397   30.303527    15.640562               15.60223   \n",
      "\n",
      "                                                   \n",
      "      T=4 ResSWIM Grad-id Tabular RidgeClassifier  \n",
      "MNIST            15.41781                8.700585  \n",
      "\n",
      "[1 rows x 36 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"9\" halign=\"left\">acc_test</th>\n",
       "      <th>acc_train</th>\n",
       "      <th>...</th>\n",
       "      <th>t_feat</th>\n",
       "      <th colspan=\"9\" halign=\"left\">t_fit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>T=1 Dense</th>\n",
       "      <th>T=1 SWIM Grad</th>\n",
       "      <th>T=10 RandFeatureBoost</th>\n",
       "      <th>T=4 End2End</th>\n",
       "      <th>T=4 ResDense</th>\n",
       "      <th>T=4 ResSWIM Grad-dense</th>\n",
       "      <th>T=4 ResSWIM Grad-id</th>\n",
       "      <th>Tabular RidgeClassifier</th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>...</th>\n",
       "      <th>Tabular RidgeClassifier</th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>T=1 Dense</th>\n",
       "      <th>T=1 SWIM Grad</th>\n",
       "      <th>T=10 RandFeatureBoost</th>\n",
       "      <th>T=4 End2End</th>\n",
       "      <th>T=4 ResDense</th>\n",
       "      <th>T=4 ResSWIM Grad-dense</th>\n",
       "      <th>T=4 ResSWIM Grad-id</th>\n",
       "      <th>Tabular RidgeClassifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MNIST</th>\n",
       "      <td>0.854</td>\n",
       "      <td>0.8506</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.8578</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.8486</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.8297</td>\n",
       "      <td>0.8608</td>\n",
       "      <td>0.88682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03654</td>\n",
       "      <td>20.387619</td>\n",
       "      <td>15.102281</td>\n",
       "      <td>15.790536</td>\n",
       "      <td>208.186397</td>\n",
       "      <td>30.303527</td>\n",
       "      <td>15.640562</td>\n",
       "      <td>15.60223</td>\n",
       "      <td>15.41781</td>\n",
       "      <td>8.700585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 acc_test                                                \\\n",
       "      Logistic Regression T=1 Dense T=1 SWIM Grad T=10 RandFeatureBoost   \n",
       "MNIST               0.854    0.8506         0.855                0.8578   \n",
       "\n",
       "                                                                           \\\n",
       "      T=4 End2End T=4 ResDense T=4 ResSWIM Grad-dense T=4 ResSWIM Grad-id   \n",
       "MNIST        0.87       0.8486                  0.835              0.8297   \n",
       "\n",
       "                                        acc_train  ...  \\\n",
       "      Tabular RidgeClassifier Logistic Regression  ...   \n",
       "MNIST                  0.8608             0.88682  ...   \n",
       "\n",
       "                       t_feat               t_fit                           \\\n",
       "      Tabular RidgeClassifier Logistic Regression  T=1 Dense T=1 SWIM Grad   \n",
       "MNIST                 0.03654           20.387619  15.102281     15.790536   \n",
       "\n",
       "                                                                             \\\n",
       "      T=10 RandFeatureBoost T=4 End2End T=4 ResDense T=4 ResSWIM Grad-dense   \n",
       "MNIST            208.186397   30.303527    15.640562               15.60223   \n",
       "\n",
       "                                                   \n",
       "      T=4 ResSWIM Grad-id Tabular RidgeClassifier  \n",
       "MNIST            15.41781                8.700585  \n",
       "\n",
       "[1 rows x 36 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_all_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T=4 End2End                0.8700\n",
       "Tabular RidgeClassifier    0.8608\n",
       "T=10 RandFeatureBoost      0.8578\n",
       "T=1 SWIM Grad              0.8550\n",
       "Logistic Regression        0.8540\n",
       "T=1 Dense                  0.8506\n",
       "T=4 ResDense               0.8486\n",
       "T=4 ResSWIM Grad-dense     0.8350\n",
       "T=4 ResSWIM Grad-id        0.8297\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(\"cifar10_PLACEHOLDER.pkl\")\n",
    "df[\"acc_test\"].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T=4 End2End                0.99192\n",
       "Logistic Regression        0.88682\n",
       "T=10 RandFeatureBoost      0.87678\n",
       "T=1 Dense                  0.87458\n",
       "T=1 SWIM Grad              0.87114\n",
       "T=4 ResDense               0.87020\n",
       "Tabular RidgeClassifier    0.86964\n",
       "T=4 ResSWIM Grad-dense     0.84318\n",
       "T=4 ResSWIM Grad-id        0.83656\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"acc_train\"].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to experiment with RandFeatureBoost\n",
    "\n",
    "generator = torch.Generator(device=device).manual_seed(999)\n",
    "D = X_train.shape[1]\n",
    "hidden_size = 512\n",
    "num_epochs = 50\n",
    "batch_size = 512\n",
    "n_blocks = 20\n",
    "adam_lr=0.01\n",
    "boost_lr=1.0\n",
    "\n",
    "model = RandFeatBoost(\n",
    "    generator=generator,\n",
    "    in_dim=D,\n",
    "    hidden_size=hidden_size,\n",
    "    out_dim=10,\n",
    "    n_blocks=n_blocks,\n",
    "    activation=nn.Tanh(),\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    adam_lr=adam_lr,\n",
    "    boost_lr=boost_lr,\n",
    "    epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    upscale_type=\"SWIM\",\n",
    "    ).to(device)\n",
    "\n",
    "pred_train, _ = model.fit(X_train, y_train)\n",
    "pred_test = model(X_test)\n",
    "pred_train = torch.argmax(pred_train, dim=1)\n",
    "pred_test = torch.argmax(pred_test, dim=1)\n",
    "acc_train = (pred_train == y_train_cat).float().mean().item()\n",
    "acc_test = (pred_test == y_test_cat).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc_train)\n",
    "print(acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def batch_forward(model, X, batch_size=512):\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         pred = []\n",
    "#         for i in range(0, X.shape[0], batch_size):\n",
    "#             pred.append(model(X[i:i+batch_size]))\n",
    "#         pred = torch.cat(pred, dim=0)\n",
    "#     return pred\n",
    "\n",
    "def print_all_accuracies(X_train, X_test):\n",
    "    with torch.no_grad():\n",
    "        X_train = model.upscale(X_train)\n",
    "        X_test  = model.upscale(X_test)\n",
    "\n",
    "        for t, (layer, DELTA, classifier) in enumerate(zip(model.layers, model.deltas, model.classifiers)):\n",
    "            X_train = X_train + model.boost_lr * DELTA * (layer(X_train) - X_train)\n",
    "            X_test = X_test + model.boost_lr * DELTA * (layer(X_test) - X_test)\n",
    "            \n",
    "            pred_train = classifier(X_train)\n",
    "            pred_test = classifier(X_test)\n",
    "            pred_train = torch.argmax(pred_train, dim=1)\n",
    "            pred_test = torch.argmax(pred_test, dim=1)\n",
    "            acc_train = (pred_train == y_train_cat).float().mean().item()\n",
    "            acc_test = (pred_test == y_test_cat).float().mean().item()\n",
    "\n",
    "            print(f\"Block {t}: Train acc: {acc_train}, Test acc: {acc_test}\")\n",
    "\n",
    "\n",
    "print_all_accuracies(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#lr = 0.05?\n",
    "# Block 0: Train acc: 0.8704400062561035, Test acc: 0.8562999963760376\n",
    "# Block 1: Train acc: 0.8750399947166443, Test acc: 0.8580999970436096\n",
    "# Block 2: Train acc: 0.8756399750709534, Test acc: 0.8606999516487122\n",
    "# Block 3: Train acc: 0.8766399621963501, Test acc: 0.8583999872207642\n",
    "# Block 4: Train acc: 0.8761399984359741, Test acc: 0.859499990940094\n",
    "# Block 5: Train acc: 0.8769800066947937, Test acc: 0.8606999516487122\n",
    "# Block 6: Train acc: 0.8761999607086182, Test acc: 0.8598999977111816\n",
    "# Block 7: Train acc: 0.877020001411438, Test acc: 0.8601999878883362\n",
    "# Block 8: Train acc: 0.8753599524497986, Test acc: 0.8592999577522278\n",
    "# Block 9: Train acc: 0.8775799870491028, Test acc: 0.85999995470047\n",
    "# Block 10: Train acc: 0.8776399493217468, Test acc: 0.8608999848365784\n",
    "# Block 11: Train acc: 0.8754799962043762, Test acc: 0.8572999835014343\n",
    "# Block 12: Train acc: 0.8762399554252625, Test acc: 0.8598999977111816\n",
    "# Block 13: Train acc: 0.8745200037956238, Test acc: 0.8574000000953674\n",
    "# Block 14: Train acc: 0.8747199773788452, Test acc: 0.8578000068664551\n",
    "# Block 15: Train acc: 0.8781999945640564, Test acc: 0.8606999516487122\n",
    "# Block 16: Train acc: 0.8761000037193298, Test acc: 0.859499990940094\n",
    "# Block 17: Train acc: 0.8755199909210205, Test acc: 0.8597999811172485\n",
    "# Block 18: Train acc: 0.8772199749946594, Test acc: 0.8621000051498413\n",
    "# Block 19: Train acc: 0.8749600052833557, Test acc: 0.85999995470047"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SWIM-ID vs DENSE-ID vs SWIM-DENSE \n",
    "# implement 'finding gradient direction' gradient boosting\n",
    "\n",
    "# Test whether this is actually better than non-boost with same hidden size !!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:27<00:00,  1.79it/s]\n",
      "100%|██████████| 50/50 [00:28<00:00,  1.75it/s]\n",
      "100%|██████████| 50/50 [00:27<00:00,  1.83it/s]\n",
      "100%|██████████| 50/50 [00:28<00:00,  1.73it/s]\n",
      "100%|██████████| 50/50 [00:28<00:00,  1.78it/s]\n",
      "100%|██████████| 50/50 [00:28<00:00,  1.72it/s]\n",
      "100%|██████████| 50/50 [00:28<00:00,  1.77it/s]\n",
      "100%|██████████| 50/50 [00:28<00:00,  1.74it/s]\n",
      "100%|██████████| 50/50 [00:27<00:00,  1.84it/s]\n",
      "100%|██████████| 50/50 [00:27<00:00,  1.84it/s]\n",
      "100%|██████████| 50/50 [00:27<00:00,  1.82it/s]\n",
      "100%|██████████| 50/50 [00:27<00:00,  1.84it/s]\n",
      "100%|██████████| 50/50 [00:26<00:00,  1.85it/s]\n",
      "100%|██████████| 50/50 [00:26<00:00,  1.86it/s]\n",
      "100%|██████████| 50/50 [00:27<00:00,  1.83it/s]\n",
      "100%|██████████| 50/50 [00:27<00:00,  1.85it/s]\n",
      "100%|██████████| 50/50 [00:27<00:00,  1.81it/s]\n",
      "100%|██████████| 50/50 [00:27<00:00,  1.85it/s]\n",
      "100%|██████████| 50/50 [00:27<00:00,  1.83it/s]\n",
      "100%|██████████| 50/50 [00:27<00:00,  1.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# experiment with DENSE\n",
    "\n",
    "generator = torch.Generator(device=device).manual_seed(999)\n",
    "D = X_train.shape[1]\n",
    "hidden_size = 512\n",
    "num_epochs = 50\n",
    "batch_size = 512\n",
    "n_blocks = 20\n",
    "adam_lr=0.01\n",
    "boost_lr=1.0\n",
    "\n",
    "model = RandFeatBoost(\n",
    "    generator=generator,\n",
    "    in_dim=D,\n",
    "    hidden_size=hidden_size,\n",
    "    out_dim=10,\n",
    "    n_blocks=n_blocks,\n",
    "    activation=nn.Tanh(),\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    adam_lr=adam_lr,\n",
    "    boost_lr=boost_lr,\n",
    "    epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    upscale_type=\"dense\",\n",
    "    second_in_resblock=\"identity\",\n",
    "    ).to(device)\n",
    "\n",
    "pred_train, _ = model.fit(X_train, y_train)\n",
    "pred_test = model(X_test)\n",
    "pred_train = torch.argmax(pred_train, dim=1)\n",
    "pred_test = torch.argmax(pred_test, dim=1)\n",
    "acc_train = (pred_train == y_train_cat).float().mean().item()\n",
    "acc_test = (pred_test == y_test_cat).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 0: Train acc: 0.8825199604034424, Test acc: 0.8538999557495117\n",
      "Block 1: Train acc: 0.8873800039291382, Test acc: 0.8561999797821045\n",
      "Block 2: Train acc: 0.8834799528121948, Test acc: 0.849299967288971\n",
      "Block 3: Train acc: 0.8905400037765503, Test acc: 0.8593999743461609\n",
      "Block 4: Train acc: 0.884119987487793, Test acc: 0.8543999791145325\n",
      "Block 5: Train acc: 0.891819953918457, Test acc: 0.8586999773979187\n",
      "Block 6: Train acc: 0.8880800008773804, Test acc: 0.8535999655723572\n",
      "Block 7: Train acc: 0.8888799548149109, Test acc: 0.8560999631881714\n",
      "Block 8: Train acc: 0.8908199667930603, Test acc: 0.85589998960495\n",
      "Block 9: Train acc: 0.8820199966430664, Test acc: 0.851099967956543\n",
      "Block 10: Train acc: 0.876800000667572, Test acc: 0.8470999598503113\n",
      "Block 11: Train acc: 0.884939968585968, Test acc: 0.85589998960495\n",
      "Block 12: Train acc: 0.8805199861526489, Test acc: 0.8496999740600586\n",
      "Block 13: Train acc: 0.8872399926185608, Test acc: 0.8531000018119812\n",
      "Block 14: Train acc: 0.88673996925354, Test acc: 0.8539999723434448\n",
      "Block 15: Train acc: 0.8854199647903442, Test acc: 0.8511999845504761\n",
      "Block 16: Train acc: 0.8913599848747253, Test acc: 0.8537999987602234\n",
      "Block 17: Train acc: 0.8837599754333496, Test acc: 0.8525999784469604\n",
      "Block 18: Train acc: 0.8889199495315552, Test acc: 0.8545999526977539\n",
      "Block 19: Train acc: 0.8808599710464478, Test acc: 0.8471999764442444\n"
     ]
    }
   ],
   "source": [
    "def print_all_accuracies(X_train, X_test):\n",
    "    X_train = model.upscale(X_train)\n",
    "    X_test  = model.upscale(X_test)\n",
    "\n",
    "    for t, (layer, DELTA, classifier) in enumerate(zip(model.layers, model.deltas, model.classifiers)):\n",
    "        X_train = X_train + model.boost_lr * DELTA * (layer(X_train) - X_train)\n",
    "        X_test = X_test + model.boost_lr * DELTA * (layer(X_test) - X_test)\n",
    "        \n",
    "        pred_train = classifier(X_train)\n",
    "        pred_test = classifier(X_test)\n",
    "        pred_train = torch.argmax(pred_train, dim=1)\n",
    "        pred_test = torch.argmax(pred_test, dim=1)\n",
    "        acc_train = (pred_train == y_train_cat).float().mean().item()\n",
    "        acc_test = (pred_test == y_test_cat).float().mean().item()\n",
    "\n",
    "        print(f\"Block {t}: Train acc: {acc_train}, Test acc: {acc_test}\")\n",
    "\n",
    "print_all_accuracies(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
