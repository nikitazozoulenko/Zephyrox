{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Union, Any, Optional, Dict, Literal, Callable\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "\n",
    "import numpy as np\n",
    "import aeon\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import pandas as pd\n",
    "from aeon.datasets.tser_datasets import tser_soton\n",
    "from aeon.datasets import load_regression\n",
    "from aeon.regression.sklearn import RotationForestRegressor\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from aeon.transformations.collection.convolution_based import Rocket, MultiRocketMultivariate, MiniRocketMultivariate\n",
    "\n",
    "from utils.utils import print_name, print_shape\n",
    "# from rocket import Rocket, RocketFeatures\n",
    "# from ridge_loocv import fit_ridge_LOOCV\n",
    "from preprocessing.stream_transforms import normalize_mean_std_traindata, normalize_streams, augment_time, add_basepoint_zero\n",
    "from random_sig_fourier import SigTensorisedRandProj\n",
    "from signature import SigTransform, LogSigTransform\n",
    "from features.base import TimeseriesFeatureExtractor, TabularTimeseriesFeatures, RandomGuesser\n",
    "from randomized_sig import RandomizedSignature\n",
    "\n",
    "np.set_printoptions(precision=3, threshold=5) # Print options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "####        ROCKET wrappers           ####\n",
    "##########################################\n",
    "\n",
    "class MultiRocketWrapper():\n",
    "    def __init__(\n",
    "            self,\n",
    "            max_batch: int = 10000,\n",
    "            n_features: int = 3000,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Wrapper for the MultiRocketTransform from the aeon library.\n",
    "        Original paper: https://link.springer.com/article/10.1007/s10618-022-00844-1\n",
    "\n",
    "        Args:\n",
    "            max_batch (int): Maximum batch size for computations.\n",
    "            n_features (int):  Number of random features.\n",
    "        \"\"\"\n",
    "        self.max_batch = max_batch\n",
    "        self.n_features = n_features\n",
    "        self.rocket = MultiRocketMultivariate(max(1, n_features//4))\n",
    "\n",
    "\n",
    "    def fit(self, X: Tensor): #shape (N, T, D)\n",
    "        self.rocket.fit(np.array(X).transpose(0,2,1))\n",
    "\n",
    "\n",
    "    def _batched_transform(self, X: Tensor) -> Tensor:\n",
    "        \"\"\"MultiRocket features.\n",
    "\n",
    "        Args:\n",
    "            X (Tensor): Shape (N, T, D)\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Transformed tensor of shape (N, n_features)\n",
    "        \"\"\"\n",
    "        X_np = X.cpu().numpy().transpose(0,2,1)\n",
    "        features = self.rocket.transform(X_np)\n",
    "        return torch.from_numpy(features, dtype=X.dtype, device=X.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#######          Dataset Code         #######\n",
    "#############################################\n",
    "\n",
    "def get_aeon_dataset(\n",
    "        dataset_name:str, \n",
    "        #extract_path = \"/rds/general/user/nz423/home/Data/TSER/\"\n",
    "        extract_path = \"/home/nikita/hdd/Data/TSER/\"\n",
    "        ):\n",
    "    \"\"\"Loads a dataset from the UCR/UEA archive using \n",
    "    the aeon library.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): Name of the dataset\n",
    "\n",
    "    Returns:\n",
    "        Tuple: 4-tuple of the form (X_train, y_train, X_test, y_test)\n",
    "    \"\"\"\n",
    "    X_train, y_train = load_regression(dataset_name, split=\"train\", extract_path=extract_path)\n",
    "    X_test, y_test = load_regression(dataset_name, split=\"test\", extract_path=extract_path)\n",
    "    X_train = torch.from_numpy(X_train.transpose(0,2,1))\n",
    "    X_test = torch.from_numpy(X_test.transpose(0,2,1))\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "####  Linear Model (Ridge)  ######\n",
    "##################################\n",
    "\n",
    "def train_and_test_linear(\n",
    "        train_X, train_y, test_X, test_y,\n",
    "        feat_extractor: TimeseriesFeatureExtractor,\n",
    "        apply_augmentation:bool=True,\n",
    "        normalize_features:bool=True,\n",
    "        clf=RidgeCV(alphas=np.logspace(-3, 3, 20))\n",
    "    ):\n",
    "    # augment data\n",
    "    print(train_X.shape)\n",
    "    if apply_augmentation:\n",
    "        train_X, test_X = normalize_mean_std_traindata(train_X, test_X)\n",
    "        train_X = add_basepoint_zero(train_X)\n",
    "        train_X = augment_time(train_X)\n",
    "        test_X = add_basepoint_zero(test_X)\n",
    "        test_X = augment_time(test_X)\n",
    "\n",
    "    # fit transformer\n",
    "    t0 = time.time()\n",
    "    feat_extractor.fit(train_X)\n",
    "    feat_train_X = np.array(feat_extractor.transform(train_X))\n",
    "    feat_test_X = np.array(feat_extractor.transform(test_X))\n",
    "    if normalize_features:\n",
    "        feat_train_X, feat_test_X = normalize_mean_std_traindata(feat_train_X, feat_test_X)\n",
    "\n",
    "\n",
    "    # feed into linear classifier\n",
    "    t1 = time.time()\n",
    "    clf.fit(feat_train_X, train_y)\n",
    "    t2 = time.time()\n",
    "\n",
    "    # predict\n",
    "    pred = clf.predict(feat_test_X)\n",
    "    test_rmse = root_mean_squared_error(test_y, pred)\n",
    "    train_rmse = root_mean_squared_error(train_y, clf.predict(feat_train_X))\n",
    "    alpha = clf.alpha_ if hasattr(clf, 'alpha_') else None\n",
    "    return train_rmse, test_rmse, alpha, t1-t0, t2-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_allModels_singleDataset(X_train, y_train, X_test, y_test):\n",
    "    max_batch = 32\n",
    "    trunc_level = 4\n",
    "    n_features = 1000\n",
    "\n",
    "    models = [\n",
    "        [\"Random Guesser\", RandomGuesser()],\n",
    "        [\"Tabular\", TabularTimeseriesFeatures()],\n",
    "        # [\"Sig\", SigTransform(trunc_level, max_batch)],\n",
    "        # [\"Log Sig\", LogSigTransform(trunc_level, max_batch)],\n",
    "        [\"Randomized Signature\", RandomizedSignature(\n",
    "            n_features,\n",
    "            activation = \"tanh\",\n",
    "            max_batch=10,\n",
    "            )],\n",
    "        [\"TRP\", SigTensorisedRandProj(\n",
    "            trunc_level,\n",
    "            n_features,\n",
    "            only_last=True,\n",
    "            method=\"linear\",\n",
    "            max_batch=max_batch,\n",
    "            )],\n",
    "        [\"TRP rbf\", SigTensorisedRandProj(\n",
    "            trunc_level,\n",
    "            n_features,\n",
    "            only_last=True,\n",
    "            method=\"RBF\",\n",
    "            sigma_rbf=1.0,\n",
    "            max_batch=max_batch,\n",
    "            )],\n",
    "        [\"concat TRP\", SigTensorisedRandProj(\n",
    "            trunc_level,\n",
    "            n_features,\n",
    "            only_last=False,\n",
    "            method=\"linear\",\n",
    "            max_batch=max_batch,\n",
    "            )],\n",
    "        [\"concat TRP rbf\", SigTensorisedRandProj(\n",
    "            trunc_level,\n",
    "            n_features,\n",
    "            only_last=False,\n",
    "            method=\"RBF\",\n",
    "            sigma_rbf=1.0,\n",
    "            max_batch=max_batch,\n",
    "            )],\n",
    "        [\"MultiRocket\", SigTensorisedRandProj(\n",
    "            trunc_level,\n",
    "            n_features,\n",
    "            only_last=False,\n",
    "            method=\"RBF\",\n",
    "            sigma_rbf=1.0,\n",
    "            max_batch=max_batch,\n",
    "            )],\n",
    "        ]\n",
    "    \n",
    "    # rocket_models = [\n",
    "    #     [\"Rocket\", Rocket(n_features//2, random_state=numpy_seed)],\n",
    "    #     [\"MiniRocket\", MiniRocketMultivariate(n_features, random_state=numpy_seed)],\n",
    "    #     [\"MultiRocket\", MultiRocketMultivariate(n_features//4, random_state=numpy_seed)],\n",
    "    #     ]\n",
    "    \n",
    "    # Run experiments\n",
    "    model_names = [name for (name, _) in models]\n",
    "    results_ridge = []\n",
    "    for name, model in models:\n",
    "        result = train_and_test_linear(\n",
    "            X_train, y_train, X_test, y_test, model\n",
    "            )\n",
    "        results_ridge.append(result)\n",
    "    \n",
    "    return model_names, results_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dataset(dataset_name:str):\n",
    "    X_train, y_train, X_test, y_test = get_aeon_dataset(dataset_name)\n",
    "    model_names, results_ridge = run_allModels_singleDataset(X_train, y_train, X_test, y_test)\n",
    "    return model_names, results_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1099, 24, 6])\n",
      "torch.Size([1099, 24, 6])\n",
      "torch.Size([1099, 24, 6])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m tser_soton \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mlist\u001b[39m(tser_soton))\n\u001b[0;32m----> 2\u001b[0m \u001b[43mrun_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtser_soton\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m, in \u001b[0;36mrun_dataset\u001b[0;34m(dataset_name)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_dataset\u001b[39m(dataset_name:\u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m      2\u001b[0m     X_train, y_train, X_test, y_test \u001b[38;5;241m=\u001b[39m get_aeon_dataset(dataset_name)\n\u001b[0;32m----> 3\u001b[0m     model_names, results_ridge \u001b[38;5;241m=\u001b[39m \u001b[43mrun_allModels_singleDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_names, results_ridge\n",
      "Cell \u001b[0;32mIn[18], line 66\u001b[0m, in \u001b[0;36mrun_allModels_singleDataset\u001b[0;34m(X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m     64\u001b[0m results_ridge \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[0;32m---> 66\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_test_linear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     results_ridge\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_names, results_ridge\n",
      "Cell \u001b[0;32mIn[11], line 24\u001b[0m, in \u001b[0;36mtrain_and_test_linear\u001b[0;34m(train_X, train_y, test_X, test_y, feat_extractor, apply_augmentation, normalize_features, clf)\u001b[0m\n\u001b[1;32m     22\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     23\u001b[0m feat_extractor\u001b[38;5;241m.\u001b[39mfit(train_X)\n\u001b[0;32m---> 24\u001b[0m feat_train_X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mfeat_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_X\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     25\u001b[0m feat_test_X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(feat_extractor\u001b[38;5;241m.\u001b[39mtransform(test_X))\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m normalize_features:\n",
      "File \u001b[0;32m~/Code/zephyrox/.conda/lib/python3.11/site-packages/sklearn/utils/_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    301\u001b[0m         )\n",
      "File \u001b[0;32m~/Code/zephyrox/pytorch_based/features/base.py:59\u001b[0m, in \u001b[0;36mTimeseriesFeatureExtractor.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transform the input time series data into features. Splits the\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03mdata into sub-batches if necessary based on 'self.max_batch'.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m    (Tensor): Feature vectors of shape (N, ...)\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     57\u001b[0m split_X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msplit(X, \u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[0;32m---> 59\u001b[0m     \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batched_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msplit_X\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[1;32m     60\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     61\u001b[0m     )\n",
      "File \u001b[0;32m~/Code/zephyrox/pytorch_based/features/base.py:59\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transform the input time series data into features. Splits the\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03mdata into sub-batches if necessary based on 'self.max_batch'.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m    (Tensor): Feature vectors of shape (N, ...)\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     57\u001b[0m split_X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msplit(X, \u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(\n\u001b[0;32m---> 59\u001b[0m     [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batched_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m split_X],\n\u001b[1;32m     60\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     61\u001b[0m     )\n",
      "File \u001b[0;32m~/Code/zephyrox/pytorch_based/features/randomized_sig.py:141\u001b[0m, in \u001b[0;36mRandomizedSignature._batched_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the RBF TRP-RFSF features for the given input tensor,\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03mmapping time series from (T,d) to (n_features).\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03m    Tensor: Tensor of shape (N, n_features).\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mrandomized_sig_tanh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mY_0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     features \u001b[38;5;241m=\u001b[39m randomized_sig_linear(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mA, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mY_0)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tser_soton = sorted(list(tser_soton))\n",
    "run_dataset(tser_soton[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def do_experiments(datasets: List[str]):\n",
    "#     experiments = {}\n",
    "#     experiments_metadata = {}\n",
    "#     failed = {}\n",
    "#     for dataset_name in tqdm(datasets):\n",
    "#         t0 = time.time()\n",
    "#         try:\n",
    "#             print(dataset_name)\n",
    "#             X_train, y_train, X_test, y_test = get_aeon_dataset(dataset_name)\n",
    "#             X_train, X_test = normalize_streams(X_train, X_test, max_T=1000)\n",
    "#             y_train, y_test = normalize_mean_std_traindata(y_train, y_test)\n",
    "#             N_train = X_train.shape[0]\n",
    "#             N_test = X_test.shape[0]\n",
    "#             T = X_train.shape[1]\n",
    "#             D = X_train.shape[2]\n",
    "#             if N_train > 2000 or D > 20:\n",
    "#                 continue\n",
    "#             results = run_all_experiments(\n",
    "#                 X_train, y_train, X_test, y_test\n",
    "#                 )\n",
    "#             experiments_metadata[dataset_name] = {\n",
    "#                 \"N_train\": N_train,\n",
    "#                 \"N_test\": N_test,\n",
    "#                 \"T\": T,\n",
    "#                 \"D\": D,\n",
    "#             }\n",
    "#             experiments[dataset_name] = results\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error: {e}\")\n",
    "#             failed[dataset_name] = e\n",
    "#         print(\"Elapsed time\", time.time()-t0)\n",
    "#     return experiments, experiments_metadata, failed\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     d_res, d_meta, d_failed = do_experiments(list(tser_soton))\n",
    "    \n",
    "#     # Define the attributes and methods\n",
    "#     attributes = [\"RMSE_train\", \"RMSE_test\", \"time_transform\", \"time_fit\", \"alpha\"]\n",
    "#     methods = [\"ridge\", \"rotforest\"]\n",
    "    \n",
    "#     # Extract model_names from d_res\n",
    "#     model_names = next(iter(d_res.values()))[0]\n",
    "\n",
    "#     # Create and save DataFrames for each attribute and method\n",
    "#     for attribute in attributes:\n",
    "#         for method in methods:\n",
    "#             df = pd.DataFrame(columns=model_names)\n",
    "#             for dataset_name, (model_names, results_ridge, results_rotforest) in d_res.items():\n",
    "#                 if method == \"ridge\":\n",
    "#                     results = results_ridge\n",
    "#                 elif method == \"rotforest\":\n",
    "#                     results = results_rotforest\n",
    "\n",
    "#                 values = [res[attributes.index(attribute)] for res in results]\n",
    "#                 df.loc[dataset_name] = values\n",
    "\n",
    "#             # Save the DataFrame\n",
    "#             print(df)\n",
    "#             df.to_pickle(f\"TESR_{attribute}_{method}_results.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
