{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikita/Code/zephyrox/.conda/lib/python3.10/site-packages/aeon/base/__init__.py:24: FutureWarning: The aeon package will soon be releasing v1.0.0 with the removal of legacy modules and interfaces such as BaseTransformer and BaseForecaster. This will contain breaking changes. See aeon-toolkit.org for more information. Set aeon.AEON_DEPRECATION_WARNING or the AEON_DEPRECATION_WARNING environmental variable to 'False' to disable this warning.\n",
      "  warnings.warn(\n",
      "2024-10-10 15:28:09.517220: W external/xla/xla/service/gpu/nvptx_compiler.cc:718] The NVIDIA driver's CUDA version is 12.4 which is older than the ptxas CUDA version (12.5.82). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple, List, Union, Any, Optional, Dict, Literal, Callable\n",
    "import time\n",
    "import collections\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "\n",
    "from tqdm import tqdm\n",
    "import openml\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.lax as lax\n",
    "from jaxtyping import Array, Float, Int, PRNGKeyArray\n",
    "import aeon\n",
    "import pandas as pd\n",
    "from preprocessing.timeseries_augmentation import normalize_mean_std_traindata, normalize_streams, augment_time, add_basepoint_zero\n",
    "from aeon.regression.sklearn import RotationForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from aeon.datasets.tser_datasets import tser_soton\n",
    "from aeon.datasets import load_regression, load_classification\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from features.sig import SigTransform, LogSigTransform\n",
    "from features.base import TimeseriesFeatureTransformer, TabularTimeseriesFeatures, RandomGuesser, RandomProjectionFeatures\n",
    "from features.sig_neural import RandomizedSignature, TimeInhomogenousRandomizedSignature\n",
    "from features.SWIM_controlled_resnet import SampledControlledResNet\n",
    "from features.efficient_SCRN import memory_efficient_SCRN\n",
    "from features.rocket_wrappers import RocketWrapper\n",
    "from utils.utils import print_name, print_shape\n",
    "\n",
    "jax.config.update('jax_platform_name', 'gpu') # Used to set the platform (cpu, gpu, etc.)\n",
    "np.set_printoptions(precision=3, threshold=5) # Print options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenML code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/35 Processed dataset 44956: abalone\n",
      " 2/35 Processed dataset 44957: airfoil_self_noise\n",
      " 3/35 Processed dataset 44958: auction_verification\n",
      " 4/35 Processed dataset 44959: concrete_compressive_strength\n",
      " 5/35 Processed dataset 44963: physiochemical_protein\n",
      " 6/35 Processed dataset 44964: superconductivity\n",
      " 7/35 Processed dataset 44965: geographical_origin_of_music\n",
      " 8/35 Processed dataset 44966: solar_flare\n",
      " 9/35 Processed dataset 44969: naval_propulsion_plant\n",
      " 10/35 Processed dataset 44971: white_wine\n",
      " 11/35 Processed dataset 44972: red_wine\n",
      " 12/35 Processed dataset 44973: grid_stability\n",
      " 13/35 Processed dataset 44974: video_transcoding\n",
      " 14/35 Processed dataset 44975: wave_energy\n",
      " 15/35 Processed dataset 44976: sarcos\n",
      " 16/35 Processed dataset 44977: california_housing\n",
      " 17/35 Processed dataset 44978: cpu_activity\n",
      " 18/35 Processed dataset 44979: diamonds\n",
      " 19/35 Processed dataset 44980: kin8nm\n",
      " 20/35 Processed dataset 44981: pumadyn32nh\n",
      " 21/35 Processed dataset 44983: miami_housing\n",
      " 22/35 Processed dataset 44984: cps88wages\n",
      " 23/35 Processed dataset 44987: socmob\n",
      " 24/35 Processed dataset 44989: kings_county\n",
      " 25/35 Processed dataset 44990: brazilian_houses\n",
      " 26/35 Processed dataset 44992: fps_benchmark\n",
      " 27/35 Processed dataset 44993: health_insurance\n",
      " 28/35 Processed dataset 45012: fifa\n",
      " 29/35 Processed dataset 41021: Moneyball\n",
      " 30/35 Processed dataset 44960: energy_efficiency\n",
      " 31/35 Processed dataset 44962: forest_fires\n",
      " 32/35 Processed dataset 44967: student_performance_por\n",
      " 33/35 Processed dataset 44970: QSAR_fish_toxicity\n",
      " 34/35 Processed dataset 44994: cars\n",
      " 35/35 Processed dataset 45402: space_ga\n"
     ]
    }
   ],
   "source": [
    "# Fetch the collection with ID 353\n",
    "collection = openml.study.get_suite(353)\n",
    "dataset_ids = collection.data\n",
    "metadata_list = []\n",
    "\n",
    "# Fetch and process each dataset\n",
    "for i, dataset_id in enumerate(dataset_ids):\n",
    "    dataset = openml.datasets.get_dataset(dataset_id)\n",
    "    X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "        target=dataset.default_target_attribute\n",
    "    )\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Extract the required metadata\n",
    "    metadata = {\n",
    "        'dataset_id': dataset.id,\n",
    "        'name': dataset.name,\n",
    "        'n_obs': int(dataset.qualities['NumberOfInstances']),\n",
    "        'n_features': int(dataset.qualities['NumberOfFeatures']),\n",
    "        '%_unique_y': len(np.unique(y))/len(y),\n",
    "        'n_unique_y': len(np.unique(y)),\n",
    "    }\n",
    "    \n",
    "    metadata_list.append(metadata)\n",
    "    print(f\" {i+1}/{len(dataset_ids)} Processed dataset {dataset.id}: {dataset.name}\")\n",
    "\n",
    "# Create a DataFrame from the metadata list\n",
    "df_metadata = pd.DataFrame(metadata_list).sort_values('%_unique_y', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>name</th>\n",
       "      <th>n_obs</th>\n",
       "      <th>n_features</th>\n",
       "      <th>%_unique_y</th>\n",
       "      <th>n_unique_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>44971</td>\n",
       "      <td>white_wine</td>\n",
       "      <td>4898</td>\n",
       "      <td>12</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>44993</td>\n",
       "      <td>health_insurance</td>\n",
       "      <td>22272</td>\n",
       "      <td>12</td>\n",
       "      <td>0.003367</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>44972</td>\n",
       "      <td>red_wine</td>\n",
       "      <td>1599</td>\n",
       "      <td>12</td>\n",
       "      <td>0.003752</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>44969</td>\n",
       "      <td>naval_propulsion_plant</td>\n",
       "      <td>11934</td>\n",
       "      <td>15</td>\n",
       "      <td>0.004274</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44956</td>\n",
       "      <td>abalone</td>\n",
       "      <td>4177</td>\n",
       "      <td>9</td>\n",
       "      <td>0.006703</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>44978</td>\n",
       "      <td>cpu_activity</td>\n",
       "      <td>8192</td>\n",
       "      <td>22</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>45012</td>\n",
       "      <td>fifa</td>\n",
       "      <td>19178</td>\n",
       "      <td>29</td>\n",
       "      <td>0.006935</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>44966</td>\n",
       "      <td>solar_flare</td>\n",
       "      <td>1066</td>\n",
       "      <td>11</td>\n",
       "      <td>0.007505</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>44967</td>\n",
       "      <td>student_performance_por</td>\n",
       "      <td>649</td>\n",
       "      <td>31</td>\n",
       "      <td>0.026194</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>44965</td>\n",
       "      <td>geographical_origin_of_music</td>\n",
       "      <td>1059</td>\n",
       "      <td>117</td>\n",
       "      <td>0.029273</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>44992</td>\n",
       "      <td>fps_benchmark</td>\n",
       "      <td>24624</td>\n",
       "      <td>44</td>\n",
       "      <td>0.108634</td>\n",
       "      <td>2675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>44964</td>\n",
       "      <td>superconductivity</td>\n",
       "      <td>21263</td>\n",
       "      <td>82</td>\n",
       "      <td>0.141419</td>\n",
       "      <td>3007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>44983</td>\n",
       "      <td>miami_housing</td>\n",
       "      <td>13932</td>\n",
       "      <td>16</td>\n",
       "      <td>0.151522</td>\n",
       "      <td>2111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>44974</td>\n",
       "      <td>video_transcoding</td>\n",
       "      <td>68784</td>\n",
       "      <td>19</td>\n",
       "      <td>0.159339</td>\n",
       "      <td>10960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>44977</td>\n",
       "      <td>california_housing</td>\n",
       "      <td>20640</td>\n",
       "      <td>9</td>\n",
       "      <td>0.186143</td>\n",
       "      <td>3842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>44989</td>\n",
       "      <td>kings_county</td>\n",
       "      <td>21613</td>\n",
       "      <td>22</td>\n",
       "      <td>0.186369</td>\n",
       "      <td>4028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>44984</td>\n",
       "      <td>cps88wages</td>\n",
       "      <td>28155</td>\n",
       "      <td>7</td>\n",
       "      <td>0.212040</td>\n",
       "      <td>5970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>44979</td>\n",
       "      <td>diamonds</td>\n",
       "      <td>53940</td>\n",
       "      <td>10</td>\n",
       "      <td>0.215091</td>\n",
       "      <td>11602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>44976</td>\n",
       "      <td>sarcos</td>\n",
       "      <td>48933</td>\n",
       "      <td>22</td>\n",
       "      <td>0.233258</td>\n",
       "      <td>11414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>41021</td>\n",
       "      <td>Moneyball</td>\n",
       "      <td>1232</td>\n",
       "      <td>15</td>\n",
       "      <td>0.303571</td>\n",
       "      <td>374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>44987</td>\n",
       "      <td>socmob</td>\n",
       "      <td>1156</td>\n",
       "      <td>6</td>\n",
       "      <td>0.312284</td>\n",
       "      <td>361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44963</td>\n",
       "      <td>physiochemical_protein</td>\n",
       "      <td>45730</td>\n",
       "      <td>10</td>\n",
       "      <td>0.347759</td>\n",
       "      <td>15903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>44962</td>\n",
       "      <td>forest_fires</td>\n",
       "      <td>517</td>\n",
       "      <td>13</td>\n",
       "      <td>0.485493</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>44990</td>\n",
       "      <td>brazilian_houses</td>\n",
       "      <td>10692</td>\n",
       "      <td>10</td>\n",
       "      <td>0.537879</td>\n",
       "      <td>5751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>44960</td>\n",
       "      <td>energy_efficiency</td>\n",
       "      <td>768</td>\n",
       "      <td>9</td>\n",
       "      <td>0.764323</td>\n",
       "      <td>587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44959</td>\n",
       "      <td>concrete_compressive_strength</td>\n",
       "      <td>1030</td>\n",
       "      <td>9</td>\n",
       "      <td>0.910680</td>\n",
       "      <td>938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>44970</td>\n",
       "      <td>QSAR_fish_toxicity</td>\n",
       "      <td>908</td>\n",
       "      <td>7</td>\n",
       "      <td>0.910793</td>\n",
       "      <td>827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44957</td>\n",
       "      <td>airfoil_self_noise</td>\n",
       "      <td>1503</td>\n",
       "      <td>6</td>\n",
       "      <td>0.968729</td>\n",
       "      <td>1456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>44994</td>\n",
       "      <td>cars</td>\n",
       "      <td>804</td>\n",
       "      <td>18</td>\n",
       "      <td>0.992537</td>\n",
       "      <td>798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44958</td>\n",
       "      <td>auction_verification</td>\n",
       "      <td>2043</td>\n",
       "      <td>8</td>\n",
       "      <td>0.998042</td>\n",
       "      <td>2039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>45402</td>\n",
       "      <td>space_ga</td>\n",
       "      <td>3107</td>\n",
       "      <td>7</td>\n",
       "      <td>0.999356</td>\n",
       "      <td>3105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>44981</td>\n",
       "      <td>pumadyn32nh</td>\n",
       "      <td>8192</td>\n",
       "      <td>33</td>\n",
       "      <td>0.999878</td>\n",
       "      <td>8191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>44980</td>\n",
       "      <td>kin8nm</td>\n",
       "      <td>8192</td>\n",
       "      <td>9</td>\n",
       "      <td>0.999878</td>\n",
       "      <td>8191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>44975</td>\n",
       "      <td>wave_energy</td>\n",
       "      <td>72000</td>\n",
       "      <td>49</td>\n",
       "      <td>0.999903</td>\n",
       "      <td>71993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>44973</td>\n",
       "      <td>grid_stability</td>\n",
       "      <td>10000</td>\n",
       "      <td>13</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset_id                           name  n_obs  n_features  %_unique_y  \\\n",
       "9        44971                     white_wine   4898          12    0.001429   \n",
       "26       44993               health_insurance  22272          12    0.003367   \n",
       "10       44972                       red_wine   1599          12    0.003752   \n",
       "8        44969         naval_propulsion_plant  11934          15    0.004274   \n",
       "0        44956                        abalone   4177           9    0.006703   \n",
       "16       44978                   cpu_activity   8192          22    0.006836   \n",
       "27       45012                           fifa  19178          29    0.006935   \n",
       "7        44966                    solar_flare   1066          11    0.007505   \n",
       "31       44967        student_performance_por    649          31    0.026194   \n",
       "6        44965   geographical_origin_of_music   1059         117    0.029273   \n",
       "25       44992                  fps_benchmark  24624          44    0.108634   \n",
       "5        44964              superconductivity  21263          82    0.141419   \n",
       "20       44983                  miami_housing  13932          16    0.151522   \n",
       "12       44974              video_transcoding  68784          19    0.159339   \n",
       "15       44977             california_housing  20640           9    0.186143   \n",
       "23       44989                   kings_county  21613          22    0.186369   \n",
       "21       44984                     cps88wages  28155           7    0.212040   \n",
       "17       44979                       diamonds  53940          10    0.215091   \n",
       "14       44976                         sarcos  48933          22    0.233258   \n",
       "28       41021                      Moneyball   1232          15    0.303571   \n",
       "22       44987                         socmob   1156           6    0.312284   \n",
       "4        44963         physiochemical_protein  45730          10    0.347759   \n",
       "30       44962                   forest_fires    517          13    0.485493   \n",
       "24       44990               brazilian_houses  10692          10    0.537879   \n",
       "29       44960              energy_efficiency    768           9    0.764323   \n",
       "3        44959  concrete_compressive_strength   1030           9    0.910680   \n",
       "32       44970             QSAR_fish_toxicity    908           7    0.910793   \n",
       "1        44957             airfoil_self_noise   1503           6    0.968729   \n",
       "33       44994                           cars    804          18    0.992537   \n",
       "2        44958           auction_verification   2043           8    0.998042   \n",
       "34       45402                       space_ga   3107           7    0.999356   \n",
       "19       44981                    pumadyn32nh   8192          33    0.999878   \n",
       "18       44980                         kin8nm   8192           9    0.999878   \n",
       "13       44975                    wave_energy  72000          49    0.999903   \n",
       "11       44973                 grid_stability  10000          13    1.000000   \n",
       "\n",
       "    n_unique_y  \n",
       "9            7  \n",
       "26          75  \n",
       "10           6  \n",
       "8           51  \n",
       "0           28  \n",
       "16          56  \n",
       "27         133  \n",
       "7            8  \n",
       "31          17  \n",
       "6           31  \n",
       "25        2675  \n",
       "5         3007  \n",
       "20        2111  \n",
       "12       10960  \n",
       "15        3842  \n",
       "23        4028  \n",
       "21        5970  \n",
       "17       11602  \n",
       "14       11414  \n",
       "28         374  \n",
       "22         361  \n",
       "4        15903  \n",
       "30         251  \n",
       "24        5751  \n",
       "29         587  \n",
       "3          938  \n",
       "32         827  \n",
       "1         1456  \n",
       "33         798  \n",
       "2         2039  \n",
       "34        3105  \n",
       "19        8191  \n",
       "18        8191  \n",
       "13       71993  \n",
       "11       10000  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metadata.sort_values('%_unique_y', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download single dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_openml_dataset(dataset_id, \n",
    "                        normalize_X:bool = True,\n",
    "                        normalize_y:bool = True,\n",
    "                        train_test_size:float = 0.7,\n",
    "                        split_seed:int = 0) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    # Fetch dataset from OpenML by its ID\n",
    "    dataset = openml.datasets.get_dataset(dataset_id)\n",
    "    X, y, categorical_indicator, attribute_names = dataset.get_data(target=dataset.default_target_attribute)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_test_size, random_state=split_seed)\n",
    "\n",
    "    #normalize\n",
    "    if normalize_X:\n",
    "        X_train, X_test = normalize_mean_std_traindata(X_train, X_test)\n",
    "    if normalize_y:\n",
    "        y_train, y_test = normalize_mean_std_traindata(y_train, y_test)\n",
    "\n",
    "    return (jnp.array(X_train.astype(np.float32)), \n",
    "            jnp.array(y_train.astype(np.float32)), \n",
    "            jnp.array(X_test.astype(np.float32)), \n",
    "            jnp.array(y_test.astype(np.float32)))\n",
    "\n",
    "dataset_id = 44971  # Replace with the dataset ID you want\n",
    "X_train, X_test, y_train, y_test = load_openml_dataset(dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWIM tabular model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Union, Any, Optional, Dict, Set, Literal, Callable\n",
    "from abc import ABC, abstractmethod\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.lax as lax\n",
    "from jaxtyping import Array, Float, Int, PRNGKeyArray\n",
    "\n",
    "from features.base import TimeseriesFeatureTransformer\n",
    "\n",
    "\n",
    "\n",
    "def init_single_SWIM_layer(\n",
    "        seed: PRNGKeyArray,\n",
    "        X: Float[Array, \"N  d\"],\n",
    "        y: Float[Array, \"N  p\"],\n",
    "        n_features: int,\n",
    "        sampling_method: Literal[\"uniform\", \"gradient-weighted\"] = \"uniform\",\n",
    "    ) -> Tuple[Float[Array, \"d  n_features\"], Float[Array, \"n_features\"]]:\n",
    "    \"\"\"\n",
    "    Fits the weights for a single layer of the SWIM model.\n",
    "\n",
    "    Args:\n",
    "        seed (PRNGKeyArray): Random seed for the weights and biases.\n",
    "        X (Float[Array, \"N  d\"]): Previous layer's output.\n",
    "        y (Float[Array, \"N  p\"]): Target training data.\n",
    "        n_features (int): Next hidden layer size.\n",
    "        sampling_method (str): Uniform or gradient-weighted pair sampling.\n",
    "    Returns:\n",
    "        Weights (d, n_features) and biases (1, n_features) for the next layer.\n",
    "    \"\"\"\n",
    "    seed_idxs, seed_sample = jax.random.split(seed, 2)\n",
    "    N, d = X.shape\n",
    "    EPS = 1e-06\n",
    "\n",
    "    #obtain pair indices\n",
    "    n = 3*N\n",
    "    idx1 = jnp.arange(0, n) % N\n",
    "    delta = jax.random.randint(seed_idxs, shape=(n,), minval=1, maxval=N)\n",
    "    idx2 = (idx1 + delta) % N\n",
    "    \n",
    "    if sampling_method==\"gradient-weighted\":\n",
    "        #calculate 'gradients'\n",
    "        dx = X[idx2] - X[idx1]\n",
    "        dy = y[idx2] - y[idx1]\n",
    "        dists = jnp.maximum(EPS, jnp.linalg.norm(dx, axis=1, keepdims=True) )\n",
    "        gradients = (jnp.linalg.norm(dy, axis=1, keepdims=True) / dists, ).reshape(-1) #NOTE paper uses ord=inf instead of ord=2\n",
    "        p = gradients/gradients.sum()\n",
    "    elif sampling_method==\"uniform\":\n",
    "        p = None\n",
    "    else:\n",
    "        raise ValueError(f\"sampling_method must be 'uniform' or 'gradient-weighted'. Given: {sampling_method}\")\n",
    "\n",
    "    #sample pairs\n",
    "    selected_idx = jax.random.choice(\n",
    "        seed_sample, \n",
    "        n,\n",
    "        shape=(n_features,), \n",
    "        replace=True,\n",
    "        p=p\n",
    "        )\n",
    "    idx1 = idx1[selected_idx]\n",
    "    dx = dx[selected_idx]\n",
    "    dists = dists[selected_idx]\n",
    "\n",
    "    #define weights and biases\n",
    "    weights = (dx / dists**2).T\n",
    "    biases = -jnp.sum(weights * X[idx1].T, axis=0, keepdims=True) - 0.5  # NOTE experiment with this. also +-0.5 ?\n",
    "    return weights, biases\n",
    "\n",
    "\n",
    "\n",
    "def forward_1_layer(\n",
    "        X: Float[Array, \"N  d\"],\n",
    "        weights: Float[Array, \"d  n_features\"],\n",
    "        biases: Float[Array, \"1  n_features\"],\n",
    "        add_residual: bool,\n",
    "        activation = lambda x : jnp.maximum(0,x+0.5), # jnp.tanh,\n",
    "        scaling_factor: float = 1.0,\n",
    "    ) -> Float[Array, \"N  n_features\"]:\n",
    "    \"\"\"\n",
    "    Forward pass for a single layer of the SWIM model.\n",
    "    \"\"\"\n",
    "    d, D = weights.shape\n",
    "    X1 = activation(X @ weights + biases)\n",
    "    if add_residual:\n",
    "        return scaling_factor*X1 + X\n",
    "    else:\n",
    "        return X1\n",
    "\n",
    "\n",
    "\n",
    "def SWIM_all_layers(\n",
    "        seed: PRNGKeyArray,\n",
    "        X0: Float[Array, \"N  d\"],\n",
    "        y: Float[Array, \"N  p\"],\n",
    "        n_features: int,\n",
    "        activation: Callable,\n",
    "        n_layers: int,\n",
    "        add_residual: bool,\n",
    "        residual_scaling_factor: float = 1.0,\n",
    "        sampling_method: Literal[\"uniform\", \"gradient-weighted\"] = \"gradient-weighted\",\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Fits the weights for the SWIM model, iteratively layer by layer\n",
    "\n",
    "    Args:\n",
    "        seed (PRNGKeyArray): Random seed for the weights and biases.\n",
    "        X0 (Float[Array, \"N  d\"]): First layer input.\n",
    "        y (Float[Array, \"N  p\"]): Target training data.\n",
    "        n_features (int): Hidden layer size.\n",
    "        activation (Callable): Activation function for the network.\n",
    "        n_layers (int): Number of layers in the network.\n",
    "        add_residual (bool): Whether to use residual connections.\n",
    "        residual_scaling_factor (float): Scaling factor for the residual connections.\n",
    "        sampling_method (str): Uniform or gradient-weighted pair sampling for weight initialization.\n",
    "\n",
    "    Returns:\n",
    "        Weights (d, n_features) and biases (1, n_features) for the next layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def scan_body(carry, seed): # (carry, x) -> (carry, y)\n",
    "        X, y = carry\n",
    "        w, b = init_single_SWIM_layer(seed, X, y, n_features, sampling_method)\n",
    "        X = forward_1_layer(X, w, b, add_residual, activation, residual_scaling_factor)\n",
    "        return (X, y), (w, b)\n",
    "\n",
    "    init_carry = (X0, y)\n",
    "    carry, WaB = lax.scan(\n",
    "        scan_body,\n",
    "        init_carry,\n",
    "        xs=jax.random.split(seed, n_layers),\n",
    "    )\n",
    "    return WaB\n",
    "\n",
    "\n",
    "\n",
    "def all_forward(\n",
    "        X: Float[Array, \"N  d\"], \n",
    "        w1: Float[Array, \"d  D\"],\n",
    "        b1: Float[Array, \"1  D\"], \n",
    "        weights: Float[Array, \"n_layers-1  d  D\"],\n",
    "        biases: Float[Array, \"n_layers-1  1  D\"], \n",
    "        n_layers:int,\n",
    "        add_residual: bool,\n",
    "        activation = lambda x : jnp.maximum(0,x+0.5), # jnp.tanh,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Forward pass for the SWIM model.\n",
    "\n",
    "    Args:\n",
    "        X (Float[Array, \"N  d\"]): Input to the model.\n",
    "        w1 (Float[Array, \"d  D\"]): Weights for the first layer.\n",
    "        b1 (Float[Array, \"1  D\"]): Biases for the first layer.\n",
    "        weights (Float[Array, \"n_layers-1  d  D\"]): Weights for the remaining layers.\n",
    "        biases (Float[Array, \"n_layers-1  1  D\"]): Biases for the remaining layers.\n",
    "        n_layers (int): Number of layers in the network.\n",
    "        add_residual (bool): Whether to use residual connections\n",
    "        activation (Callable): Activation function for the network.\n",
    "    Returns:\n",
    "        Output of the model of shape (N, D).\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SWIM_MLP(TimeseriesFeatureTransformer):\n",
    "    def __init__(\n",
    "            self,\n",
    "            seed: PRNGKeyArray,\n",
    "            n_features: int = 512,\n",
    "            n_layers: int = 3,\n",
    "            add_residual: bool = False,\n",
    "            max_batch: int = 512,\n",
    "            activation = lambda x : jnp.maximum(0,x+0.5), # jnp.tanh,\n",
    "        ):\n",
    "        \"\"\"Implementation of the original paper's SWIM model\n",
    "        https://gitlab.com/felix.dietrich/swimnetworks-paper/,\n",
    "        but with support for residual connections.\n",
    "\n",
    "        Args:\n",
    "            seed (PRNGKeyArray): Random seed for matrices, biases, initial value.\n",
    "            n_features (int): Hidden layer dimension.\n",
    "            n_layers (int): Number of layers in the network.\n",
    "            add_residual (bool): Whether to use residual connections.\n",
    "            max_batch (int): Max batch size for computations.\n",
    "            activation (Callable): Activation function for the network.\n",
    "        \"\"\"\n",
    "        super().__init__(max_batch)\n",
    "        self.n_features = n_features\n",
    "        self.n_layers = n_layers\n",
    "        self.seed = seed\n",
    "        self.add_residual = add_residual\n",
    "        self.activation = activation\n",
    "        self.w1 = None\n",
    "        self.b1 = None\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "\n",
    "\n",
    "    def fit(\n",
    "            self, \n",
    "            X: Float[Array, \"N  D\"], \n",
    "            y: Float[Array, \"N  d\"]\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initializes MLP weights and biases, using SWIM algorithm.\n",
    "\n",
    "        Args:\n",
    "            X (Float[Array, \"N  D\"]): Input training data.\n",
    "            y (Float[Array, \"N  d\"]): Target training data.\n",
    "        \"\"\"\n",
    "        #TODO TODO TODO do this, add new args to all the functions and class init, such as activation and residual_scale_factor TODO\n",
    "        # Get shape, dtype\n",
    "        N, D = X.shape\n",
    "        seed1, seedrest = jax.random.split(self.seed, 2)\n",
    "\n",
    "        #first do first layer, which cannot always be done in a scan loop\n",
    "        self.w1, self.b1 = init_single_SWIM_layer(\n",
    "            seed1, X, y, self.n_features\n",
    "            )\n",
    "        X = forward_1_layer(X, self.w1, self.b1, self.add_residual)\n",
    "        \n",
    "        #rest of the layers\n",
    "        if self.n_layers > 1:\n",
    "            self.weights, self.biases = SWIM_all_layers(\n",
    "                X, y, self.n_features, self.n_layers-1, self.add_residual, seedrest\n",
    "                )\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self, X: Float[Array, \"N  D\"]) -> Float[Array, \"N  n_features\"]:\n",
    "        #TODO TODO TODO do this\n",
    "        #First hidden layer\n",
    "        X = forward_1_layer(X, w1, b1, add_residual, activation)\n",
    "        if n_layers == 1:\n",
    "            return X\n",
    "        #subsequent layers in a scan loop\n",
    "        else:\n",
    "            def scan_body(carry, t):\n",
    "                X = carry\n",
    "                w, b = weights[t], biases[t]\n",
    "                return forward_1_layer(X, w, b, add_residual, activation), None\n",
    "\n",
    "            X, _ = lax.scan(scan_body, X, xs=jnp.arange(n_layers-1))\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def neuron_distribution_for_each_layer(\n",
    "        X_train: Array, \n",
    "        y_train: Array, \n",
    "        X_test: Array, \n",
    "        model: Callable, \n",
    "        hidden_size: int, \n",
    "        n_layers: int, \n",
    "        random_seed: int) -> Tuple[Array, Array]:\n",
    "    \"\"\"Looks at the distribution of neurons for each layer of a neural network model\n",
    "    (used to compare SWIM, residual sampling, and random feature networks).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the arrays to store the neuron distribution\n",
    "    train_layers= []\n",
    "    test_layers = []\n",
    "    \n",
    "    # for each layer\n",
    "    for t in range(n_layers):\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    return train_neuron_distribution, test_neuron_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to look at the distribution of weights (eigenvalues? absolute values of rows? distribution of (assuming iid) matrix entries?)\n",
    "\n",
    "distribution of neurons at each layer\n",
    "\n",
    "This is for both SWIM, Residual SWIM, random features, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
