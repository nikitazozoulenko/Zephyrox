{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Union, Any, Optional, Dict, Set, Literal, Callable\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.lax as lax\n",
    "from jaxtyping import Array, Float, Int, PRNGKeyArray\n",
    "from jax.random import PRNGKey\n",
    "import aeon\n",
    "import pandas as pd\n",
    "\n",
    "from features.sig_trp import SigVanillaTensorizedRandProj, SigRBFTensorizedRandProj\n",
    "from features.sig import SigTransform, LogSigTransform\n",
    "from features.base import TimeseriesFeatureTransformer, TabularTimeseriesFeatures, RandomGuesser\n",
    "from features.sig_neural import RandomizedSignature\n",
    "from utils.utils import print_name, print_shape\n",
    "from preprocessing.timeseries_augmentation import normalize_mean_std_traindata, normalize_streams, augment_time, add_basepoint_zero\n",
    "\n",
    "jax.config.update('jax_platform_name', 'cpu') # Used to set the platform (cpu, gpu, etc.)\n",
    "np.set_printoptions(precision=3, threshold=5) # Print options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_single_SWIM_layer(\n",
    "        X: Float[Array, \"N  d\"],\n",
    "        y: Float[Array, \"N  D\"],\n",
    "        n_features: int,\n",
    "        seed: PRNGKeyArray,\n",
    "    ) -> Tuple[Float[Array, \"d  n_features\"], Float[Array, \"n_features\"]]:\n",
    "    \"\"\"\n",
    "    Fits the weights for the SWIM model, iteratively layer by layer\n",
    "\n",
    "    Args:\n",
    "        X (Float[Array, \"N  d\"]): Previous layer's output.\n",
    "        n_features (int): Next hidden layer size.\n",
    "        seed (PRNGKeyArray): Random seed for the weights and biases.\n",
    "    Returns:\n",
    "        Weights (d, n_features) and biases (1, n_features) for the next layer.\n",
    "    \"\"\"\n",
    "    seed_idxs, seed_sample = jax.random.split(seed, 2)\n",
    "    N, d = X.shape\n",
    "    EPS = 1e-06\n",
    "\n",
    "    #obtain pair indices\n",
    "    n_pairs_pre = jnp.minimum(N * (N - 1), 3*n_features) #maybe this should be a parameter TODO\n",
    "    idx1 = jnp.arange(0, n_pairs_pre)\n",
    "    delta = jax.random.randint(seed_idxs, shape=(n_pairs_pre,), minval=1, maxval=N)\n",
    "    idx2 = (idx1 + delta) % N\n",
    "\n",
    "    #calculate 'gradients'\n",
    "    dx = X[idx2] - X[idx1]\n",
    "    dy = y[idx2] - y[idx1]\n",
    "    dists = jnp.maximum(EPS, jnp.linalg.norm(dx, axis=1, keepdims=True) )\n",
    "    gradients = (jnp.linalg.norm(dy, axis=1, keepdims=True) / dists ).reshape(-1)\n",
    "    #gradients = (np.max(np.abs(dy), axis=1, keepdims=True) / dists ).reshape(-1) #NOTE paper uses this instead\n",
    "\n",
    "    #sample pairs, weighted by gradients     NOTE make replace a parameter\n",
    "    selected_idx = jax.random.choice(\n",
    "        seed_sample, \n",
    "        n_pairs_pre,\n",
    "        shape=(n_features,), \n",
    "        replace=True, \n",
    "        p=gradients/gradients.sum()\n",
    "        )\n",
    "    idx1 = idx1[selected_idx]\n",
    "    dx = dx[selected_idx]\n",
    "    dists = dists[selected_idx]\n",
    "    \n",
    "    #define weights and biases\n",
    "    weights = (dx / dists**2).T\n",
    "    biases = -jnp.sum(weights * X[idx1].T, axis=0, keepdims=True) - 0.5  # NOTE experiment with this. also +-0.5 ?\n",
    "    return weights, biases\n",
    "\n",
    "\n",
    "seed1, seed2, seed3 = jax.random.split(PRNGKey(0), 3)\n",
    "N=10\n",
    "d=2\n",
    "dim_y = 3\n",
    "n_features=6\n",
    "\n",
    "X = jax.random.normal(seed1, (N, d))\n",
    "y = jax.random.normal(seed2, (N, dim_y))\n",
    "\n",
    "weights, biases = init_single_SWIM_layer(X, y, n_features, seed3)\n",
    "\n",
    "\n",
    "###### TODO NEXT TODO -----  use lax.scan to iterate over layers, and to implement the forward pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
